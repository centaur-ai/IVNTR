{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demo-intro",
   "metadata": {},
   "source": [
    "# Demonstration Data Generation: IVNTR's Training Data\n",
    "\n",
    "In this notebook, we'll explore how IVNTR generates training data from demonstrations using the actual implementation from `predicators/datasets/demo_only.py`. This is crucial for understanding:\n",
    "\n",
    "1. **The demonstration collection process**: Using the oracle approach through CogMan\n",
    "2. **Dataset and LowLevelTrajectory structures**: How demonstration data is organized\n",
    "3. **The actual implementation**: Real code that generates IVNTR's training data\n",
    "\n",
    "## Key Insight\n",
    "IVNTR uses the `_generate_demonstrations()` function to collect expert trajectories from the oracle approach. These demonstrations contain state-action sequences with metadata that becomes the supervised learning signal for neural predicate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Dict, Set, List, Sequence, Tuple\n",
    "\n",
    "# Add the project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import IVNTR components\n",
    "from predicators.envs.satellites import SatellitesEnv\n",
    "from predicators.structs import Object, State, GroundAtom, Action, NSRT, ParameterizedOption, Task\n",
    "from predicators.structs import LowLevelTrajectory, Dataset\n",
    "from predicators import utils\n",
    "from predicators.settings import CFG\n",
    "from predicators.approaches.oracle_approach import OracleApproach\n",
    "from predicators.ground_truth_models import get_gt_options\n",
    "from predicators.cogman import CogMan, run_episode_and_get_states\n",
    "from predicators.perception import create_perceiver\n",
    "from predicators.execution_monitoring import create_execution_monitor\n",
    "from predicators.datasets.demo_only import _generate_demonstrations\n",
    "\n",
    "# Configure for tutorial\n",
    "CFG.seed = 42\n",
    "CFG.num_train_tasks = 3\n",
    "CFG.satellites_num_sat_train = [2, 3]\n",
    "CFG.satellites_num_obj_train = [2, 3]\n",
    "CFG.timeout = 10.0\n",
    "CFG.demonstrator = \"oracle\"\n",
    "CFG.max_initial_demos = 3\n",
    "\n",
    "# Create environment\n",
    "env = SatellitesEnv(use_gui=False)\n",
    "train_tasks = env.get_train_tasks()[:3]  # Use first 3 tasks for demo\n",
    "\n",
    "print(\"‚úÖ Setup complete! Environment and tasks ready for demonstration generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstration-process",
   "metadata": {},
   "source": [
    "## 1. The Demonstration Collection Process\n",
    "\n",
    "The `_generate_demonstrations()` function in `demo_only.py` is the core of IVNTR's data collection. Let's examine how it works:\n",
    "\n",
    "### Key Components:\n",
    "1. **Oracle Approach**: Uses ground truth NSRTs and predicates to solve tasks\n",
    "2. **CogMan (Cognitive Manager)**: Orchestrates planning and execution\n",
    "3. **Perceiver**: Processes observations into states\n",
    "4. **Execution Monitor**: Monitors option execution\n",
    "\n",
    "Let's replicate this process step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oracle-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Set up the oracle approach (as done in _generate_demonstrations)\n",
    "print(\"ü§ñ Setting up Oracle Approach (from demo_only.py implementation)\\n\")\n",
    "\n",
    "# Get ground truth options for the environment\n",
    "options = get_gt_options(env.get_name())\n",
    "print(f\"üì¶ Ground truth options: {len(options)}\")\n",
    "for opt in list(options)[:5]:  # Show first 5\n",
    "    print(f\"   - {opt.name}\")\n",
    "if len(options) > 5:\n",
    "    print(f\"   ... ({len(options)-5} more)\")\n",
    "\n",
    "# Create the oracle approach (exactly as in demo_only.py lines 152-160)\n",
    "oracle_approach = OracleApproach(\n",
    "    env.predicates,\n",
    "    options,\n",
    "    env.types,\n",
    "    env.action_space,\n",
    "    train_tasks,\n",
    "    task_planning_heuristic=CFG.offline_data_task_planning_heuristic,\n",
    "    max_skeletons_optimized=CFG.offline_data_max_skeletons_optimized,\n",
    "    bilevel_plan_without_sim=CFG.offline_data_bilevel_plan_without_sim\n",
    ")\n",
    "\n",
    "print(f\"\\nüß† Oracle Approach created with:\")\n",
    "print(f\"   Predicates: {len(oracle_approach.get_predicates())}\")\n",
    "print(f\"   NSRTs: {len(oracle_approach.get_nsrts())}\")\n",
    "print(f\"   Options: {len(oracle_approach.get_options())}\")\n",
    "\n",
    "# Create perceiver and execution monitor (as in demo_only.py lines 161-162)\n",
    "perceiver = create_perceiver(CFG.perceiver)\n",
    "execution_monitor = create_execution_monitor(CFG.execution_monitor)\n",
    "\n",
    "# Create CogMan (Cognitive Manager) - the orchestrator (line 163)\n",
    "cogman = CogMan(oracle_approach, perceiver, execution_monitor)\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è CogMan created with perceiver: {type(perceiver).__name__}\")\n",
    "print(f\"   Execution monitor: {type(execution_monitor).__name__}\")\n",
    "print(f\"\\nüí° This setup exactly replicates the demo_only.py implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trajectory-generation",
   "metadata": {},
   "source": [
    "## 2. Understanding Trajectory Generation\n",
    "\n",
    "Now let's see how individual trajectories are generated using the actual `run_episode_and_get_states()` function. This is the core loop from lines 189-206 in `demo_only.py`.\n",
    "\n",
    "### The Process:\n",
    "1. **Reset CogMan** for the specific task\n",
    "2. **Run episode** and collect state-action pairs\n",
    "3. **Verify goal achievement** \n",
    "4. **Create LowLevelTrajectory** with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-single-trajectory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a single trajectory (replicating demo_only.py lines 189-243)\n",
    "print(\"üéØ Generating Single Demonstration Trajectory\\n\")\n",
    "\n",
    "# Use first task\n",
    "task_idx = 0\n",
    "env_task = env.get_train_tasks()[task_idx]\n",
    "\n",
    "print(f\"üìã Task {task_idx}:\")\n",
    "print(f\"   Goals: {len(env_task.goal)}\")\n",
    "for goal in env_task.goal:\n",
    "    print(f\"     - {goal}\")\n",
    "\n",
    "# Reset CogMan for this task (line 195)\n",
    "cogman.reset(env_task)\n",
    "\n",
    "try:\n",
    "    # Generate trajectory using run_episode_and_get_states (lines 196-206)\n",
    "    print(f\"\\nüöÄ Running episode with oracle approach...\")\n",
    "    \n",
    "    traj, _, _ = run_episode_and_get_states(\n",
    "        cogman,\n",
    "        env,\n",
    "        \"train\",\n",
    "        task_idx,\n",
    "        max_num_steps=CFG.horizon,\n",
    "        exceptions_to_break_on={\n",
    "            utils.OptionExecutionFailure,\n",
    "            utils.HumanDemonstrationFailure,\n",
    "        },\n",
    "        monitor=None  # No video monitor for tutorial\n",
    "    )\n",
    "    \n",
    "    # Check goal achievement (lines 232-234)\n",
    "    goal_achieved = env_task.goal_holds(traj.states[-1])\n",
    "    print(f\"‚úÖ Trajectory generated successfully!\")\n",
    "    print(f\"   Goal achieved: {goal_achieved}\")\n",
    "    print(f\"   Length: {len(traj.states)} states, {len(traj.actions)} actions\")\n",
    "    \n",
    "    if goal_achieved:\n",
    "        print(f\"\\nüé¨ Action sequence preview:\")\n",
    "        for i, action in enumerate(traj.actions[:5]):  # Show first 5 actions\n",
    "            # Show action type and first few values\n",
    "            action_str = f\"[{action.arr[0]:.2f}, {action.arr[1]:.2f}, ..., flags: {action.arr[6:]}\"\n",
    "            print(f\"   Step {i:2d}: {action_str}\")\n",
    "        if len(traj.actions) > 5:\n",
    "            print(f\"   ... ({len(traj.actions)-5} more actions)\")\n",
    "    \n",
    "    # Create the final LowLevelTrajectory with demo metadata (lines 240-243)\n",
    "    demo_trajectory = LowLevelTrajectory(\n",
    "        traj.states,\n",
    "        traj.actions,\n",
    "        _is_demo=True,\n",
    "        _train_task_idx=task_idx\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìã LowLevelTrajectory created with:\")\n",
    "    print(f\"   is_demo: {demo_trajectory.is_demo}\")\n",
    "    print(f\"   train_task_idx: {demo_trajectory.train_task_idx}\")\n",
    "    print(f\"   states: {len(demo_trajectory.states)}\")\n",
    "    print(f\"   actions: {len(demo_trajectory.actions)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during trajectory generation: {e}\")\n",
    "    demo_trajectory = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-structure",
   "metadata": {},
   "source": [
    "## 3. Dataset Structure and Organization\n",
    "\n",
    "Now let's examine the `Dataset` and `LowLevelTrajectory` structures that are central to IVNTR's data organization. Understanding these structures is crucial for neural predicate learning.\n",
    "\n",
    "### Key Structures:\n",
    "- **Dataset**: Collection of LowLevelTrajectory objects with optional annotations\n",
    "- **LowLevelTrajectory**: State-action sequence with demonstration metadata\n",
    "- **State**: Continuous feature vectors for all objects\n",
    "- **Action**: 10-dimensional vectors encoding control decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-structures",
   "metadata": {},
   "outputs": [],
   "source": [
    "if demo_trajectory is not None:\n",
    "    print(\"üîç Analyzing Dataset and Trajectory Structures\\n\")\n",
    "    \n",
    "    # Create a mini-dataset (as done in demo_only.py line 274)\n",
    "    mini_dataset = Dataset([demo_trajectory])\n",
    "    \n",
    "    print(f\"üìä Dataset Structure:\")\n",
    "    print(f\"   Type: {type(mini_dataset).__name__}\")\n",
    "    print(f\"   Number of trajectories: {len(mini_dataset.trajectories)}\")\n",
    "    print(f\"   Has annotations: {mini_dataset.has_annotations}\")\n",
    "    \n",
    "    # Analyze LowLevelTrajectory structure\n",
    "    traj = mini_dataset.trajectories[0]\n",
    "    print(f\"\\nüé¨ LowLevelTrajectory Structure:\")\n",
    "    print(f\"   Type: {type(traj).__name__}\")\n",
    "    print(f\"   Is demonstration: {traj.is_demo}\")\n",
    "    print(f\"   Train task index: {traj.train_task_idx}\")\n",
    "    print(f\"   States: {len(traj.states)} (should be actions + 1)\")\n",
    "    print(f\"   Actions: {len(traj.actions)}\")\n",
    "    \n",
    "    # Analyze state structure\n",
    "    init_state = traj.states[0]\n",
    "    final_state = traj.states[-1]\n",
    "    \n",
    "    print(f\"\\nüèóÔ∏è State Structure Analysis:\")\n",
    "    print(f\"   Type: {type(init_state).__name__}\")\n",
    "    \n",
    "    # Get objects from state\n",
    "    satellites = list(init_state.get_objects(env._sat_type))\n",
    "    objects = list(init_state.get_objects(env._obj_type))\n",
    "    \n",
    "    print(f\"   Objects: {len(satellites)} satellites + {len(objects)} objects\")\n",
    "    \n",
    "    # Show state data for one satellite\n",
    "    if satellites:\n",
    "        sat = satellites[0]\n",
    "        print(f\"\\n   Satellite '{sat.name}' state features:\")\n",
    "        sat_features = init_state.data[sat]\n",
    "        for feature_name, feature_value in sorted(sat_features.items()):\n",
    "            print(f\"     {feature_name}: {feature_value} ({type(feature_value).__name__})\")\n",
    "    \n",
    "    # Show state data for one object  \n",
    "    if objects:\n",
    "        obj = objects[0]\n",
    "        print(f\"\\n   Object '{obj.name}' state features:\")\n",
    "        obj_features = init_state.data[obj]\n",
    "        for feature_name, feature_value in sorted(obj_features.items()):\n",
    "            print(f\"     {feature_name}: {feature_value} ({type(feature_value).__name__})\")\n",
    "    \n",
    "    # Analyze action structure\n",
    "    if traj.actions:\n",
    "        sample_action = traj.actions[0]\n",
    "        print(f\"\\n‚ö° Action Structure:\")\n",
    "        print(f\"   Type: {type(sample_action).__name__}\")\n",
    "        print(f\"   Array shape: {sample_action.arr.shape}\")\n",
    "        print(f\"   Array dtype: {sample_action.arr.dtype}\")\n",
    "        print(f\"   Sample values: {sample_action.arr}\")\n",
    "        \n",
    "        # Check if action has option information\n",
    "        if hasattr(sample_action, '_option') and sample_action._option is not None:\n",
    "            print(f\"   Associated option: {sample_action.get_option().name}\")\n",
    "        else:\n",
    "            print(f\"   Associated option: None (removed for learning)\")\n",
    "    \n",
    "    print(f\"\\nüí° Key Insights:\")\n",
    "    print(f\"   ‚Ä¢ States contain continuous feature dictionaries for each object\")\n",
    "    print(f\"   ‚Ä¢ Actions are 10D numpy arrays encoding control decisions\")\n",
    "    print(f\"   ‚Ä¢ Trajectories include metadata (is_demo, train_task_idx) for learning\")\n",
    "    print(f\"   ‚Ä¢ This structure provides the foundation for neural predicate learning!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No trajectory available for structure analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-generation",
   "metadata": {},
   "source": [
    "## 4. Batch Demonstration Generation\n",
    "\n",
    "Finally, let's use the actual `_generate_demonstrations()` function to generate a batch of demonstrations, exactly as IVNTR does in practice. This replicates the complete process from `demo_only.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the actual _generate_demonstrations function from demo_only.py\n",
    "print(\"üéØ Generating Batch Demonstrations Using Actual Implementation\\n\")\n",
    "\n",
    "try:\n",
    "    # This calls the exact function used in IVNTR (demo_only.py lines 141-275)\n",
    "    known_options = set()  # Empty set - options will be removed from actions\n",
    "    \n",
    "    print(f\"üìä Generating demonstrations for {len(train_tasks)} tasks...\")\n",
    "    print(f\"   Using oracle demonstrator\")\n",
    "    print(f\"   Max initial demos: {CFG.max_initial_demos}\")\n",
    "    \n",
    "    # Call the actual implementation\n",
    "    dataset = _generate_demonstrations(\n",
    "        env=env,\n",
    "        train_tasks=train_tasks,\n",
    "        known_options=known_options,\n",
    "        train_tasks_start_idx=0,\n",
    "        annotate_with_gt_ops=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully generated {len(dataset.trajectories)} demonstrations!\")\n",
    "    \n",
    "    # Analyze the generated dataset\n",
    "    print(f\"\\nüìã Generated Dataset Analysis:\")\n",
    "    print(f\"   Type: {type(dataset).__name__}\")\n",
    "    print(f\"   Total trajectories: {len(dataset.trajectories)}\")\n",
    "    print(f\"   Has annotations: {dataset.has_annotations}\")\n",
    "    \n",
    "    # Analyze each trajectory\n",
    "    total_states = 0\n",
    "    total_actions = 0\n",
    "    \n",
    "    print(f\"\\nüìä Per-Trajectory Analysis:\")\n",
    "    for i, traj in enumerate(dataset.trajectories):\n",
    "        total_states += len(traj.states)\n",
    "        total_actions += len(traj.actions)\n",
    "        \n",
    "        print(f\"   Trajectory {i}:\")\n",
    "        print(f\"     Task index: {traj.train_task_idx}\")\n",
    "        print(f\"     Is demo: {traj.is_demo}\")\n",
    "        print(f\"     Length: {len(traj.states)} states, {len(traj.actions)} actions\")\n",
    "        \n",
    "        # Check goal achievement\n",
    "        if traj.train_task_idx is not None:\n",
    "            task = train_tasks[traj.train_task_idx]\n",
    "            goal_achieved = task.goal_holds(traj.states[-1])\n",
    "            print(f\"     Goal achieved: {goal_achieved}\")\n",
    "    \n",
    "    print(f\"\\nüìà Dataset Statistics:\")\n",
    "    print(f\"   Total states across all trajectories: {total_states}\")\n",
    "    print(f\"   Total actions across all trajectories: {total_actions}\")\n",
    "    print(f\"   Average trajectory length: {total_actions/len(dataset.trajectories):.1f} actions\")\n",
    "    \n",
    "    print(f\"\\nüí° This is the exact dataset structure that IVNTR uses for learning!\")\n",
    "    print(f\"   Each trajectory provides state-action pairs with ground truth predicate labels.\")\n",
    "    print(f\"   Neural networks will learn to predict predicate values from state features.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during batch demonstration generation: {e}\")\n",
    "    dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored IVNTR's demonstration data generation using the actual implementation:\n",
    "\n",
    "### ü§ñ **The Generation Process**\n",
    "- **Oracle Approach**: Ground truth planner with perfect domain knowledge\n",
    "- **CogMan**: Cognitive manager that orchestrates planning and execution\n",
    "- **Episode Execution**: `run_episode_and_get_states()` collects state-action trajectories\n",
    "- **Metadata Addition**: Trajectories marked as demonstrations with task indices\n",
    "\n",
    "### üìä **Data Structures**\n",
    "- **Dataset**: Collection of LowLevelTrajectory objects with optional annotations\n",
    "- **LowLevelTrajectory**: State sequences, action sequences, and demonstration metadata\n",
    "- **State**: Continuous feature dictionaries for all objects in the environment\n",
    "- **Action**: 10-dimensional numpy arrays encoding control decisions\n",
    "\n",
    "### üéØ **Key Implementation Details**\n",
    "- **Option Removal**: Oracle options removed from actions to prevent cheating (lines 247-251)\n",
    "- **Goal Verification**: Each demonstration must achieve the task goal (lines 232-234)\n",
    "- **Batch Processing**: Multiple tasks processed to create diverse training data\n",
    "- **Error Handling**: Robust handling of planning failures and timeouts\n",
    "\n",
    "### üí° **The Learning Foundation**\n",
    "This demonstration data provides the foundation for IVNTR's neural predicate learning:\n",
    "- **Supervised Signal**: Ground truth predicate labels computed from oracle knowledge\n",
    "- **State Features**: Continuous observations that neural networks must learn from\n",
    "- **Task Diversity**: Multiple scenarios ensure robust generalization\n",
    "- **Structured Format**: Consistent data organization enables systematic learning\n",
    "\n",
    "The `_generate_demonstrations()` function is the bridge between expert knowledge (oracle approach) and neural learning (predicate classifiers), providing IVNTR with the training data needed to learn symbolic abstractions from continuous observations.\n",
    "\n",
    "---\n",
    "\n",
    "**Next: `04_bilevel_learning.ipynb` - The IVNTR Learning Algorithm**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}