{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bilevel-intro",
   "metadata": {},
   "source": [
    "# IVNTR Bilevel Learning: Neural Predicate Invention\n",
    "\n",
    "This notebook explores the core IVNTR algorithm implemented in `predicators/approaches/bilevel_learning_approach.py`. IVNTR (bIleVel learNing from TRansitions) learns neural predicates that enable symbolic planning from demonstration data.\n",
    "\n",
    "## The Core Algorithm\n",
    "\n",
    "IVNTR implements a **bilevel learning approach** where:\n",
    "\n",
    "1. **Upper Level (Symbolic)**: Searches over candidate predicate effects and constructs NSRTs\n",
    "2. **Lower Level (Neural)**: Learns predicate classifiers using **Any** Neural Networks\n",
    "3. **Alternating Process**: The two levels guide each other through iterative refinement\n",
    "\n",
    "For each predicate, IVNTR alternates between:\n",
    "- **Effect Discovery**: Searching for high-level symbolic effects the predicate should have\n",
    "- **Grounding Learning**: Training neural networks to classify when the predicate holds\n",
    "\n",
    "## Tutorial Focus: \"HasChemX\" Predicate\n",
    "\n",
    "In this tutorial, we'll examine the simplest predicate invention process using the \"HasChemX\" predicate:\n",
    "- **Purpose**: Determines if an object has been shot with Chemical X\n",
    "- **Input**: Continous feature vectors of satellites and objects (position, chemical states, ...)\n",
    "- **Output**: Binary classification (has ChemX or not)\n",
    "- **Role**: Required precondition for camera readings\n",
    "\n",
    "## What We'll Cover\n",
    "\n",
    "1. **Dummy NSRTs**: Starting point with incomplete effect knowledge.\n",
    "2. **Demonstration Data**: Training examples from oracle trajectories.\n",
    "3. **Neural Learning Process**: If we have perfect AE vector, how to train a neural network (single iteration).\n",
    "4. **Symbolic Search Process**: If we do not know the AE vector, how to use neural loss to guide the search (multiple iterations).\n",
    "5. **Advanced In-depth Highlight**: Binary predicate, variable bindings, quantifiers, and more.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Bilevel Learning Environment\n",
    "\n",
    "We'll recreate the bilevel learning setup from `bilevel_learning_approach.py`, focusing on the key components that IVNTR needs for predicate invention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful!\n",
      "✅ IVNTR Bilevel Learning Setup Complete!\n",
      "Environment: satellites\n",
      "Training tasks: 50\n",
      "Demonstration trajectories: 50\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Add the project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Set FastDownward path\n",
    "FD_EXEC_PATH = os.path.join(os.path.dirname(os.path.abspath('.')), 'ext', 'downward')\n",
    "os.environ['FD_EXEC_PATH'] = FD_EXEC_PATH\n",
    "\n",
    "# Import IVNTR components\n",
    "from predicators.envs.satellites import SatellitesEnv\n",
    "from predicators.ground_truth_models import get_dummy_nsrts, get_gt_options\n",
    "from predicators.datasets.demo_only import _generate_demonstrations\n",
    "from predicators.approaches import create_approach\n",
    "from predicators import utils\n",
    "from predicators.settings import CFG\n",
    "\n",
    "config_path = os.path.abspath(\"example_gt_vec.yaml\") \n",
    "log_file_path = os.path.abspath(\"example_gt_vec_learning.log\")\n",
    "neupi_save_path = os.path.abspath(\"saved_neural\")\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "\n",
    "# Test the configuration section\n",
    "utils.reset_config({\n",
    "    \"device\": \"cpu\",\n",
    "    \"env\": \"satellites\",\n",
    "    \"approach\": \"ivntr\",\n",
    "    \"neupi_pred_config\": config_path,\n",
    "    \"neupi_gt_ae_matrix\": True,\n",
    "    \"excluded_predicates\": \"ViewClear,IsCalibrated,HasChemX,HasChemY,Sees\",\n",
    "    \"neupi_do_normalization\": True,\n",
    "    \"num_train_tasks\": 50,\n",
    "    \"num_test_tasks\": 20,\n",
    "    \"seed\": 0,\n",
    "    \"bilevel_plan_without_sim\": False,\n",
    "    \"exclude_domain_feat\": None,\n",
    "    \"log_file\": log_file_path,\n",
    "})\n",
    "\n",
    "handlers = [logging.StreamHandler()]\n",
    "handlers.append(logging.FileHandler(CFG.log_file, mode='w'))\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(message)s\",\n",
    "                    handlers=handlers,\n",
    "                    force=True)\n",
    "\n",
    "CFG.seed = 42\n",
    "CFG.num_train_tasks = 50  # Generate 50 demonstration trajectories\n",
    "CFG.satellites_num_sat_train = [2, 3]\n",
    "CFG.satellites_num_obj_train = [2, 3]\n",
    "CFG.timeout = 10.0\n",
    "CFG.demonstrator = \"oracle\"\n",
    "CFG.max_initial_demos = 50\n",
    "\n",
    "print(\"✅ IVNTR Bilevel Learning Setup Complete!\")\n",
    "print(f\"Environment: {CFG.env}\")\n",
    "print(f\"Training tasks: {CFG.num_train_tasks}\")\n",
    "print(f\"Demonstration trajectories: {CFG.max_initial_demos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dummy-nsrts-section",
   "metadata": {},
   "source": [
    "## 2. Understanding Dummy NSRTs\n",
    "\n",
    "IVNTR starts with **dummy NSRTs** - symbolic operators with incomplete knowledge. These are defined in `predicators/ground_truth_models/satellites/dummy_nsrts.py` and serve as the starting point for predicate invention.\n",
    "\n",
    "### Key Characteristics of Dummy NSRTs:\n",
    "- **Known preconditions**: Static predicates we can observe (e.g., `HasCamera`, `ShootsChemX`)\n",
    "- **Unknown effects**: Dynamic predicates we need to learn (e.g., `HasChemX`, `IsCalibrated`)\n",
    "- **Placeholder structure**: Provides the symbolic scaffolding for learning\n",
    "\n",
    "Let's examine these dummy NSRTs and understand what IVNTR needs to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "load-dummy-nsrts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 Dummy NSRTs: 8 operators\n",
      "\n",
      "📋 Calibrate\n",
      "   Parameters: ['?sat:satellite', '?obj:object']\n",
      "   Known Preconditions:\n",
      "     ✓ CalibrationTarget(?sat:satellite, ?obj:object)\n",
      "   Known Add Effects: None (TO BE LEARNED!)\n",
      "\n",
      "📋 ShootChemX\n",
      "   Parameters: ['?sat:satellite', '?obj:object']\n",
      "   Known Preconditions:\n",
      "     ✓ ShootsChemX(?sat:satellite)\n",
      "   Known Add Effects: None (TO BE LEARNED!)\n",
      "\n",
      "📋 TakeCameraReading\n",
      "   Parameters: ['?sat:satellite', '?obj:object']\n",
      "   Known Preconditions:\n",
      "     ✓ HasCamera(?sat:satellite)\n",
      "   Known Add Effects:\n",
      "     + CameraReadingTaken(?sat:satellite, ?obj:object)\n",
      "\n",
      "✅ Dummy NSRTs loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test the dummy NSRTs section\n",
    "# Create environment and load dummy NSRTs\n",
    "env = SatellitesEnv(use_gui=False)\n",
    "train_tasks = env.get_train_tasks()\n",
    "\n",
    "# Get ground truth options (needed for dummy NSRTs)\n",
    "options = get_gt_options(env.get_name())\n",
    "\n",
    "# Load dummy NSRTs (the incomplete starting point)\n",
    "# get_dummy_nsrts takes: env_name, predicates_to_keep, options_to_keep\n",
    "dummy_nsrts = get_dummy_nsrts(env.get_name(), env.predicates, options)\n",
    "\n",
    "print(f\"🧩 Dummy NSRTs: {len(dummy_nsrts)} operators\\n\")\n",
    "\n",
    "# Test just the first few NSRTs to verify structure\n",
    "for nsrt in sorted(list(dummy_nsrts)[:3], key=lambda x: x.name):\n",
    "    print(f\"📋 {nsrt.name}\")\n",
    "    print(f\"   Parameters: {[str(p) for p in nsrt.parameters]}\")\n",
    "    \n",
    "    if nsrt.preconditions:\n",
    "        print(f\"   Known Preconditions:\")\n",
    "        for pre in nsrt.preconditions:\n",
    "            print(f\"     ✓ {pre}\")\n",
    "    else:\n",
    "        print(f\"   Known Preconditions: None\")\n",
    "    \n",
    "    if nsrt.add_effects:\n",
    "        print(f\"   Known Add Effects:\")\n",
    "        for eff in nsrt.add_effects:\n",
    "            print(f\"     + {eff}\")\n",
    "    else:\n",
    "        print(f\"   Known Add Effects: None (TO BE LEARNED!)\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"✅ Dummy NSRTs loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-data-section",
   "metadata": {},
   "source": [
    "## 3. Generating Demonstration Data\n",
    "\n",
    "IVNTR learns from demonstration trajectories collected using the oracle approach. These trajectories contain the training signal for both effect discovery and neural predicate learning.\n",
    "\n",
    "Let's generate 50 demonstration trajectories using the process from `03_demo_trajectories.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "generate-demonstrations",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 8 NSRTs: {NSRT-TakeInfraredReading:\n",
      "    Parameters: [?sat:satellite, ?obj:object]\n",
      "    Preconditions: [HasChemY(?obj:object), HasInfrared(?sat:satellite), IsCalibrated(?sat:satellite), Sees(?sat:satellite, ?obj:object)]\n",
      "    Add Effects: [InfraredReadingTaken(?sat:satellite, ?obj:object)]\n",
      "    Delete Effects: []\n",
      "    Ignore Effects: []\n",
      "    Option Spec: UseInfraRed(?sat:satellite, ?obj:object), NSRT-ShootChemY:\n",
      "    Parameters: [?sat:satellite, ?obj:object]\n",
      "    Preconditions: [Sees(?sat:satellite, ?obj:object), ShootsChemY(?sat:satellite)]\n",
      "    Add Effects: [HasChemY(?obj:object)]\n",
      "    Delete Effects: []\n",
      "    Ignore Effects: []\n",
      "    Option Spec: ShootChemY(?sat:satellite, ?obj:object), NSRT-MoveTo:\n",
      "    Parameters: [?sat:satellite, ?obj:object]\n",
      "    Preconditions: [ViewClear(?sat:satellite)]\n",
      "    Add Effects: [Sees(?sat:satellite, ?obj:object)]\n",
      "    Delete Effects: [ViewClear(?sat:satellite)]\n",
      "    Ignore Effects: []\n",
      "    Option Spec: MoveTo(?sat:satellite, ?obj:object), NSRT-TakeCameraReading:\n",
      "    Parameters: [?sat:satellite, ?obj:object]\n",
      "    Preconditions: [HasCamera(?sat:satellite), HasChemX(?obj:object), IsCalibrated(?sat:satellite), Sees(?sat:satellite, ?obj:object)]\n",
      "    Add Effects: [CameraReadingTaken(?sat:satellite, ?obj:object)]\n",
      "    Delete Effects: []\n",
      "    Ignore Effects: []\n",
      "    Option Spec: UseCamera(?sat:satellite, ?obj:object), NSRT-ShootChemX:\n",
      "    Parameters: [?sat:satellite, ?obj:object]\n",
      "    Preconditions: [Sees(?sat:satellite, ?obj:object), ShootsChemX(?sat:satellite)]\n",
      "    Add Effects: [HasChemX(?obj:object)]\n",
      "    Delete Effects: []\n",
      "    Ignore Effects: []\n",
      "    Option Spec: ShootChemX(?sat:satellite, ?obj:object), NSRT-Calibrate:\n",
      "    Parameters: [?sat:satellite, ?obj:object]\n",
      "    Preconditions: [CalibrationTarget(?sat:satellite, ?obj:object), Sees(?sat:satellite, ?obj:object)]\n",
      "    Add Effects: [IsCalibrated(?sat:satellite)]\n",
      "    Delete Effects: []\n",
      "    Ignore Effects: []\n",
      "    Option Spec: Calibrate(?sat:satellite, ?obj:object), NSRT-TakeGeigerReading:\n",
      "    Parameters: [?sat:satellite, ?obj:object]\n",
      "    Preconditions: [HasGeiger(?sat:satellite), IsCalibrated(?sat:satellite), Sees(?sat:satellite, ?obj:object)]\n",
      "    Add Effects: [GeigerReadingTaken(?sat:satellite, ?obj:object)]\n",
      "    Delete Effects: []\n",
      "    Ignore Effects: []\n",
      "    Option Spec: UseGeiger(?sat:satellite, ?obj:object), NSRT-MoveAway:\n",
      "    Parameters: [?sat:satellite, ?obj:object]\n",
      "    Preconditions: [Sees(?sat:satellite, ?obj:object)]\n",
      "    Add Effects: [ViewClear(?sat:satellite)]\n",
      "    Delete Effects: [Sees(?sat:satellite, ?obj:object)]\n",
      "    Ignore Effects: []\n",
      "    Option Spec: MoveAway(?sat:satellite, ?obj:object)}\n",
      "Rows of the AE Matrix\n",
      "Calibrate(satellite0, object0)\n",
      "MoveAway(satellite0, object0)\n",
      "MoveTo(satellite0, object0)\n",
      "ShootChemX(satellite0, object0)\n",
      "ShootChemY(satellite0, object0)\n",
      "UseCamera(satellite0, object0)\n",
      "UseGeiger(satellite0, object0)\n",
      "UseInfraRed(satellite0, object0)\n",
      "Name and Columns of the AE Matrix (1: Add, 2: Del):\n",
      "InfraredReadingTaken(0, 0) ([Type(name='satellite'), Type(name='object')]): [0, 0, 0, 0, 0, 0, 0, 1]\n",
      "HasChemY(0) ([Type(name='object')]): [0, 0, 0, 0, 1, 0, 0, 0]\n",
      "Sees(0, 0) ([Type(name='satellite'), Type(name='object')]): [0, 2, 1, 0, 0, 0, 0, 0]\n",
      "ViewClear(0) ([Type(name='satellite')]): [0, 1, 2, 0, 0, 0, 0, 0]\n",
      "CameraReadingTaken(0, 0) ([Type(name='satellite'), Type(name='object')]): [0, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HasChemX(0) ([Type(name='object')]): [0, 0, 0, 1, 0, 0, 0, 0]\n",
      "IsCalibrated(0) ([Type(name='satellite')]): [1, 0, 0, 0, 0, 0, 0, 0]\n",
      "GeigerReadingTaken(0, 0) ([Type(name='satellite'), Type(name='object')]): [0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[CogMan] Reset called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎬 Generating Demonstration Trajectories...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset Generated Successfully!\n",
      "   Total trajectories: 50\n",
      "   All demonstrations: True\n",
      "\n",
      "📊 Dataset Statistics:\n",
      "   Total action steps: 637\n",
      "   Average trajectory length: 12.7 steps\n",
      "   Successful demonstrations: 50/50\n",
      "✅ Demonstration generation successful!\n",
      "\n",
      "🔍 Sample Trajectory Analysis:\n",
      "   Task index: 0\n",
      "   Length: 11 states, 10 actions\n",
      "   Goal: {CameraReadingTaken(sat0:satellite, obj2:object), InfraredReadingTaken(sat1:satellite, obj2:object)}\n",
      "   Action sequence preview:\n",
      "     Step 0: MoveTo with params [0.0521853  0.65423506]\n",
      "     Step 1: Calibrate with params []\n",
      "     Step 2: ShootChemY with params []\n",
      "     Step 3: UseInfraRed with params []\n",
      "     Step 4: MoveTo with params [0.7846738 0.7001005]\n",
      "     ... (5 more actions)\n",
      "\n",
      "💡 This demonstration data provides the training signal for IVNTR's bilevel learning!\n"
     ]
    }
   ],
   "source": [
    "# Test the demonstration generation section (with a smaller number for testing)\n",
    "print(\"🎬 Generating Demonstration Trajectories...\\n\")\n",
    "\n",
    "# This replicates the _generate_demonstrations function from demo_only.py\n",
    "# Note: We need to provide all required arguments\n",
    "training_tasks = [task.task for task in train_tasks]  # Use only first 5 tasks for testing\n",
    "dataset = _generate_demonstrations(\n",
    "    env,\n",
    "    training_tasks,\n",
    "    options,          # Set of ground truth options\n",
    "    0,               # train_tasks_start_idx \n",
    "    False            # annotate_with_gt_ops\n",
    ")\n",
    "\n",
    "print(f\"✅ Dataset Generated Successfully!\")\n",
    "print(f\"   Total trajectories: {len(dataset.trajectories)}\")\n",
    "print(f\"   All demonstrations: {all(traj.is_demo for traj in dataset.trajectories)}\")\n",
    "\n",
    "# Analyze the demonstration data\n",
    "total_steps = sum(len(traj.actions) for traj in dataset.trajectories)\n",
    "avg_length = total_steps / len(dataset.trajectories) if dataset.trajectories else 0\n",
    "successful_demos = sum(1 for traj in dataset.trajectories \n",
    "                      if traj.train_task_idx is not None)\n",
    "\n",
    "print(f\"\\n📊 Dataset Statistics:\")\n",
    "print(f\"   Total action steps: {total_steps}\")\n",
    "print(f\"   Average trajectory length: {avg_length:.1f} steps\")\n",
    "print(f\"   Successful demonstrations: {successful_demos}/{len(dataset.trajectories)}\")\n",
    "\n",
    "print(\"✅ Demonstration generation successful!\")\n",
    "\n",
    "# Sample a trajectory to examine structure (if we have any)\n",
    "if dataset.trajectories:\n",
    "    sample_traj = dataset.trajectories[0]\n",
    "    sample_task = train_tasks[sample_traj.train_task_idx]\n",
    "\n",
    "    print(f\"\\n🔍 Sample Trajectory Analysis:\")\n",
    "    print(f\"   Task index: {sample_traj.train_task_idx}\")\n",
    "    print(f\"   Length: {len(sample_traj.states)} states, {len(sample_traj.actions)} actions\")\n",
    "    print(f\"   Goal: {sample_task.goal}\")\n",
    "    print(f\"   Action sequence preview:\")\n",
    "\n",
    "    for i, action in enumerate(sample_traj.actions[:5]):\n",
    "        if hasattr(action, '_option') and action._option:\n",
    "            print(f\"     Step {i}: {action.get_option().name} with params {action.get_option().params}\")\n",
    "        else:\n",
    "            print(f\"     Step {i}: Raw action {action.arr[:4]}...\")\n",
    "\n",
    "    if len(sample_traj.actions) > 5:\n",
    "        print(f\"     ... ({len(sample_traj.actions)-5} more actions)\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No trajectories generated - check configuration\")\n",
    "\n",
    "print(\"\\n💡 This demonstration data provides the training signal for IVNTR's bilevel learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7gprnllk1rm",
   "metadata": {},
   "source": [
    "## 4. Neural Predicate Training: \"IsCalibrated\" Example\n",
    "\n",
    "Now we'll dive into the core of IVNTR's neural predicate learning using the \"IsCalibrated\" predicate as our example. This predicate is crucial because:\n",
    "\n",
    "- **Purpose**: Determines if a satellite has been calibrated and ready to take readings\n",
    "- **Input**: Satellite features (position, calibration state, instrument type)\n",
    "- **Output**: Binary classification (calibrated or not)\n",
    "- **Role**: Required precondition for all reading operations\n",
    "\n",
    "We'll demonstrate how IVNTR learns this predicate given a known **effect vector** that specifies which actions add/delete the predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "89rj4q5nngo",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Options (Clusters): Calibrate, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): MoveAway, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): MoveTo, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): ShootChemX, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): ShootChemY, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): UseCamera, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): UseGeiger, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): UseInfraRed, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Using: satellite_0\n",
      "Predicate Type TOBE Invented: neural_u_p1\n",
      "Learning Conifg: {'name': 'neural_u_p1', 'types': ['satellite'], 'gt': [[1, 0, 0, 0, 0, 0, 0, 0]], 'ent_idx': [0], 'architecture': {'type': 'MLP', 'layer_size': 32, 'initializer': 'xavier'}, 'optimizer': {'type': 'AdamW', 'kwargs': {'lr': 0.001}}, 'lr_scheduler': {'type': 'StepLR', 'kwargs': {'step_size': 70, 'gamma': 0.1}}, 'batch_vect_num': 12, 'batch_size': 512, 'epochs': 100, 'gumbel_temp': 0.66, 'val_freq': 10, 'num_iter': 5, 'matrix_vec_try': 100, 'search_tree_max_level': 1, 'guidance_thresh': 0.05, 'loss_thresh': 0.005, 'skip_train': False}\n",
      "GT Provided for Predicate neural_u_p1!\n",
      "GT AE Vector: [1, 0, 0, 0, 0, 0, 0, 0]\n",
      "/Users/libowen/Documents/Research/RSS2025/code/IVNTR/predicators/gnn/neupi_utils.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cate_vector_sampled = torch.tensor(one).unsqueeze(0)\n",
      "Search Region Not Provided, Search All possible effects\n",
      "Quantified Decision Boundary Not Provided, Use 0.5\n",
      "Belived Ent Idx for Predicate: CameraReadingTaken\n",
      "Using: satellite_0\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): CameraReadingTaken\n",
      "Belived Ent Idx for Predicate: GeigerReadingTaken\n",
      "Using: satellite_0\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): GeigerReadingTaken\n",
      "Belived Ent Idx for Predicate: HasChemX\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): HasChemX\n",
      "Belived Ent Idx for Predicate: HasChemY\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): HasChemY\n",
      "Belived Ent Idx for Predicate: InfraredReadingTaken\n",
      "Using: satellite_0\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): InfraredReadingTaken\n",
      "Belived Ent Idx for Predicate: IsCalibrated\n",
      "Using: satellite_0\n",
      "Provided Predicate (No Learning): IsCalibrated\n",
      "Belived Ent Idx for Predicate: Sees\n",
      "Using: satellite_0\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): Sees\n",
      "Belived Ent Idx for Predicate: ViewClear\n",
      "Using: satellite_0\n",
      "Provided Predicate (No Learning): ViewClear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Creating IVNTR Bilevel Learning Approach...\n",
      "\n",
      "✅ Approach created: BilevelLearningApproach\n",
      "   Approach name: ivntr\n",
      "\n",
      "📦 Action Options in IVNTR (sorted order):\n",
      "    0: Calibrate\n",
      "    1: MoveAway\n",
      "    2: MoveTo\n",
      "    3: ShootChemX\n",
      "    4: ShootChemY\n",
      "    5: UseCamera\n",
      "    6: UseGeiger\n",
      "    7: UseInfraRed\n",
      "\n",
      "🎯 Understanding Effect Vectors:\n",
      "   Effect vectors are 8-dimensional binary vectors\n",
      "   Each position corresponds to an action option above\n",
      "   Value 1 = action ADDS the predicate\n",
      "   Value 0 = action has no effect on the predicate\n",
      "   Value -1 = action DELETES the predicate\n",
      "\n",
      "💡 This ordering is crucial for interpreting effect vectors in example_gt_vec.yaml!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create the IVNTR Bilevel Learning Approach\n",
    "print(\"🧠 Creating IVNTR Bilevel Learning Approach...\\n\")\n",
    "\n",
    "# Create the approach using the same process as main.py\n",
    "approach = create_approach(\n",
    "    CFG.approach,          # \"ivntr\"\n",
    "    env.predicates,        # Initial predicates \n",
    "    options,               # Ground truth options\n",
    "    env.types,             # Environment types\n",
    "    env.action_space,      # Action space\n",
    "    training_tasks         # Training tasks\n",
    ")\n",
    "\n",
    "print(f\"✅ Approach created: {type(approach).__name__}\")\n",
    "print(f\"   Approach name: {approach.get_name()}\")\n",
    "\n",
    "# Step 2: Examine the sorted options for effect vector understanding\n",
    "print(f\"\\n📦 Action Options in IVNTR (sorted order):\")\n",
    "sorted_options = approach._sorted_options\n",
    "for i, option in enumerate(sorted_options):\n",
    "    print(f\"   {i:2d}: {option.name}\")\n",
    "\n",
    "print(f\"\\n🎯 Understanding Effect Vectors:\")\n",
    "print(f\"   Effect vectors are {len(sorted_options)}-dimensional binary vectors\")\n",
    "print(f\"   Each position corresponds to an action option above\")\n",
    "print(f\"   Value 1 = action ADDS the predicate\")\n",
    "print(f\"   Value 0 = action has no effect on the predicate\") \n",
    "print(f\"   Value -1 = action DELETES the predicate\")\n",
    "\n",
    "print(f\"\\n💡 This ordering is crucial for interpreting effect vectors in example_gt_vec.yaml!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mwxkmrxm7n",
   "metadata": {},
   "source": [
    "### Understanding the Effect Vector for IsCalibrated\n",
    "\n",
    "The effect vector `[1, 0, 0, 0, 0, 0, 0, 0]` from `example_gt_vec.yaml` tells us:\n",
    "\n",
    "1. **Most actions (positions 1-7) have no effect** on IsCalibrated (value 0)\n",
    "2. **One action (position 0) DELETES IsCalibrated** (value -1)\n",
    "\n",
    "This is a **partial effect vector** - it only shows delete effects, not add effects. In reality:\n",
    "- **Calibrate action** should ADD IsCalibrated (but it's not in this vector)\n",
    "- **Use actions** (UseGeiger, UseCamera, UseInfraRed) DELETE IsCalibrated after taking readings\n",
    "\n",
    "### The Learning Process\n",
    "\n",
    "Given this effect vector, IVNTR will:\n",
    "1. **Generate training labels** using the effect vector and demonstration trajectories\n",
    "2. **Train a neural network** to predict IsCalibrated from satellite features  \n",
    "3. **Validate the classifier** on held-out data\n",
    "\n",
    "Let's see this process in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fpw0dul0y0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyzing Updated IsCalibrated Effect Vector...\n",
      "\n",
      "📋 Updated IsCalibrated Effect Vector: [1, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "📊 Effect Distribution Analysis:\n",
      "    0: Calibrate       → ADDS IsCalibrated ⭐\n",
      "    1: MoveAway        → no effect IsCalibrated\n",
      "    2: MoveTo          → no effect IsCalibrated\n",
      "    3: ShootChemX      → no effect IsCalibrated\n",
      "    4: ShootChemY      → no effect IsCalibrated\n",
      "    5: UseCamera       → no effect IsCalibrated\n",
      "    6: UseGeiger       → no effect IsCalibrated\n",
      "    7: UseInfraRed     → no effect IsCalibrated\n",
      "\n",
      "🎯 Key Insight: Calibrate action (position 0) ADDS IsCalibrated!\n",
      "   This makes perfect sense - calibration sets the satellite as ready\n",
      "   All other actions have no effect on IsCalibrated state\n"
     ]
    }
   ],
   "source": [
    "# Test the updated effect vector analysis\n",
    "print(\"🔍 Analyzing Updated IsCalibrated Effect Vector...\\n\")\n",
    "\n",
    "# From updated example_gt_vec.yaml: gt_ae_vecs: [[1, 0, 0, 0, 0, 0, 0, 0]]\n",
    "# This shows the Calibrate action ADDS IsCalibrated\n",
    "updated_effect_vector = [1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "print(f\"📋 Updated IsCalibrated Effect Vector: {updated_effect_vector}\")\n",
    "print(f\"\\n📊 Effect Distribution Analysis:\")\n",
    "\n",
    "# Analyze each action's effect on IsCalibrated with the updated vector\n",
    "for i, (option, effect) in enumerate(zip(sorted_options, updated_effect_vector)):\n",
    "    effect_str = {1: \"ADDS\", 0: \"no effect\", -1: \"DELETES\"}[effect]\n",
    "    if effect != 0:\n",
    "        print(f\"   {i:2d}: {option.name:<15} → {effect_str} IsCalibrated ⭐\")\n",
    "    else:\n",
    "        print(f\"   {i:2d}: {option.name:<15} → {effect_str} IsCalibrated\")\n",
    "\n",
    "print(f\"\\n🎯 Key Insight: Calibrate action (position 0) ADDS IsCalibrated!\")\n",
    "print(f\"   This makes perfect sense - calibration sets the satellite as ready\")\n",
    "print(f\"   All other actions have no effect on IsCalibrated state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "s17fvq8y5v",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Constructing NeuPi Data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Starting Neural Predicate Learning Process...\n",
      "\n",
      "📋 Learning Process Overview:\n",
      "   1. Generate training data from demonstration trajectories\n",
      "   2. Setup input fields for neural predicates\n",
      "   3. Initialize Action-Effect (AE) matrix constraints\n",
      "   4. Compute input normalizers for stable training\n",
      "   5. Train neural networks using the effect vector as ground truth\n",
      "   6. Validate learned predicates on held-out data\n",
      "\n",
      "🚀 Calling approach.learn_neural_predicates(dataset)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 150.00it/s]\n",
      "Low-level feat not changed for Predicate HasChemX in Row 0\n",
      "Low-level feat not changed for Predicate HasChemX in Row 0\n",
      "Low-level feat not changed for Predicate HasChemY in Row 0\n",
      "Low-level feat not changed for Predicate HasChemY in Row 0\n",
      "Low-level feat not changed for Predicate HasChemX in Row 1\n",
      "Low-level feat not changed for Predicate HasChemX in Row 1\n",
      "Low-level feat not changed for Predicate HasChemY in Row 1\n",
      "Low-level feat not changed for Predicate HasChemY in Row 1\n",
      "Low-level feat not changed for Predicate HasChemX in Row 2\n",
      "Low-level feat not changed for Predicate HasChemX in Row 2\n",
      "Low-level feat not changed for Predicate HasChemY in Row 2\n",
      "Low-level feat not changed for Predicate HasChemY in Row 2\n",
      "Low-level feat not changed for Predicate neural_u_p1 in Row 3\n",
      "Low-level feat not changed for Predicate neural_u_p1 in Row 3\n",
      "Low-level feat not changed for Predicate IsCalibrated in Row 3\n",
      "Low-level feat not changed for Predicate IsCalibrated in Row 3\n",
      "Low-level feat not changed for Predicate ViewClear in Row 3\n",
      "Low-level feat not changed for Predicate ViewClear in Row 3\n",
      "Low-level feat not changed for Predicate neural_u_p1 in Row 4\n",
      "Low-level feat not changed for Predicate neural_u_p1 in Row 4\n",
      "Low-level feat not changed for Predicate IsCalibrated in Row 4\n",
      "Low-level feat not changed for Predicate IsCalibrated in Row 4\n",
      "Low-level feat not changed for Predicate ViewClear in Row 4\n",
      "Low-level feat not changed for Predicate ViewClear in Row 4\n",
      "Low-level feat not changed for Predicate HasChemX in Row 5\n",
      "Low-level feat not changed for Predicate HasChemX in Row 5\n",
      "Low-level feat not changed for Predicate HasChemY in Row 5\n",
      "Low-level feat not changed for Predicate HasChemY in Row 5\n",
      "Low-level feat not changed for Predicate HasChemX in Row 6\n",
      "Low-level feat not changed for Predicate HasChemX in Row 6\n",
      "Low-level feat not changed for Predicate HasChemY in Row 6\n",
      "Low-level feat not changed for Predicate HasChemY in Row 6\n",
      "Low-level feat not changed for Predicate HasChemX in Row 7\n",
      "Low-level feat not changed for Predicate HasChemX in Row 7\n",
      "Low-level feat not changed for Predicate HasChemY in Row 7\n",
      "Low-level feat not changed for Predicate HasChemY in Row 7\n",
      "Predicate neural_u_p1 Not Provided for action Calibrate in Data\n",
      "Predicate neural_u_p1 Not Provided for action MoveAway in Data\n",
      "Predicate neural_u_p1 Not Provided for action MoveTo in Data\n",
      "Predicate neural_u_p1 Not Provided for action ShootChemX in Data\n",
      "Predicate neural_u_p1 Not Provided for action ShootChemY in Data\n",
      "Action UseCamera Definitely has no effect for pred neural_u_p1\n",
      "Action UseGeiger Definitely has no effect for pred neural_u_p1\n",
      "Action UseInfraRed Definitely has no effect for pred neural_u_p1\n",
      "Action Calibrate Definitely has no effect for pred CameraReadingTaken\n",
      "Action MoveAway Definitely has no effect for pred CameraReadingTaken\n",
      "Action MoveTo Definitely has no effect for pred CameraReadingTaken\n",
      "Action ShootChemX Definitely has no effect for pred CameraReadingTaken\n",
      "Action ShootChemY Definitely has no effect for pred CameraReadingTaken\n",
      "Action UseCamera Definitely has add effect for pred CameraReadingTaken\n",
      "Action UseGeiger Definitely has no effect for pred CameraReadingTaken\n",
      "Action UseInfraRed Definitely has no effect for pred CameraReadingTaken\n",
      "Action Calibrate Definitely has no effect for pred GeigerReadingTaken\n",
      "Action MoveAway Definitely has no effect for pred GeigerReadingTaken\n",
      "Action MoveTo Definitely has no effect for pred GeigerReadingTaken\n",
      "Action ShootChemX Definitely has no effect for pred GeigerReadingTaken\n",
      "Action ShootChemY Definitely has no effect for pred GeigerReadingTaken\n",
      "Action UseCamera Definitely has no effect for pred GeigerReadingTaken\n",
      "Action UseGeiger Definitely has add effect for pred GeigerReadingTaken\n",
      "Action UseInfraRed Definitely has no effect for pred GeigerReadingTaken\n",
      "Action Calibrate Definitely has no effect for pred HasChemX\n",
      "Action MoveAway Definitely has no effect for pred HasChemX\n",
      "Action MoveTo Definitely has no effect for pred HasChemX\n",
      "Action ShootChemX Definitely has add effect for pred HasChemX\n",
      "Action ShootChemY Definitely has no effect for pred HasChemX\n",
      "Action UseCamera Definitely has no effect for pred HasChemX\n",
      "Action UseGeiger Definitely has no effect for pred HasChemX\n",
      "Action UseInfraRed Definitely has no effect for pred HasChemX\n",
      "Action Calibrate Definitely has no effect for pred HasChemY\n",
      "Action MoveAway Definitely has no effect for pred HasChemY\n",
      "Action MoveTo Definitely has no effect for pred HasChemY\n",
      "Action ShootChemX Definitely has no effect for pred HasChemY\n",
      "Action ShootChemY Definitely has add effect for pred HasChemY\n",
      "Action UseCamera Definitely has no effect for pred HasChemY\n",
      "Action UseGeiger Definitely has no effect for pred HasChemY\n",
      "Action UseInfraRed Definitely has no effect for pred HasChemY\n",
      "Action Calibrate Definitely has no effect for pred InfraredReadingTaken\n",
      "Action MoveAway Definitely has no effect for pred InfraredReadingTaken\n",
      "Action MoveTo Definitely has no effect for pred InfraredReadingTaken\n",
      "Action ShootChemX Definitely has no effect for pred InfraredReadingTaken\n",
      "Action ShootChemY Definitely has no effect for pred InfraredReadingTaken\n",
      "Action UseCamera Definitely has no effect for pred InfraredReadingTaken\n",
      "Action UseGeiger Definitely has no effect for pred InfraredReadingTaken\n",
      "Action UseInfraRed Definitely has add effect for pred InfraredReadingTaken\n",
      "Action Calibrate Definitely has add effect for pred IsCalibrated\n",
      "Action MoveAway Definitely has no effect for pred IsCalibrated\n",
      "Action MoveTo Definitely has no effect for pred IsCalibrated\n",
      "Action ShootChemX Definitely has no effect for pred IsCalibrated\n",
      "Action ShootChemY Definitely has no effect for pred IsCalibrated\n",
      "Action UseCamera Definitely has no effect for pred IsCalibrated\n",
      "Action UseGeiger Definitely has no effect for pred IsCalibrated\n",
      "Action UseInfraRed Definitely has no effect for pred IsCalibrated\n",
      "Action Calibrate Definitely has no effect for pred Sees\n",
      "Action MoveAway Definitely has delete effect for pred Sees\n",
      "Action MoveTo Definitely has add effect for pred Sees\n",
      "Action ShootChemX Definitely has no effect for pred Sees\n",
      "Action ShootChemY Definitely has no effect for pred Sees\n",
      "Action UseCamera Definitely has no effect for pred Sees\n",
      "Action UseGeiger Definitely has no effect for pred Sees\n",
      "Action UseInfraRed Definitely has no effect for pred Sees\n",
      "Action Calibrate Definitely has no effect for pred ViewClear\n",
      "Action MoveAway Definitely has add effect for pred ViewClear\n",
      "Action MoveTo Definitely has delete effect for pred ViewClear\n",
      "Action ShootChemX Definitely has no effect for pred ViewClear\n",
      "Action ShootChemY Definitely has no effect for pred ViewClear\n",
      "Action UseCamera Definitely has no effect for pred ViewClear\n",
      "Action UseGeiger Definitely has no effect for pred ViewClear\n",
      "Action UseInfraRed Definitely has no effect for pred ViewClear\n",
      "Computing Normalizer for Inouts...\n",
      "100%|██████████| 637/637 [00:00<00:00, 2502.65it/s]\n",
      "Skipping learning for CameraReadingTaken since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 0, 0, 0, 1, 0, 0])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for GeigerReadingTaken since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 0, 0, 0, 0, 1, 0])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for InfraredReadingTaken since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 0, 0, 0, 0, 0, 1])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for Sees since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 1, 0, 0, 0, 0, 0])\n",
      "AE Vector (Del): tensor([0, 1, 0, 0, 0, 0, 0, 0])\n",
      "**************Learning Typed Predicate: neural_u_p1**************\n",
      "Learning Config: {'name': 'neural_u_p1', 'types': ['satellite'], 'gt': [[1, 0, 0, 0, 0, 0, 0, 0]], 'ent_idx': [0], 'architecture': {'type': 'MLP', 'layer_size': 32, 'initializer': 'xavier'}, 'optimizer': {'type': 'AdamW', 'kwargs': {'lr': 0.001}}, 'lr_scheduler': {'type': 'StepLR', 'kwargs': {'step_size': 70, 'gamma': 0.1}}, 'batch_vect_num': 12, 'batch_size': 512, 'epochs': 100, 'gumbel_temp': 0.66, 'val_freq': 10, 'num_iter': 5, 'matrix_vec_try': 100, 'search_tree_max_level': 1, 'guidance_thresh': 0.05, 'loss_thresh': 0.005, 'skip_train': False, 'decision_b': 0.5}\n",
      "Created transition dataset with 1 transition pairs\n",
      "/Users/libowen/Documents/Research/RSS2025/code/IVNTR/predicators/gnn/neupi.py:1496: RuntimeWarning: Mean of empty slice.\n",
      "  if (non_zero_guidance.mean() < 0.4) or (non_zero_indexes.size == 0):\n",
      "/Users/libowen/opt/anaconda3/envs/ivntr/lib/python3.8/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Training from scratch.\n",
      "***************Bi-level Optimizing (neural_u_p1)***************\n",
      "-----Iteration: 0 (neural_u_p1)------\n",
      "Generate 12 AE Vectors-Score Pairs for Predicate neural_u_p1\n",
      "GT AE Vec Provided, No Need to Generate AE Vectors.\n",
      "Optimizing 1 Neural Models with AE Vector from BO...\n",
      "*******Vec 0 (neural_u_p1)*******\n",
      "(Add): \n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "(Del): \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Generating Graph Data with Current AE matrix...\n",
      "100%|██████████| 637/637 [00:00<00:00, 1537.44it/s]\n",
      "Created transition dataset with 509 transition pairs\n",
      "Created transition dataset with 128 transition pairs\n",
      "Training Neural Model 0...\n",
      "Predicat 1 Iteration 0 | Starting process 0\n",
      "Iteration 0 Epoch 0/99\n",
      "----------\n",
      "Epoch 0, model processing time: 0.0281 seconds\n",
      "Epoch 0, data loading time: 0.0000 seconds\n",
      "Epoch 0, average training loss (change): 1.4375122785568237\n",
      "Epoch 0, average training loss (non-change): 0.00021723139798268676\n",
      "Found new best model with val loss 100.0at epoch 0\n",
      "Epoch 1, model processing time: 0.0224 seconds\n",
      "Epoch 1, data loading time: 0.0000 seconds\n",
      "Epoch 1, average training loss (change): 1.2234357595443726\n",
      "Epoch 1, average training loss (non-change): 0.0003905600751750171\n",
      "Epoch 2, model processing time: 0.0212 seconds\n",
      "Epoch 2, data loading time: 0.0000 seconds\n",
      "Epoch 2, average training loss (change): 1.0583637952804565\n",
      "Epoch 2, average training loss (non-change): 0.0006300212698988616\n",
      "Epoch 3, model processing time: 0.0248 seconds\n",
      "Epoch 3, data loading time: 0.0000 seconds\n",
      "Epoch 3, average training loss (change): 0.9317415952682495\n",
      "Epoch 3, average training loss (non-change): 0.0008834293112158775\n",
      "Epoch 4, model processing time: 0.0204 seconds\n",
      "Epoch 4, data loading time: 0.0000 seconds\n",
      "Epoch 4, average training loss (change): 0.823565661907196\n",
      "Epoch 4, average training loss (non-change): 0.0012008604826405644\n",
      "Epoch 5, model processing time: 0.0208 seconds\n",
      "Epoch 5, data loading time: 0.0000 seconds\n",
      "Epoch 5, average training loss (change): 0.7323721647262573\n",
      "Epoch 5, average training loss (non-change): 0.0015304596163332462\n",
      "Epoch 6, model processing time: 0.0203 seconds\n",
      "Epoch 6, data loading time: 0.0000 seconds\n",
      "Epoch 6, average training loss (change): 0.6574462652206421\n",
      "Epoch 6, average training loss (non-change): 0.0018116557039320469\n",
      "Epoch 7, model processing time: 0.0203 seconds\n",
      "Epoch 7, data loading time: 0.0000 seconds\n",
      "Epoch 7, average training loss (change): 0.5952582359313965\n",
      "Epoch 7, average training loss (non-change): 0.0020346110686659813\n",
      "Epoch 8, model processing time: 0.0201 seconds\n",
      "Epoch 8, data loading time: 0.0000 seconds\n",
      "Epoch 8, average training loss (change): 0.5414873957633972\n",
      "Epoch 8, average training loss (non-change): 0.002223891206085682\n",
      "Epoch 9, model processing time: 0.0198 seconds\n",
      "Epoch 9, data loading time: 0.0000 seconds\n",
      "Epoch 9, average training loss (change): 0.4930402934551239\n",
      "Epoch 9, average training loss (non-change): 0.0024061081930994987\n",
      "Found new best model with val loss 1.0041134357452393at epoch 9\n",
      "Iteration 0 Epoch 10/99\n",
      "----------\n",
      "Epoch 10, model processing time: 0.0238 seconds\n",
      "Epoch 10, data loading time: 0.0000 seconds\n",
      "Epoch 10, average training loss (change): 0.4483182430267334\n",
      "Epoch 10, average training loss (non-change): 0.00259900139644742\n",
      "Epoch 11, model processing time: 0.0217 seconds\n",
      "Epoch 11, data loading time: 0.0000 seconds\n",
      "Epoch 11, average training loss (change): 0.4066794514656067\n",
      "Epoch 11, average training loss (non-change): 0.00280957855284214\n",
      "Epoch 12, model processing time: 0.0212 seconds\n",
      "Epoch 12, data loading time: 0.0000 seconds\n",
      "Epoch 12, average training loss (change): 0.36791515350341797\n",
      "Epoch 12, average training loss (non-change): 0.0030362862162292004\n",
      "Epoch 13, model processing time: 0.0210 seconds\n",
      "Epoch 13, data loading time: 0.0000 seconds\n",
      "Epoch 13, average training loss (change): 0.3320087194442749\n",
      "Epoch 13, average training loss (non-change): 0.0032721608877182007\n",
      "Epoch 14, model processing time: 0.0200 seconds\n",
      "Epoch 14, data loading time: 0.0000 seconds\n",
      "Epoch 14, average training loss (change): 0.2990759313106537\n",
      "Epoch 14, average training loss (non-change): 0.003504459746181965\n",
      "Epoch 15, model processing time: 0.0200 seconds\n",
      "Epoch 15, data loading time: 0.0000 seconds\n",
      "Epoch 15, average training loss (change): 0.26927077770233154\n",
      "Epoch 15, average training loss (non-change): 0.0037153877783566713\n",
      "Epoch 16, model processing time: 0.0200 seconds\n",
      "Epoch 16, data loading time: 0.0000 seconds\n",
      "Epoch 16, average training loss (change): 0.24257852137088776\n",
      "Epoch 16, average training loss (non-change): 0.003890919266268611\n",
      "Epoch 17, model processing time: 0.0199 seconds\n",
      "Epoch 17, data loading time: 0.0000 seconds\n",
      "Epoch 17, average training loss (change): 0.21869724988937378\n",
      "Epoch 17, average training loss (non-change): 0.004029375500977039\n",
      "Epoch 18, model processing time: 0.0202 seconds\n",
      "Epoch 18, data loading time: 0.0000 seconds\n",
      "Epoch 18, average training loss (change): 0.19722570478916168\n",
      "Epoch 18, average training loss (non-change): 0.00413707597181201\n",
      "Epoch 19, model processing time: 0.0247 seconds\n",
      "Epoch 19, data loading time: 0.0000 seconds\n",
      "Epoch 19, average training loss (change): 0.17786358296871185\n",
      "Epoch 19, average training loss (non-change): 0.004221558570861816\n",
      "Found new best model with val loss 0.5242300629615784at epoch 19\n",
      "Iteration 0 Epoch 20/99\n",
      "----------\n",
      "Epoch 20, model processing time: 0.0201 seconds\n",
      "Epoch 20, data loading time: 0.0000 seconds\n",
      "Epoch 20, average training loss (change): 0.16042457520961761\n",
      "Epoch 20, average training loss (non-change): 0.004288898780941963\n",
      "Epoch 21, model processing time: 0.0201 seconds\n",
      "Epoch 21, data loading time: 0.0000 seconds\n",
      "Epoch 21, average training loss (change): 0.1447875201702118\n",
      "Epoch 21, average training loss (non-change): 0.004342649132013321\n",
      "Epoch 22, model processing time: 0.0200 seconds\n",
      "Epoch 22, data loading time: 0.0000 seconds\n",
      "Epoch 22, average training loss (change): 0.130854532122612\n",
      "Epoch 22, average training loss (non-change): 0.004383298568427563\n",
      "Epoch 23, model processing time: 0.0206 seconds\n",
      "Epoch 23, data loading time: 0.0000 seconds\n",
      "Epoch 23, average training loss (change): 0.11852395534515381\n",
      "Epoch 23, average training loss (non-change): 0.004408654291182756\n",
      "Epoch 24, model processing time: 0.0198 seconds\n",
      "Epoch 24, data loading time: 0.0000 seconds\n",
      "Epoch 24, average training loss (change): 0.1076781153678894\n",
      "Epoch 24, average training loss (non-change): 0.004415163770318031\n",
      "Epoch 25, model processing time: 0.0201 seconds\n",
      "Epoch 25, data loading time: 0.0000 seconds\n",
      "Epoch 25, average training loss (change): 0.09818565845489502\n",
      "Epoch 25, average training loss (non-change): 0.004399645142257214\n",
      "Epoch 26, model processing time: 0.0206 seconds\n",
      "Epoch 26, data loading time: 0.0000 seconds\n",
      "Epoch 26, average training loss (change): 0.08991143852472305\n",
      "Epoch 26, average training loss (non-change): 0.004360638093203306\n",
      "Epoch 27, model processing time: 0.0197 seconds\n",
      "Epoch 27, data loading time: 0.0000 seconds\n",
      "Epoch 27, average training loss (change): 0.08272319287061691\n",
      "Epoch 27, average training loss (non-change): 0.0042990949004888535\n",
      "Epoch 28, model processing time: 0.0198 seconds\n",
      "Epoch 28, data loading time: 0.0000 seconds\n",
      "Epoch 28, average training loss (change): 0.07649171352386475\n",
      "Epoch 28, average training loss (non-change): 0.004218083806335926\n",
      "Epoch 29, model processing time: 0.0211 seconds\n",
      "Epoch 29, data loading time: 0.0000 seconds\n",
      "Epoch 29, average training loss (change): 0.07108918577432632\n",
      "Epoch 29, average training loss (non-change): 0.004121807869523764\n",
      "Found new best model with val loss 0.15163327753543854at epoch 29\n",
      "Iteration 0 Epoch 30/99\n",
      "----------\n",
      "Epoch 30, model processing time: 0.0204 seconds\n",
      "Epoch 30, data loading time: 0.0000 seconds\n",
      "Epoch 30, average training loss (change): 0.06639036536216736\n",
      "Epoch 30, average training loss (non-change): 0.0040146904066205025\n",
      "Epoch 31, model processing time: 0.0202 seconds\n",
      "Epoch 31, data loading time: 0.0000 seconds\n",
      "Epoch 31, average training loss (change): 0.062277160584926605\n",
      "Epoch 31, average training loss (non-change): 0.0039006408769637346\n",
      "Epoch 32, model processing time: 0.0209 seconds\n",
      "Epoch 32, data loading time: 0.0000 seconds\n",
      "Epoch 32, average training loss (change): 0.05864376947283745\n",
      "Epoch 32, average training loss (non-change): 0.0037828288041055202\n",
      "Epoch 33, model processing time: 0.0199 seconds\n",
      "Epoch 33, data loading time: 0.0000 seconds\n",
      "Epoch 33, average training loss (change): 0.05539996922016144\n",
      "Epoch 33, average training loss (non-change): 0.0036636393051594496\n",
      "Epoch 34, model processing time: 0.0200 seconds\n",
      "Epoch 34, data loading time: 0.0000 seconds\n",
      "Epoch 34, average training loss (change): 0.0524727888405323\n",
      "Epoch 34, average training loss (non-change): 0.003544931299984455\n",
      "Epoch 35, model processing time: 0.0199 seconds\n",
      "Epoch 35, data loading time: 0.0000 seconds\n",
      "Epoch 35, average training loss (change): 0.049805305898189545\n",
      "Epoch 35, average training loss (non-change): 0.003428085707128048\n",
      "Epoch 36, model processing time: 0.0204 seconds\n",
      "Epoch 36, data loading time: 0.0000 seconds\n",
      "Epoch 36, average training loss (change): 0.04735470190644264\n",
      "Epoch 36, average training loss (non-change): 0.003314204979687929\n",
      "Epoch 37, model processing time: 0.0197 seconds\n",
      "Epoch 37, data loading time: 0.0000 seconds\n",
      "Epoch 37, average training loss (change): 0.04508981853723526\n",
      "Epoch 37, average training loss (non-change): 0.0032042101956903934\n",
      "Epoch 38, model processing time: 0.0210 seconds\n",
      "Epoch 38, data loading time: 0.0000 seconds\n",
      "Epoch 38, average training loss (change): 0.04298797622323036\n",
      "Epoch 38, average training loss (non-change): 0.003098796820268035\n",
      "Epoch 39, model processing time: 0.0197 seconds\n",
      "Epoch 39, data loading time: 0.0000 seconds\n",
      "Epoch 39, average training loss (change): 0.041032642126083374\n",
      "Epoch 39, average training loss (non-change): 0.0029984693974256516\n",
      "Found new best model with val loss 0.043813399970531464at epoch 39\n",
      "Iteration 0 Epoch 40/99\n",
      "----------\n",
      "Epoch 40, model processing time: 0.0234 seconds\n",
      "Epoch 40, data loading time: 0.0000 seconds\n",
      "Epoch 40, average training loss (change): 0.039211470633745193\n",
      "Epoch 40, average training loss (non-change): 0.0029035997577011585\n",
      "Epoch 41, model processing time: 0.0214 seconds\n",
      "Epoch 41, data loading time: 0.0000 seconds\n",
      "Epoch 41, average training loss (change): 0.03751472756266594\n",
      "Epoch 41, average training loss (non-change): 0.0028143920935690403\n",
      "Epoch 42, model processing time: 0.0199 seconds\n",
      "Epoch 42, data loading time: 0.0000 seconds\n",
      "Epoch 42, average training loss (change): 0.035934124141931534\n",
      "Epoch 42, average training loss (non-change): 0.0027308815624564886\n",
      "Epoch 43, model processing time: 0.0200 seconds\n",
      "Epoch 43, data loading time: 0.0000 seconds\n",
      "Epoch 43, average training loss (change): 0.03446226567029953\n",
      "Epoch 43, average training loss (non-change): 0.002652974333614111\n",
      "Epoch 44, model processing time: 0.0201 seconds\n",
      "Epoch 44, data loading time: 0.0000 seconds\n",
      "Epoch 44, average training loss (change): 0.03309211507439613\n",
      "Epoch 44, average training loss (non-change): 0.00258050300180912\n",
      "Epoch 45, model processing time: 0.0203 seconds\n",
      "Epoch 45, data loading time: 0.0000 seconds\n",
      "Epoch 45, average training loss (change): 0.03181682527065277\n",
      "Epoch 45, average training loss (non-change): 0.0025132258888334036\n",
      "Epoch 46, model processing time: 0.0208 seconds\n",
      "Epoch 46, data loading time: 0.0000 seconds\n",
      "Epoch 46, average training loss (change): 0.030629528686404228\n",
      "Epoch 46, average training loss (non-change): 0.002450774423778057\n",
      "Epoch 47, model processing time: 0.0200 seconds\n",
      "Epoch 47, data loading time: 0.0000 seconds\n",
      "Epoch 47, average training loss (change): 0.029523534700274467\n",
      "Epoch 47, average training loss (non-change): 0.002392869908362627\n",
      "Epoch 48, model processing time: 0.0202 seconds\n",
      "Epoch 48, data loading time: 0.0000 seconds\n",
      "Epoch 48, average training loss (change): 0.028492256999015808\n",
      "Epoch 48, average training loss (non-change): 0.0023391051217913628\n",
      "Epoch 49, model processing time: 0.0209 seconds\n",
      "Epoch 49, data loading time: 0.0000 seconds\n",
      "Epoch 49, average training loss (change): 0.02752925269305706\n",
      "Epoch 49, average training loss (non-change): 0.0022891461849212646\n",
      "Found new best model with val loss 0.024835815653204918at epoch 49\n",
      "Iteration 0 Epoch 50/99\n",
      "----------\n",
      "Epoch 50, model processing time: 0.0203 seconds\n",
      "Epoch 50, data loading time: 0.0000 seconds\n",
      "Epoch 50, average training loss (change): 0.026628464460372925\n",
      "Epoch 50, average training loss (non-change): 0.002242628950625658\n",
      "Epoch 51, model processing time: 0.0205 seconds\n",
      "Epoch 51, data loading time: 0.0000 seconds\n",
      "Epoch 51, average training loss (change): 0.025784190744161606\n",
      "Epoch 51, average training loss (non-change): 0.0021992409601807594\n",
      "Epoch 52, model processing time: 0.0208 seconds\n",
      "Epoch 52, data loading time: 0.0000 seconds\n",
      "Epoch 52, average training loss (change): 0.024991091340780258\n",
      "Epoch 52, average training loss (non-change): 0.0021586385555565357\n",
      "Epoch 53, model processing time: 0.0202 seconds\n",
      "Epoch 53, data loading time: 0.0000 seconds\n",
      "Epoch 53, average training loss (change): 0.024244438856840134\n",
      "Epoch 53, average training loss (non-change): 0.0021205642260611057\n",
      "Epoch 54, model processing time: 0.0206 seconds\n",
      "Epoch 54, data loading time: 0.0000 seconds\n",
      "Epoch 54, average training loss (change): 0.023539897054433823\n",
      "Epoch 54, average training loss (non-change): 0.002084747888147831\n",
      "Epoch 55, model processing time: 0.0203 seconds\n",
      "Epoch 55, data loading time: 0.0000 seconds\n",
      "Epoch 55, average training loss (change): 0.022873638197779655\n",
      "Epoch 55, average training loss (non-change): 0.0020509487949311733\n",
      "Epoch 56, model processing time: 0.0214 seconds\n",
      "Epoch 56, data loading time: 0.0000 seconds\n",
      "Epoch 56, average training loss (change): 0.022242344915866852\n",
      "Epoch 56, average training loss (non-change): 0.0020189746282994747\n",
      "Epoch 57, model processing time: 0.0204 seconds\n",
      "Epoch 57, data loading time: 0.0000 seconds\n",
      "Epoch 57, average training loss (change): 0.02164304442703724\n",
      "Epoch 57, average training loss (non-change): 0.0019886125810444355\n",
      "Epoch 58, model processing time: 0.0202 seconds\n",
      "Epoch 58, data loading time: 0.0000 seconds\n",
      "Epoch 58, average training loss (change): 0.021073266863822937\n",
      "Epoch 58, average training loss (non-change): 0.0019597343634814024\n",
      "Epoch 59, model processing time: 0.0198 seconds\n",
      "Epoch 59, data loading time: 0.0000 seconds\n",
      "Epoch 59, average training loss (change): 0.02053075283765793\n",
      "Epoch 59, average training loss (non-change): 0.00193216057959944\n",
      "Found new best model with val loss 0.01867511309683323at epoch 59\n",
      "Iteration 0 Epoch 60/99\n",
      "----------\n",
      "Epoch 60, model processing time: 0.0202 seconds\n",
      "Epoch 60, data loading time: 0.0000 seconds\n",
      "Epoch 60, average training loss (change): 0.02001360058784485\n",
      "Epoch 60, average training loss (non-change): 0.0019057762110605836\n",
      "Epoch 61, model processing time: 0.0199 seconds\n",
      "Epoch 61, data loading time: 0.0000 seconds\n",
      "Epoch 61, average training loss (change): 0.019520144909620285\n",
      "Epoch 61, average training loss (non-change): 0.0018804458668455482\n",
      "Epoch 62, model processing time: 0.0204 seconds\n",
      "Epoch 62, data loading time: 0.0000 seconds\n",
      "Epoch 62, average training loss (change): 0.019048936665058136\n",
      "Epoch 62, average training loss (non-change): 0.001856092712841928\n",
      "Epoch 63, model processing time: 0.0197 seconds\n",
      "Epoch 63, data loading time: 0.0000 seconds\n",
      "Epoch 63, average training loss (change): 0.01859862729907036\n",
      "Epoch 63, average training loss (non-change): 0.0018326115095987916\n",
      "Epoch 64, model processing time: 0.0200 seconds\n",
      "Epoch 64, data loading time: 0.0000 seconds\n",
      "Epoch 64, average training loss (change): 0.0181680079549551\n",
      "Epoch 64, average training loss (non-change): 0.0018099078442901373\n",
      "Epoch 65, model processing time: 0.0201 seconds\n",
      "Epoch 65, data loading time: 0.0000 seconds\n",
      "Epoch 65, average training loss (change): 0.017755992710590363\n",
      "Epoch 65, average training loss (non-change): 0.0017879018560051918\n",
      "Epoch 66, model processing time: 0.0202 seconds\n",
      "Epoch 66, data loading time: 0.0000 seconds\n",
      "Epoch 66, average training loss (change): 0.01736157201230526\n",
      "Epoch 66, average training loss (non-change): 0.0017665383638814092\n",
      "Epoch 67, model processing time: 0.0198 seconds\n",
      "Epoch 67, data loading time: 0.0000 seconds\n",
      "Epoch 67, average training loss (change): 0.016983767971396446\n",
      "Epoch 67, average training loss (non-change): 0.0017457461217418313\n",
      "Epoch 68, model processing time: 0.0199 seconds\n",
      "Epoch 68, data loading time: 0.0000 seconds\n",
      "Epoch 68, average training loss (change): 0.01662166230380535\n",
      "Epoch 68, average training loss (non-change): 0.0017254494596272707\n",
      "Epoch 69, model processing time: 0.0200 seconds\n",
      "Epoch 69, data loading time: 0.0000 seconds\n",
      "Epoch 69, average training loss (change): 0.01627439633011818\n",
      "Epoch 69, average training loss (non-change): 0.0017056140350177884\n",
      "Found new best model with val loss 0.014838934876024723at epoch 69\n",
      "Iteration 0 Epoch 70/99\n",
      "----------\n",
      "Epoch 70, model processing time: 0.0208 seconds\n",
      "Epoch 70, data loading time: 0.0000 seconds\n",
      "Epoch 70, average training loss (change): 0.015941133722662926\n",
      "Epoch 70, average training loss (non-change): 0.0016861732583492994\n",
      "Epoch 71, model processing time: 0.0198 seconds\n",
      "Epoch 71, data loading time: 0.0000 seconds\n",
      "Epoch 71, average training loss (change): 0.01590879075229168\n",
      "Epoch 71, average training loss (non-change): 0.0016842590412124991\n",
      "Epoch 72, model processing time: 0.0226 seconds\n",
      "Epoch 72, data loading time: 0.0000 seconds\n",
      "Epoch 72, average training loss (change): 0.015876971185207367\n",
      "Epoch 72, average training loss (non-change): 0.001682346686720848\n",
      "Epoch 73, model processing time: 0.0198 seconds\n",
      "Epoch 73, data loading time: 0.0000 seconds\n",
      "Epoch 73, average training loss (change): 0.01584565080702305\n",
      "Epoch 73, average training loss (non-change): 0.0016804597107693553\n",
      "Epoch 74, model processing time: 0.0200 seconds\n",
      "Epoch 74, data loading time: 0.0000 seconds\n",
      "Epoch 74, average training loss (change): 0.015814747661352158\n",
      "Epoch 74, average training loss (non-change): 0.0016785787884145975\n",
      "Epoch 75, model processing time: 0.0201 seconds\n",
      "Epoch 75, data loading time: 0.0000 seconds\n",
      "Epoch 75, average training loss (change): 0.015784213319420815\n",
      "Epoch 75, average training loss (non-change): 0.001676698331721127\n",
      "Epoch 76, model processing time: 0.0200 seconds\n",
      "Epoch 76, data loading time: 0.0000 seconds\n",
      "Epoch 76, average training loss (change): 0.015754031017422676\n",
      "Epoch 76, average training loss (non-change): 0.0016748264897614717\n",
      "Epoch 77, model processing time: 0.0199 seconds\n",
      "Epoch 77, data loading time: 0.0000 seconds\n",
      "Epoch 77, average training loss (change): 0.0157241839915514\n",
      "Epoch 77, average training loss (non-change): 0.001672966405749321\n",
      "Epoch 78, model processing time: 0.0203 seconds\n",
      "Epoch 78, data loading time: 0.0000 seconds\n",
      "Epoch 78, average training loss (change): 0.015694577246904373\n",
      "Epoch 78, average training loss (non-change): 0.001671088277362287\n",
      "Epoch 79, model processing time: 0.0197 seconds\n",
      "Epoch 79, data loading time: 0.0000 seconds\n",
      "Epoch 79, average training loss (change): 0.015665221959352493\n",
      "Epoch 79, average training loss (non-change): 0.0016692326171323657\n",
      "Found new best model with val loss 0.013798763044178486at epoch 79\n",
      "Iteration 0 Epoch 80/99\n",
      "----------\n",
      "Epoch 80, model processing time: 0.0203 seconds\n",
      "Epoch 80, data loading time: 0.0000 seconds\n",
      "Epoch 80, average training loss (change): 0.015636082738637924\n",
      "Epoch 80, average training loss (non-change): 0.0016673724167048931\n",
      "Epoch 81, model processing time: 0.0202 seconds\n",
      "Epoch 81, data loading time: 0.0000 seconds\n",
      "Epoch 81, average training loss (change): 0.015607130713760853\n",
      "Epoch 81, average training loss (non-change): 0.0016655164072290063\n",
      "Epoch 82, model processing time: 0.0201 seconds\n",
      "Epoch 82, data loading time: 0.0000 seconds\n",
      "Epoch 82, average training loss (change): 0.01557835191488266\n",
      "Epoch 82, average training loss (non-change): 0.001663653296418488\n",
      "Epoch 83, model processing time: 0.0205 seconds\n",
      "Epoch 83, data loading time: 0.0000 seconds\n",
      "Epoch 83, average training loss (change): 0.015549705363810062\n",
      "Epoch 83, average training loss (non-change): 0.0016617896035313606\n",
      "Epoch 84, model processing time: 0.0198 seconds\n",
      "Epoch 84, data loading time: 0.0000 seconds\n",
      "Epoch 84, average training loss (change): 0.01552120316773653\n",
      "Epoch 84, average training loss (non-change): 0.0016599210212007165\n",
      "Epoch 85, model processing time: 0.0196 seconds\n",
      "Epoch 85, data loading time: 0.0000 seconds\n",
      "Epoch 85, average training loss (change): 0.015492819249629974\n",
      "Epoch 85, average training loss (non-change): 0.0016580531373620033\n",
      "Epoch 86, model processing time: 0.0202 seconds\n",
      "Epoch 86, data loading time: 0.0000 seconds\n",
      "Epoch 86, average training loss (change): 0.015464521944522858\n",
      "Epoch 86, average training loss (non-change): 0.0016561789670959115\n",
      "Epoch 87, model processing time: 0.0200 seconds\n",
      "Epoch 87, data loading time: 0.0000 seconds\n",
      "Epoch 87, average training loss (change): 0.015436334535479546\n",
      "Epoch 87, average training loss (non-change): 0.0016543070087209344\n",
      "Epoch 88, model processing time: 0.0199 seconds\n",
      "Epoch 88, data loading time: 0.0000 seconds\n",
      "Epoch 88, average training loss (change): 0.01540821511298418\n",
      "Epoch 88, average training loss (non-change): 0.001652424456551671\n",
      "Epoch 89, model processing time: 0.0200 seconds\n",
      "Epoch 89, data loading time: 0.0000 seconds\n",
      "Epoch 89, average training loss (change): 0.015380172058939934\n",
      "Epoch 89, average training loss (non-change): 0.001650540973059833\n",
      "Found new best model with val loss 0.013466333039104939at epoch 89\n",
      "Iteration 0 Epoch 90/99\n",
      "----------\n",
      "Epoch 90, model processing time: 0.0201 seconds\n",
      "Epoch 90, data loading time: 0.0000 seconds\n",
      "Epoch 90, average training loss (change): 0.015352177433669567\n",
      "Epoch 90, average training loss (non-change): 0.0016486523672938347\n",
      "Epoch 91, model processing time: 0.0197 seconds\n",
      "Epoch 91, data loading time: 0.0000 seconds\n",
      "Epoch 91, average training loss (change): 0.015324238687753677\n",
      "Epoch 91, average training loss (non-change): 0.0016467631794512272\n",
      "Epoch 92, model processing time: 0.0201 seconds\n",
      "Epoch 92, data loading time: 0.0000 seconds\n",
      "Epoch 92, average training loss (change): 0.015296346507966518\n",
      "Epoch 92, average training loss (non-change): 0.001644869684241712\n",
      "Epoch 93, model processing time: 0.0200 seconds\n",
      "Epoch 93, data loading time: 0.0000 seconds\n",
      "Epoch 93, average training loss (change): 0.0152684785425663\n",
      "Epoch 93, average training loss (non-change): 0.001642962801270187\n",
      "Epoch 94, model processing time: 0.0202 seconds\n",
      "Epoch 94, data loading time: 0.0000 seconds\n",
      "Epoch 94, average training loss (change): 0.015240661799907684\n",
      "Epoch 94, average training loss (non-change): 0.0016410534735769033\n",
      "Epoch 95, model processing time: 0.0200 seconds\n",
      "Epoch 95, data loading time: 0.0000 seconds\n",
      "Epoch 95, average training loss (change): 0.015212859027087688\n",
      "Epoch 95, average training loss (non-change): 0.0016391436802223325\n",
      "Epoch 96, model processing time: 0.0212 seconds\n",
      "Epoch 96, data loading time: 0.0000 seconds\n",
      "Epoch 96, average training loss (change): 0.015185084193944931\n",
      "Epoch 96, average training loss (non-change): 0.001637228182516992\n",
      "Epoch 97, model processing time: 0.0215 seconds\n",
      "Epoch 97, data loading time: 0.0000 seconds\n",
      "Epoch 97, average training loss (change): 0.015157331712543964\n",
      "Epoch 97, average training loss (non-change): 0.0016353032551705837\n",
      "Epoch 98, model processing time: 0.0212 seconds\n",
      "Epoch 98, data loading time: 0.0000 seconds\n",
      "Epoch 98, average training loss (change): 0.015129577368497849\n",
      "Epoch 98, average training loss (non-change): 0.001633368548937142\n",
      "Epoch 99, model processing time: 0.0214 seconds\n",
      "Epoch 99, data loading time: 0.0000 seconds\n",
      "Epoch 99, average training loss (change): 0.015101845376193523\n",
      "Epoch 99, average training loss (non-change): 0.0016314450185745955\n",
      "Found new best model with val loss 0.013211417011916637at epoch 99\n",
      "Training complete in 0m 39s with train loss 0.01673 and validation loss 0.01321\n",
      "Model 0 Trained\n",
      "Corresponding AE Vector: tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "Corresponding AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Model 0\n",
      "Learned AE Guidance (Lower better): tensor([2.3026e-09, 2.3026e-09, 2.3026e-09, 2.3026e-09, 2.3026e-09, 2.3026e-09,\n",
      "        2.3026e-09, 2.3026e-09])\n",
      "Learning Done in 49.50829005241394 sec, Checking if exists low-objective ae vector...\n",
      "No low-objective ae vectors found for this iteration..\n",
      "/Users/libowen/Documents/Research/RSS2025/code/IVNTR/predicators/gnn/neupi.py:1595: RuntimeWarning: divide by zero encountered in divide\n",
      "  node.update_value(self.global_zero_loss / self.visits, self.guidance_th)\n",
      "/Users/libowen/Documents/Research/RSS2025/code/IVNTR/predicators/gnn/neupi.py:1595: RuntimeWarning: invalid value encountered in divide\n",
      "  node.update_value(self.global_zero_loss / self.visits, self.guidance_th)\n",
      "Node [0 0 0 0 0 0 0 0] is fully expanded or has -inf value.\n",
      "New Frontier: []\n",
      "-----Iteration: 1 (neural_u_p1)------\n",
      "Generate 12 AE Vectors-Score Pairs for Predicate neural_u_p1\n",
      "GT AE Vec Provided, No Need to Generate AE Vectors.\n",
      "No more sat matrixes can be generated at iteration 1!\n",
      "Early Stopping at Iteration 1!\n",
      "******************Bi-level Optimization Done for neural_u_p1! Summary:******************\n",
      "After 1 iterations, we got 0 basic vectors:\n",
      "Skipping learning for HasChemX since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 0, 1, 0, 0, 0, 0])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for HasChemY since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 0, 0, 1, 0, 0, 0])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for IsCalibrated since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for ViewClear since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 1, 0, 0, 0, 0, 0, 0])\n",
      "AE Vector (Del): tensor([0, 0, 1, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Neural Learning Completed Successfully!\n",
      "   Learning trajectories: 50\n",
      "   Initial atom trajectory: <class 'list'> with 50 entries\n"
     ]
    }
   ],
   "source": [
    "# Test the neural learning process\n",
    "print(\"🧠 Starting Neural Predicate Learning Process...\\n\")\n",
    "\n",
    "print(\"📋 Learning Process Overview:\")\n",
    "print(\"   1. Generate training data from demonstration trajectories\")\n",
    "print(\"   2. Setup input fields for neural predicates\") \n",
    "print(\"   3. Initialize Action-Effect (AE) matrix constraints\")\n",
    "print(\"   4. Compute input normalizers for stable training\")\n",
    "print(\"   5. Train neural networks using the effect vector as ground truth\")\n",
    "print(\"   6. Validate learned predicates on held-out data\")\n",
    "\n",
    "# Call the main neural learning method (line 2312 from bilevel_learning_approach.py)\n",
    "print(f\"\\n🚀 Calling approach.learn_neural_predicates(dataset)...\")\n",
    "learning_trajectories, init_atom_traj = approach.learn_neural_predicates(dataset)\n",
    "\n",
    "print(f\"✅ Neural Learning Completed Successfully!\")\n",
    "print(f\"   Learning trajectories: {len(learning_trajectories)}\")\n",
    "print(f\"   Initial atom trajectory: {type(init_atom_traj)} with {len(init_atom_traj)} entries\" if init_atom_traj else \"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kf7eaj04fh",
   "metadata": {},
   "source": [
    "## 6. Examining Learned Neural Predicate Results\n",
    "\n",
    "After neural learning completes, IVNTR stores detailed information about the learned predicates. Let's examine the results and understand what was learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lk5os5du45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Detailed Learning Log File\n",
      "   Log file path: /Users/libowen/Documents/Research/RSS2025/code/IVNTR/docs/example_gt_vec_learning.log\n",
      "   You can examine the neural learning process in detail by checking this log file\n",
      "   The log contains training progress, loss values, and model statistics\n",
      "\n",
      "🧠 Learned Neural Predicate Information:\n",
      "   Source: approach.learned_ae_pred_info (line 312 in bilevel_learning_approach.py)\n",
      "\n",
      "📊 Learned Predicate Details:\n",
      "\n",
      "🔍 Predicate: neural_u_p1\n",
      "   Entity indices: []\n",
      "   Ground truth effect vectors: []\n",
      "   Learning status: Learned\n",
      "   Provided status: Not provided\n",
      "   📈 Learning Scores: No scores available\n",
      "   🧠 Model Weights: No trained models saved\n",
      "\n",
      "🔍 Predicate: CameraReadingTaken\n",
      "   Entity indices: [[0, 0]]\n",
      "   Ground truth effect vectors: N/A\n",
      "   Learning status: Learned\n",
      "   Provided status: Provided\n",
      "   📈 Learning Scores: No scores available\n",
      "   🧠 Model Weights: No trained models saved\n",
      "\n",
      "🔍 Predicate: GeigerReadingTaken\n",
      "   Entity indices: [[0, 0]]\n",
      "   Ground truth effect vectors: N/A\n",
      "   Learning status: Learned\n",
      "   Provided status: Provided\n",
      "   📈 Learning Scores: No scores available\n",
      "   🧠 Model Weights: No trained models saved\n",
      "\n",
      "🔍 Predicate: HasChemX\n",
      "   Entity indices: [[0]]\n",
      "   Ground truth effect vectors: N/A\n",
      "   Learning status: Learned\n",
      "   Provided status: Provided\n",
      "   📈 Learning Scores: No scores available\n",
      "   🧠 Model Weights: No trained models saved\n",
      "\n",
      "🔍 Predicate: HasChemY\n",
      "   Entity indices: [[0]]\n",
      "   Ground truth effect vectors: N/A\n",
      "   Learning status: Learned\n",
      "   Provided status: Provided\n",
      "   📈 Learning Scores: No scores available\n",
      "   🧠 Model Weights: No trained models saved\n",
      "\n",
      "🔍 Predicate: InfraredReadingTaken\n",
      "   Entity indices: [[0, 0]]\n",
      "   Ground truth effect vectors: N/A\n",
      "   Learning status: Learned\n",
      "   Provided status: Provided\n",
      "   📈 Learning Scores: No scores available\n",
      "   🧠 Model Weights: No trained models saved\n",
      "\n",
      "🔍 Predicate: IsCalibrated\n",
      "   Entity indices: [[0]]\n",
      "   Ground truth effect vectors: N/A\n",
      "   Learning status: Learned\n",
      "   Provided status: Provided\n",
      "   📈 Learning Scores: No scores available\n",
      "   🧠 Model Weights: No trained models saved\n",
      "\n",
      "🔍 Predicate: Sees\n",
      "   Entity indices: [[0, 0]]\n",
      "   Ground truth effect vectors: N/A\n",
      "   Learning status: Learned\n",
      "   Provided status: Provided\n",
      "   📈 Learning Scores: No scores available\n",
      "   🧠 Model Weights: No trained models saved\n",
      "\n",
      "🔍 Predicate: ViewClear\n",
      "   Entity indices: [[0]]\n",
      "   Ground truth effect vectors: N/A\n",
      "   Learning status: Learned\n",
      "   Provided status: Provided\n",
      "   📈 Learning Scores: No scores available\n",
      "   🧠 Model Weights: No trained models saved\n",
      "\n",
      "💡 Key Information:\n",
      "   • Scores: Track learning progress and model performance\n",
      "   • Model Weights: Store the trained neural networks for predicate classification\n",
      "   • Effect Vectors: Define which actions add/delete the learned predicate\n",
      "   • These components enable the learned predicates to replace ground truth predicates in planning!\n"
     ]
    }
   ],
   "source": [
    "# Point to log file for detailed learning process\n",
    "print(\"📋 Detailed Learning Log File\")\n",
    "print(f\"   Log file path: {log_file_path}\")\n",
    "print(f\"   You can examine the neural learning process in detail by checking this log file\")\n",
    "print(f\"   The log contains training progress, loss values, and model statistics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ivntr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
