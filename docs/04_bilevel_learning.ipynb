{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bilevel-intro",
   "metadata": {},
   "source": [
    "# IVNTR Bilevel Learning: Neural Predicate Invention\n",
    "\n",
    "This notebook explores the core IVNTR algorithm implemented in `predicators/approaches/bilevel_learning_approach.py`. IVNTR (bIleVel learNing from TRansitions) learns neural predicates that enable symbolic planning from demonstration data.\n",
    "\n",
    "## The Core Algorithm\n",
    "\n",
    "IVNTR implements a **bilevel learning approach** where:\n",
    "\n",
    "1. **Upper Level (Symbolic)**: Searches over candidate predicate effects and constructs NSRTs\n",
    "2. **Lower Level (Neural)**: Learns predicate classifiers using **Any** Neural Networks\n",
    "3. **Alternating Process**: The two levels guide each other through iterative refinement\n",
    "\n",
    "For each predicate, IVNTR alternates between:\n",
    "- **Effect Discovery**: Searching for high-level symbolic effects the predicate should have\n",
    "- **Grounding Learning**: Training neural networks to classify when the predicate holds\n",
    "\n",
    "## Tutorial Focus: \"IsCalibrated\" Predicate\n",
    "\n",
    "In this tutorial, we'll examine the simplest predicate invention process using the \"IsCalibrated\" predicate:\n",
    "- **Purpose**: Determines if a satellite has been calibrated and is ready to take readings\n",
    "- **Input**: Continuous feature vectors of satellites (position, calibration state, instrument type, ...)\n",
    "- **Output**: Binary classification (calibrated or not)\n",
    "- **Role**: Required precondition for camera, geiger, and infrared readings\n",
    "\n",
    "## What We'll Cover\n",
    "\n",
    "1. **Dummy NSRTs**: Starting point with incomplete effect knowledge.\n",
    "2. **Demonstration Data**: Training examples from oracle trajectories.\n",
    "3. **Neural Learning Process**: If we have perfect AE vector, how to train a neural network (single iteration).\n",
    "4. **Symbolic Search Process**: If we do not know the AE vector, how to use neural loss to guide the search (multiple iterations).\n",
    "5. **Advanced In-depth Highlight**: Binary predicate, variable bindings, quantifiers, and more.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Bilevel Learning Environment\n",
    "\n",
    "We'll recreate the bilevel learning setup from `bilevel_learning_approach.py`, focusing on the key components that IVNTR needs for predicate invention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/libowen/opt/anaconda3/envs/ivntr/lib/python3.8/site-packages/bosdyn/client/sdk.py:17: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "/Users/libowen/opt/anaconda3/envs/ivntr/lib/python3.8/site-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/Users/libowen/opt/anaconda3/envs/ivntr/lib/python3.8/site-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.ai')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/Users/libowen/opt/anaconda3/envs/ivntr/lib/python3.8/site-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n",
      "‚úÖ IVNTR Bilevel Learning Setup Complete!\n",
      "Environment: satellites\n",
      "Training tasks: 50\n",
      "Demonstration trajectories: 50\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Add the project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Set FastDownward path\n",
    "FD_EXEC_PATH = os.path.join(os.path.dirname(os.path.abspath('.')), 'ext', 'downward')\n",
    "os.environ['FD_EXEC_PATH'] = FD_EXEC_PATH\n",
    "\n",
    "# Import IVNTR components\n",
    "from predicators.envs.satellites import SatellitesEnv\n",
    "from predicators.ground_truth_models import get_dummy_nsrts, get_gt_options\n",
    "from predicators.datasets.demo_only import _generate_demonstrations\n",
    "from predicators.approaches import create_approach\n",
    "from predicators import utils\n",
    "from predicators.settings import CFG\n",
    "\n",
    "config_path = os.path.abspath(\"example_gt_vec_good.yaml\") \n",
    "log_file_path = os.path.abspath(\"example_gt_vec_learning.log\")\n",
    "neupi_save_path = os.path.abspath(\"saved_neural\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "\n",
    "# Test the configuration section\n",
    "utils.reset_config({\n",
    "    \"device\": \"cpu\",\n",
    "    \"env\": \"satellites\",\n",
    "    \"approach\": \"ivntr\",\n",
    "    \"neupi_pred_config\": config_path,\n",
    "    \"neupi_gt_ae_matrix\": True,\n",
    "    \"excluded_predicates\": \"ViewClear,IsCalibrated,HasChemX,HasChemY,Sees\",\n",
    "    \"neupi_do_normalization\": True,\n",
    "    \"num_train_tasks\": 50,\n",
    "    \"num_test_tasks\": 20,\n",
    "    \"seed\": 0,\n",
    "    \"bilevel_plan_without_sim\": False,\n",
    "    \"exclude_domain_feat\": None,\n",
    "    \"log_file\": log_file_path,\n",
    "})\n",
    "\n",
    "handlers = [logging.StreamHandler()]\n",
    "handlers.append(logging.FileHandler(CFG.log_file, mode='w'))\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(message)s\",\n",
    "                    handlers=handlers,\n",
    "                    force=True)\n",
    "\n",
    "CFG.seed = 42\n",
    "CFG.num_train_tasks = 50  # Generate 50 demonstration trajectories\n",
    "CFG.satellites_num_sat_train = [2, 3]\n",
    "CFG.satellites_num_obj_train = [2, 3]\n",
    "CFG.timeout = 10.0\n",
    "CFG.demonstrator = \"oracle\"\n",
    "CFG.max_initial_demos = 50\n",
    "\n",
    "print(\"‚úÖ IVNTR Bilevel Learning Setup Complete!\")\n",
    "print(f\"Environment: {CFG.env}\")\n",
    "print(f\"Training tasks: {CFG.num_train_tasks}\")\n",
    "print(f\"Demonstration trajectories: {CFG.max_initial_demos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dummy-nsrts-section",
   "metadata": {},
   "source": [
    "## 2. Understanding Dummy NSRTs\n",
    "\n",
    "IVNTR starts with **dummy NSRTs** - symbolic operators with incomplete knowledge. These are defined in `predicators/ground_truth_models/satellites/dummy_nsrts.py` and serve as the starting point for predicate invention.\n",
    "\n",
    "### Key Characteristics of Dummy NSRTs:\n",
    "- **Known preconditions**: Static predicates we can observe (e.g., `HasCamera`, `ShootsChemX`)\n",
    "- **Unknown effects**: Dynamic predicates we need to learn (e.g., `IsCalibrated`, `HasChemX`, `HasChemY`)\n",
    "- **Placeholder structure**: Provides the symbolic scaffolding for learning\n",
    "\n",
    "**Note**: While multiple predicates can be learned, this tutorial focuses specifically on the `IsCalibrated` predicate as our primary example.\n",
    "\n",
    "Let's examine these dummy NSRTs and understand what IVNTR needs to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-dummy-nsrts",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: you called get_or_create_env, but I couldn't find satellites in the cache. Making a new instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Dummy NSRTs: 8 operators\n",
      "\n",
      "üìã Calibrate\n",
      "   Parameters: ['?sat:satellite', '?obj:object']\n",
      "   Known Preconditions:\n",
      "     ‚úì CalibrationTarget(?sat:satellite, ?obj:object)\n",
      "   Known Add Effects: None (TO BE LEARNED!)\n",
      "\n",
      "üìã MoveTo\n",
      "   Parameters: ['?sat:satellite', '?obj:object']\n",
      "   Known Preconditions: None\n",
      "   Known Add Effects: None (TO BE LEARNED!)\n",
      "\n",
      "üìã TakeInfraredReading\n",
      "   Parameters: ['?sat:satellite', '?obj:object']\n",
      "   Known Preconditions:\n",
      "     ‚úì HasInfrared(?sat:satellite)\n",
      "   Known Add Effects:\n",
      "     + InfraredReadingTaken(?sat:satellite, ?obj:object)\n",
      "\n",
      "‚úÖ Dummy NSRTs loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test the dummy NSRTs section\n",
    "# Create environment and load dummy NSRTs\n",
    "env = SatellitesEnv(use_gui=False)\n",
    "train_tasks = env.get_train_tasks()\n",
    "\n",
    "# Get ground truth options (needed for dummy NSRTs)\n",
    "options = get_gt_options(env.get_name())\n",
    "\n",
    "# Load dummy NSRTs (the incomplete starting point)\n",
    "# get_dummy_nsrts takes: env_name, predicates_to_keep, options_to_keep\n",
    "dummy_nsrts = get_dummy_nsrts(env.get_name(), env.predicates, options)\n",
    "\n",
    "print(f\"üß© Dummy NSRTs: {len(dummy_nsrts)} operators\\n\")\n",
    "\n",
    "# Test just the first few NSRTs to verify structure\n",
    "for nsrt in sorted(list(dummy_nsrts)[:3], key=lambda x: x.name):\n",
    "    print(f\"üìã {nsrt.name}\")\n",
    "    print(f\"   Parameters: {[str(p) for p in nsrt.parameters]}\")\n",
    "    \n",
    "    if nsrt.preconditions:\n",
    "        print(f\"   Known Preconditions:\")\n",
    "        for pre in nsrt.preconditions:\n",
    "            print(f\"     ‚úì {pre}\")\n",
    "    else:\n",
    "        print(f\"   Known Preconditions: None\")\n",
    "    \n",
    "    if nsrt.add_effects:\n",
    "        print(f\"   Known Add Effects:\")\n",
    "        for eff in nsrt.add_effects:\n",
    "            print(f\"     + {eff}\")\n",
    "    else:\n",
    "        print(f\"   Known Add Effects: None (TO BE LEARNED!)\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ Dummy NSRTs loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-data-section",
   "metadata": {},
   "source": [
    "## 3. Generating Demonstration Data\n",
    "\n",
    "IVNTR learns from demonstration trajectories collected using the oracle approach. These trajectories contain the training signal for both effect discovery and neural predicate learning.\n",
    "\n",
    "Let's generate 50 demonstration trajectories using the process from `03_demo_trajectories.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "generate-demonstrations",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 8 NSRTs: {NSRT-ShootChemX:\n",
      "    Parameters: [?sat:satellite, ?obj:object]\n",
      "    Preconditions: [Sees(?sat:satellite, ?obj:object), ShootsChemX(?sat:satellite)]\n",
      "    Add Effects: [HasChemX(?obj:object)]\n",
      "    Delete Effects: []\n",
      "    Ignore Effects: []\n",
      "    Option Spec: ShootChemX(?sat:satellite, ?obj:object), NSRT-TakeInfraredReading:\n",
      "    Parameters: [?sat:satellite, ?obj:object]\n",
      "    Preconditions: [HasChemY(?obj:object), HasInfrared(?sat:satellite), IsCalibrated(?sat:satellite), Sees(?sat:satellite, ?obj:object)]\n",
      "    Add Effects: [InfraredReadingTaken(?sat:satellite, ?obj:object)]\n",
      "    Delete Effects: []\n",
      "    Ignore Effects: []\n",
      "    Option Spec: UseInfraRed(?sat:satellite, ?obj:object), NSRT-MoveTo:\n",
      "    Parameters: [?sat:satellite, ?obj:object]\n",
      "    Preconditions: [ViewClear(?sat:satellite)]\n",
      "    Add Effects: [Sees(?sat:satellite, ?obj:object)]\n",
      "    Delete Effects: [ViewClear(?sat:satellite)]\n",
      "    Ignore Effects: []\n",
      "    Option Spec: MoveTo(?sat:satellite, ?obj:object), NSRT-TakeGeigerReading:\n",
      "    Parameters: [?sat:satellite, ?obj:object]\n",
      "    Preconditions: [HasGeiger(?sat:satellite), IsCalibrated(?sat:satellite), Sees(?sat:satellite, ?obj:object)]\n",
      "    Add Effects: [GeigerReadingTaken(?sat:satellite, ?obj:object)]\n",
      "    Delete Effects: []\n",
      "    Ignore Effects: []\n",
      "    Option Spec: UseGeiger(?sat:satellite, ?obj:object), NSRT-ShootChemY:\n",
      "    Parameters: [?sat:satellite, ?obj:object]\n",
      "    Preconditions: [Sees(?sat:satellite, ?obj:object), ShootsChemY(?sat:satellite)]\n",
      "    Add Effects: [HasChemY(?obj:object)]\n",
      "    Delete Effects: []\n",
      "    Ignore Effects: []\n",
      "    Option Spec: ShootChemY(?sat:satellite, ?obj:object), NSRT-MoveAway:\n",
      "    Parameters: [?sat:satellite, ?obj:object]\n",
      "    Preconditions: [Sees(?sat:satellite, ?obj:object)]\n",
      "    Add Effects: [ViewClear(?sat:satellite)]\n",
      "    Delete Effects: [Sees(?sat:satellite, ?obj:object)]\n",
      "    Ignore Effects: []\n",
      "    Option Spec: MoveAway(?sat:satellite, ?obj:object), NSRT-Calibrate:\n",
      "    Parameters: [?sat:satellite, ?obj:object]\n",
      "    Preconditions: [CalibrationTarget(?sat:satellite, ?obj:object), Sees(?sat:satellite, ?obj:object)]\n",
      "    Add Effects: [IsCalibrated(?sat:satellite)]\n",
      "    Delete Effects: []\n",
      "    Ignore Effects: []\n",
      "    Option Spec: Calibrate(?sat:satellite, ?obj:object), NSRT-TakeCameraReading:\n",
      "    Parameters: [?sat:satellite, ?obj:object]\n",
      "    Preconditions: [HasCamera(?sat:satellite), HasChemX(?obj:object), IsCalibrated(?sat:satellite), Sees(?sat:satellite, ?obj:object)]\n",
      "    Add Effects: [CameraReadingTaken(?sat:satellite, ?obj:object)]\n",
      "    Delete Effects: []\n",
      "    Ignore Effects: []\n",
      "    Option Spec: UseCamera(?sat:satellite, ?obj:object)}\n",
      "Rows of the AE Matrix\n",
      "Calibrate(satellite0, object0)\n",
      "MoveAway(satellite0, object0)\n",
      "MoveTo(satellite0, object0)\n",
      "ShootChemX(satellite0, object0)\n",
      "ShootChemY(satellite0, object0)\n",
      "UseCamera(satellite0, object0)\n",
      "UseGeiger(satellite0, object0)\n",
      "UseInfraRed(satellite0, object0)\n",
      "Name and Columns of the AE Matrix (1: Add, 2: Del):\n",
      "HasChemX(0) ([Type(name='object')]): [0, 0, 0, 1, 0, 0, 0, 0]\n",
      "InfraredReadingTaken(0, 0) ([Type(name='satellite'), Type(name='object')]): [0, 0, 0, 0, 0, 0, 0, 1]\n",
      "Sees(0, 0) ([Type(name='satellite'), Type(name='object')]): [0, 2, 1, 0, 0, 0, 0, 0]\n",
      "ViewClear(0) ([Type(name='satellite')]): [0, 1, 2, 0, 0, 0, 0, 0]\n",
      "GeigerReadingTaken(0, 0) ([Type(name='satellite'), Type(name='object')]): [0, 0, 0, 0, 0, 0, 1, 0]\n",
      "HasChemY(0) ([Type(name='object')]): [0, 0, 0, 0, 1, 0, 0, 0]\n",
      "IsCalibrated(0) ([Type(name='satellite')]): [1, 0, 0, 0, 0, 0, 0, 0]\n",
      "CameraReadingTaken(0, 0) ([Type(name='satellite'), Type(name='object')]): [0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[CogMan] Reset called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ Generating Demonstration Trajectories...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n",
      "[CogMan] Reset called.\n",
      "[CogMan] Finishing episode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset Generated Successfully!\n",
      "   Total trajectories: 50\n",
      "   All demonstrations: True\n",
      "\n",
      "üìä Dataset Statistics:\n",
      "   Total action steps: 637\n",
      "   Average trajectory length: 12.7 steps\n",
      "   Successful demonstrations: 50/50\n",
      "‚úÖ Demonstration generation successful!\n",
      "\n",
      "üîç Sample Trajectory Analysis:\n",
      "   Task index: 0\n",
      "   Length: 11 states, 10 actions\n",
      "   Goal: {InfraredReadingTaken(sat1:satellite, obj2:object), CameraReadingTaken(sat0:satellite, obj2:object)}\n",
      "   Action sequence preview:\n",
      "     Step 0: MoveTo with params [0.0521853  0.65423506]\n",
      "     Step 1: Calibrate with params []\n",
      "     Step 2: ShootChemY with params []\n",
      "     Step 3: UseInfraRed with params []\n",
      "     Step 4: MoveTo with params [0.7846738 0.7001005]\n",
      "     ... (5 more actions)\n",
      "\n",
      "üí° This demonstration data provides the training signal for IVNTR's bilevel learning!\n"
     ]
    }
   ],
   "source": [
    "# Test the demonstration generation section (with a smaller number for testing)\n",
    "print(\"üé¨ Generating Demonstration Trajectories...\\n\")\n",
    "\n",
    "# This replicates the _generate_demonstrations function from demo_only.py\n",
    "# Note: We need to provide all required arguments\n",
    "training_tasks = [task.task for task in train_tasks]  # Use only first 5 tasks for testing\n",
    "dataset = _generate_demonstrations(\n",
    "    env,\n",
    "    training_tasks,\n",
    "    options,          # Set of ground truth options\n",
    "    0,               # train_tasks_start_idx \n",
    "    False            # annotate_with_gt_ops\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset Generated Successfully!\")\n",
    "print(f\"   Total trajectories: {len(dataset.trajectories)}\")\n",
    "print(f\"   All demonstrations: {all(traj.is_demo for traj in dataset.trajectories)}\")\n",
    "\n",
    "# Analyze the demonstration data\n",
    "total_steps = sum(len(traj.actions) for traj in dataset.trajectories)\n",
    "avg_length = total_steps / len(dataset.trajectories) if dataset.trajectories else 0\n",
    "successful_demos = sum(1 for traj in dataset.trajectories \n",
    "                      if traj.train_task_idx is not None)\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"   Total action steps: {total_steps}\")\n",
    "print(f\"   Average trajectory length: {avg_length:.1f} steps\")\n",
    "print(f\"   Successful demonstrations: {successful_demos}/{len(dataset.trajectories)}\")\n",
    "\n",
    "print(\"‚úÖ Demonstration generation successful!\")\n",
    "\n",
    "# Sample a trajectory to examine structure (if we have any)\n",
    "if dataset.trajectories:\n",
    "    sample_traj = dataset.trajectories[0]\n",
    "    sample_task = train_tasks[sample_traj.train_task_idx]\n",
    "\n",
    "    print(f\"\\nüîç Sample Trajectory Analysis:\")\n",
    "    print(f\"   Task index: {sample_traj.train_task_idx}\")\n",
    "    print(f\"   Length: {len(sample_traj.states)} states, {len(sample_traj.actions)} actions\")\n",
    "    print(f\"   Goal: {sample_task.goal}\")\n",
    "    print(f\"   Action sequence preview:\")\n",
    "\n",
    "    for i, action in enumerate(sample_traj.actions[:5]):\n",
    "        if hasattr(action, '_option') and action._option:\n",
    "            print(f\"     Step {i}: {action.get_option().name} with params {action.get_option().params}\")\n",
    "        else:\n",
    "            print(f\"     Step {i}: Raw action {action.arr[:4]}...\")\n",
    "\n",
    "    if len(sample_traj.actions) > 5:\n",
    "        print(f\"     ... ({len(sample_traj.actions)-5} more actions)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No trajectories generated - check configuration\")\n",
    "\n",
    "print(\"\\nüí° This demonstration data provides the training signal for IVNTR's bilevel learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7gprnllk1rm",
   "metadata": {},
   "source": [
    "## 4. Neural Predicate Training: \"IsCalibrated\" Example\n",
    "\n",
    "Now we'll dive into the core of IVNTR's neural predicate learning using the \"IsCalibrated\" predicate as our example. This predicate is crucial because:\n",
    "\n",
    "- **Purpose**: Determines if a satellite has been calibrated and is ready to take readings\n",
    "- **Input**: Satellite features (position, calibration state, instrument type)\n",
    "- **Output**: Binary classification (calibrated or not)\n",
    "- **Role**: Required precondition for all reading operations (camera, geiger, infrared)\n",
    "\n",
    "We'll demonstrate how IVNTR learns this predicate given a known **effect vector** that specifies which actions add/delete the predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89rj4q5nngo",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Options (Clusters): Calibrate, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): MoveAway, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): MoveTo, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): ShootChemX, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): ShootChemY, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): UseCamera, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): UseGeiger, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): UseInfraRed, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Using: satellite_0\n",
      "Predicate Type TOBE Invented: neural_u_p1\n",
      "Learning Conifg: {'name': 'neural_u_p1', 'types': ['satellite'], 'gt': [[1, 0, 0, 0, 0, 0, 0, 0]], 'ent_idx': [0], 'architecture': {'type': 'MLP', 'layer_size': 32, 'initializer': 'xavier'}, 'optimizer': {'type': 'AdamW', 'kwargs': {'lr': 0.001}}, 'lr_scheduler': {'type': 'StepLR', 'kwargs': {'step_size': 70, 'gamma': 0.1}}, 'batch_vect_num': 12, 'batch_size': 512, 'epochs': 100, 'gumbel_temp': 0.66, 'val_freq': 10, 'num_iter': 5, 'matrix_vec_try': 100, 'search_tree_max_level': 1, 'guidance_thresh': 0.05, 'loss_thresh': 0.005, 'skip_train': False}\n",
      "GT Provided for Predicate neural_u_p1!\n",
      "GT AE Vector: [1, 0, 0, 0, 0, 0, 0, 0]\n",
      "/Users/libowen/Documents/Research/RSS2025/code/IVNTR/predicators/gnn/neupi_utils.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cate_vector_sampled = torch.tensor(one).unsqueeze(0)\n",
      "Search Region Not Provided, Search All possible effects\n",
      "Quantified Decision Boundary Not Provided, Use 0.5\n",
      "Belived Ent Idx for Predicate: CameraReadingTaken\n",
      "Using: satellite_0\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): CameraReadingTaken\n",
      "Belived Ent Idx for Predicate: GeigerReadingTaken\n",
      "Using: satellite_0\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): GeigerReadingTaken\n",
      "Belived Ent Idx for Predicate: HasChemX\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): HasChemX\n",
      "Belived Ent Idx for Predicate: HasChemY\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): HasChemY\n",
      "Belived Ent Idx for Predicate: InfraredReadingTaken\n",
      "Using: satellite_0\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): InfraredReadingTaken\n",
      "Belived Ent Idx for Predicate: IsCalibrated\n",
      "Using: satellite_0\n",
      "Provided Predicate (No Learning): IsCalibrated\n",
      "Belived Ent Idx for Predicate: Sees\n",
      "Using: satellite_0\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): Sees\n",
      "Belived Ent Idx for Predicate: ViewClear\n",
      "Using: satellite_0\n",
      "Provided Predicate (No Learning): ViewClear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Creating IVNTR Bilevel Learning Approach...\n",
      "\n",
      "‚úÖ Approach created: BilevelLearningApproach\n",
      "   Approach name: ivntr\n",
      "\n",
      "üì¶ Action Options in IVNTR (sorted order):\n",
      "    0: Calibrate\n",
      "    1: MoveAway\n",
      "    2: MoveTo\n",
      "    3: ShootChemX\n",
      "    4: ShootChemY\n",
      "    5: UseCamera\n",
      "    6: UseGeiger\n",
      "    7: UseInfraRed\n",
      "\n",
      "üéØ Understanding Effect Vectors:\n",
      "   Effect vectors are 8-dimensional binary vectors\n",
      "   Each position corresponds to an action option above\n",
      "   Value 1 = action ADDS the predicate\n",
      "   Value 0 = action has no effect on the predicate\n",
      "   Value -1 = action DELETES the predicate\n",
      "\n",
      "üí° This ordering is crucial for interpreting effect vectors in example_gt_vec.yaml!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create the IVNTR Bilevel Learning Approach\n",
    "print(\"üß† Creating IVNTR Bilevel Learning Approach...\\n\")\n",
    "\n",
    "# Create the approach using the same process as main.py\n",
    "approach = create_approach(\n",
    "    CFG.approach,          # \"ivntr\"\n",
    "    env.predicates,        # Initial predicates \n",
    "    options,               # Ground truth options\n",
    "    env.types,             # Environment types\n",
    "    env.action_space,      # Action space\n",
    "    training_tasks         # Training tasks\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Approach created: {type(approach).__name__}\")\n",
    "print(f\"   Approach name: {approach.get_name()}\")\n",
    "\n",
    "# Step 2: Examine the sorted options for effect vector understanding\n",
    "print(f\"\\nüì¶ Action Options in IVNTR (sorted order):\")\n",
    "sorted_options = approach._sorted_options\n",
    "for i, option in enumerate(sorted_options):\n",
    "    print(f\"   {i:2d}: {option.name}\")\n",
    "\n",
    "print(f\"\\nüéØ Understanding Effect Vectors:\")\n",
    "print(f\"   Effect vectors are {len(sorted_options)}-dimensional binary vectors\")\n",
    "print(f\"   Each position corresponds to an action option above\")\n",
    "print(f\"   Value 1 = action ADDS the predicate\")\n",
    "print(f\"   Value 0 = action has no effect on the predicate\") \n",
    "print(f\"   Value -1 = action DELETES the predicate\")\n",
    "\n",
    "print(f\"\\nüí° This ordering is crucial for interpreting effect vectors in example_gt_vec.yaml!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mwxkmrxm7n",
   "metadata": {},
   "source": [
    "### Understanding the Effect Vector for IsCalibrated\n",
    "\n",
    "The effect vector `[1, 0, 0, 0, 0, 0, 0, 0]` from `example_gt_vec.yaml` tells us:\n",
    "\n",
    "1. **Position 0 (Calibrate action) ADDS IsCalibrated** (value 1)\n",
    "2. **All other actions (positions 1-7) have no effect** on IsCalibrated (value 0)\n",
    "\n",
    "This effect vector correctly captures the semantics of calibration:\n",
    "- **Calibrate action** sets a satellite as calibrated and ready for readings\n",
    "- **All other actions** (movement, shooting chemicals, taking readings) don't affect calibration status\n",
    "\n",
    "### The Learning Process\n",
    "\n",
    "Given this effect vector, IVNTR will:\n",
    "1. **Generate training labels** using the effect vector and demonstration trajectories\n",
    "2. **Train a neural network** to predict IsCalibrated from satellite features  \n",
    "3. **Validate the classifier** on held-out data\n",
    "\n",
    "Let's see this process in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fpw0dul0y0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing Updated IsCalibrated Effect Vector...\n",
      "\n",
      "üìã Updated IsCalibrated Effect Vector: [1, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "üìä Effect Distribution Analysis:\n",
      "    0: Calibrate       ‚Üí ADDS IsCalibrated ‚≠ê\n",
      "    1: MoveAway        ‚Üí no effect IsCalibrated\n",
      "    2: MoveTo          ‚Üí no effect IsCalibrated\n",
      "    3: ShootChemX      ‚Üí no effect IsCalibrated\n",
      "    4: ShootChemY      ‚Üí no effect IsCalibrated\n",
      "    5: UseCamera       ‚Üí no effect IsCalibrated\n",
      "    6: UseGeiger       ‚Üí no effect IsCalibrated\n",
      "    7: UseInfraRed     ‚Üí no effect IsCalibrated\n",
      "\n",
      "üéØ Key Insight: Calibrate action (position 0) ADDS IsCalibrated!\n",
      "   This makes perfect sense - calibration sets the satellite as ready\n",
      "   All other actions have no effect on IsCalibrated state\n"
     ]
    }
   ],
   "source": [
    "# Test the updated effect vector analysis\n",
    "print(\"üîç Analyzing Updated IsCalibrated Effect Vector...\\n\")\n",
    "\n",
    "# From updated example_gt_vec.yaml: gt_ae_vecs: [[1, 0, 0, 0, 0, 0, 0, 0]]\n",
    "# This shows the Calibrate action ADDS IsCalibrated\n",
    "updated_effect_vector = [1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "print(f\"üìã Updated IsCalibrated Effect Vector: {updated_effect_vector}\")\n",
    "print(f\"\\nüìä Effect Distribution Analysis:\")\n",
    "\n",
    "# Analyze each action's effect on IsCalibrated with the updated vector\n",
    "for i, (option, effect) in enumerate(zip(sorted_options, updated_effect_vector)):\n",
    "    effect_str = {1: \"ADDS\", 0: \"no effect\", -1: \"DELETES\"}[effect]\n",
    "    if effect != 0:\n",
    "        print(f\"   {i:2d}: {option.name:<15} ‚Üí {effect_str} IsCalibrated ‚≠ê\")\n",
    "    else:\n",
    "        print(f\"   {i:2d}: {option.name:<15} ‚Üí {effect_str} IsCalibrated\")\n",
    "\n",
    "print(f\"\\nüéØ Key Insight: Calibrate action (position 0) ADDS IsCalibrated!\")\n",
    "print(f\"   This makes perfect sense - calibration sets the satellite as ready\")\n",
    "print(f\"   All other actions have no effect on IsCalibrated state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "s17fvq8y5v",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Constructing NeuPi Data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Starting Neural Predicate Learning Process...\n",
      "\n",
      "üìã Learning Process Overview:\n",
      "   1. Generate training data from demonstration trajectories\n",
      "   2. Setup input fields for neural predicates\n",
      "   3. Initialize Action-Effect (AE) matrix constraints\n",
      "   4. Compute input normalizers for stable training\n",
      "   5. Train neural networks using the effect vector as ground truth\n",
      "   6. Validate learned predicates on held-out data\n",
      "\n",
      "üöÄ Calling approach.learn_neural_predicates(dataset)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 106.17it/s]\n",
      "Low-level feat not changed for Predicate HasChemX in Row 0\n",
      "Low-level feat not changed for Predicate HasChemX in Row 0\n",
      "Low-level feat not changed for Predicate HasChemY in Row 0\n",
      "Low-level feat not changed for Predicate HasChemY in Row 0\n",
      "Low-level feat not changed for Predicate HasChemX in Row 1\n",
      "Low-level feat not changed for Predicate HasChemX in Row 1\n",
      "Low-level feat not changed for Predicate HasChemY in Row 1\n",
      "Low-level feat not changed for Predicate HasChemY in Row 1\n",
      "Low-level feat not changed for Predicate HasChemX in Row 2\n",
      "Low-level feat not changed for Predicate HasChemX in Row 2\n",
      "Low-level feat not changed for Predicate HasChemY in Row 2\n",
      "Low-level feat not changed for Predicate HasChemY in Row 2\n",
      "Low-level feat not changed for Predicate neural_u_p1 in Row 3\n",
      "Low-level feat not changed for Predicate neural_u_p1 in Row 3\n",
      "Low-level feat not changed for Predicate IsCalibrated in Row 3\n",
      "Low-level feat not changed for Predicate IsCalibrated in Row 3\n",
      "Low-level feat not changed for Predicate ViewClear in Row 3\n",
      "Low-level feat not changed for Predicate ViewClear in Row 3\n",
      "Low-level feat not changed for Predicate neural_u_p1 in Row 4\n",
      "Low-level feat not changed for Predicate neural_u_p1 in Row 4\n",
      "Low-level feat not changed for Predicate IsCalibrated in Row 4\n",
      "Low-level feat not changed for Predicate IsCalibrated in Row 4\n",
      "Low-level feat not changed for Predicate ViewClear in Row 4\n",
      "Low-level feat not changed for Predicate ViewClear in Row 4\n",
      "Low-level feat not changed for Predicate HasChemX in Row 5\n",
      "Low-level feat not changed for Predicate HasChemX in Row 5\n",
      "Low-level feat not changed for Predicate HasChemY in Row 5\n",
      "Low-level feat not changed for Predicate HasChemY in Row 5\n",
      "Low-level feat not changed for Predicate HasChemX in Row 6\n",
      "Low-level feat not changed for Predicate HasChemX in Row 6\n",
      "Low-level feat not changed for Predicate HasChemY in Row 6\n",
      "Low-level feat not changed for Predicate HasChemY in Row 6\n",
      "Low-level feat not changed for Predicate HasChemX in Row 7\n",
      "Low-level feat not changed for Predicate HasChemX in Row 7\n",
      "Low-level feat not changed for Predicate HasChemY in Row 7\n",
      "Low-level feat not changed for Predicate HasChemY in Row 7\n",
      "Predicate neural_u_p1 Not Provided for action Calibrate in Data\n",
      "Predicate neural_u_p1 Not Provided for action MoveAway in Data\n",
      "Predicate neural_u_p1 Not Provided for action MoveTo in Data\n",
      "Predicate neural_u_p1 Not Provided for action ShootChemX in Data\n",
      "Predicate neural_u_p1 Not Provided for action ShootChemY in Data\n",
      "Action UseCamera Definitely has no effect for pred neural_u_p1\n",
      "Action UseGeiger Definitely has no effect for pred neural_u_p1\n",
      "Action UseInfraRed Definitely has no effect for pred neural_u_p1\n",
      "Action Calibrate Definitely has no effect for pred CameraReadingTaken\n",
      "Action MoveAway Definitely has no effect for pred CameraReadingTaken\n",
      "Action MoveTo Definitely has no effect for pred CameraReadingTaken\n",
      "Action ShootChemX Definitely has no effect for pred CameraReadingTaken\n",
      "Action ShootChemY Definitely has no effect for pred CameraReadingTaken\n",
      "Action UseCamera Definitely has add effect for pred CameraReadingTaken\n",
      "Action UseGeiger Definitely has no effect for pred CameraReadingTaken\n",
      "Action UseInfraRed Definitely has no effect for pred CameraReadingTaken\n",
      "Action Calibrate Definitely has no effect for pred GeigerReadingTaken\n",
      "Action MoveAway Definitely has no effect for pred GeigerReadingTaken\n",
      "Action MoveTo Definitely has no effect for pred GeigerReadingTaken\n",
      "Action ShootChemX Definitely has no effect for pred GeigerReadingTaken\n",
      "Action ShootChemY Definitely has no effect for pred GeigerReadingTaken\n",
      "Action UseCamera Definitely has no effect for pred GeigerReadingTaken\n",
      "Action UseGeiger Definitely has add effect for pred GeigerReadingTaken\n",
      "Action UseInfraRed Definitely has no effect for pred GeigerReadingTaken\n",
      "Action Calibrate Definitely has no effect for pred HasChemX\n",
      "Action MoveAway Definitely has no effect for pred HasChemX\n",
      "Action MoveTo Definitely has no effect for pred HasChemX\n",
      "Action ShootChemX Definitely has add effect for pred HasChemX\n",
      "Action ShootChemY Definitely has no effect for pred HasChemX\n",
      "Action UseCamera Definitely has no effect for pred HasChemX\n",
      "Action UseGeiger Definitely has no effect for pred HasChemX\n",
      "Action UseInfraRed Definitely has no effect for pred HasChemX\n",
      "Action Calibrate Definitely has no effect for pred HasChemY\n",
      "Action MoveAway Definitely has no effect for pred HasChemY\n",
      "Action MoveTo Definitely has no effect for pred HasChemY\n",
      "Action ShootChemX Definitely has no effect for pred HasChemY\n",
      "Action ShootChemY Definitely has add effect for pred HasChemY\n",
      "Action UseCamera Definitely has no effect for pred HasChemY\n",
      "Action UseGeiger Definitely has no effect for pred HasChemY\n",
      "Action UseInfraRed Definitely has no effect for pred HasChemY\n",
      "Action Calibrate Definitely has no effect for pred InfraredReadingTaken\n",
      "Action MoveAway Definitely has no effect for pred InfraredReadingTaken\n",
      "Action MoveTo Definitely has no effect for pred InfraredReadingTaken\n",
      "Action ShootChemX Definitely has no effect for pred InfraredReadingTaken\n",
      "Action ShootChemY Definitely has no effect for pred InfraredReadingTaken\n",
      "Action UseCamera Definitely has no effect for pred InfraredReadingTaken\n",
      "Action UseGeiger Definitely has no effect for pred InfraredReadingTaken\n",
      "Action UseInfraRed Definitely has add effect for pred InfraredReadingTaken\n",
      "Action Calibrate Definitely has add effect for pred IsCalibrated\n",
      "Action MoveAway Definitely has no effect for pred IsCalibrated\n",
      "Action MoveTo Definitely has no effect for pred IsCalibrated\n",
      "Action ShootChemX Definitely has no effect for pred IsCalibrated\n",
      "Action ShootChemY Definitely has no effect for pred IsCalibrated\n",
      "Action UseCamera Definitely has no effect for pred IsCalibrated\n",
      "Action UseGeiger Definitely has no effect for pred IsCalibrated\n",
      "Action UseInfraRed Definitely has no effect for pred IsCalibrated\n",
      "Action Calibrate Definitely has no effect for pred Sees\n",
      "Action MoveAway Definitely has delete effect for pred Sees\n",
      "Action MoveTo Definitely has add effect for pred Sees\n",
      "Action ShootChemX Definitely has no effect for pred Sees\n",
      "Action ShootChemY Definitely has no effect for pred Sees\n",
      "Action UseCamera Definitely has no effect for pred Sees\n",
      "Action UseGeiger Definitely has no effect for pred Sees\n",
      "Action UseInfraRed Definitely has no effect for pred Sees\n",
      "Action Calibrate Definitely has no effect for pred ViewClear\n",
      "Action MoveAway Definitely has add effect for pred ViewClear\n",
      "Action MoveTo Definitely has delete effect for pred ViewClear\n",
      "Action ShootChemX Definitely has no effect for pred ViewClear\n",
      "Action ShootChemY Definitely has no effect for pred ViewClear\n",
      "Action UseCamera Definitely has no effect for pred ViewClear\n",
      "Action UseGeiger Definitely has no effect for pred ViewClear\n",
      "Action UseInfraRed Definitely has no effect for pred ViewClear\n",
      "Computing Normalizer for Inouts...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 637/637 [00:00<00:00, 2118.77it/s]\n",
      "Skipping learning for CameraReadingTaken since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 0, 0, 0, 1, 0, 0])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for GeigerReadingTaken since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 0, 0, 0, 0, 1, 0])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for InfraredReadingTaken since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 0, 0, 0, 0, 0, 1])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for Sees since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 1, 0, 0, 0, 0, 0])\n",
      "AE Vector (Del): tensor([0, 1, 0, 0, 0, 0, 0, 0])\n",
      "**************Learning Typed Predicate: neural_u_p1**************\n",
      "Learning Config: {'name': 'neural_u_p1', 'types': ['satellite'], 'gt': [[1, 0, 0, 0, 0, 0, 0, 0]], 'ent_idx': [0], 'architecture': {'type': 'MLP', 'layer_size': 32, 'initializer': 'xavier'}, 'optimizer': {'type': 'AdamW', 'kwargs': {'lr': 0.001}}, 'lr_scheduler': {'type': 'StepLR', 'kwargs': {'step_size': 70, 'gamma': 0.1}}, 'batch_vect_num': 12, 'batch_size': 512, 'epochs': 100, 'gumbel_temp': 0.66, 'val_freq': 10, 'num_iter': 5, 'matrix_vec_try': 100, 'search_tree_max_level': 1, 'guidance_thresh': 0.05, 'loss_thresh': 0.005, 'skip_train': False, 'decision_b': 0.5}\n",
      "Created transition dataset with 1 transition pairs\n",
      "/Users/libowen/Documents/Research/RSS2025/code/IVNTR/predicators/gnn/neupi.py:1496: RuntimeWarning: Mean of empty slice.\n",
      "  if (non_zero_guidance.mean() < 0.4) or (non_zero_indexes.size == 0):\n",
      "/Users/libowen/opt/anaconda3/envs/ivntr/lib/python3.8/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Training from scratch.\n",
      "***************Bi-level Optimizing (neural_u_p1)***************\n",
      "-----Iteration: 0 (neural_u_p1)------\n",
      "Generate 12 AE Vectors-Score Pairs for Predicate neural_u_p1\n",
      "GT AE Vec Provided, No Need to Generate AE Vectors.\n",
      "Optimizing 1 Neural Models with AE Vector from BO...\n",
      "*******Vec 0 (neural_u_p1)*******\n",
      "(Add): \n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "(Del): \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Generating Graph Data with Current AE matrix...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 637/637 [00:00<00:00, 1209.76it/s]\n",
      "Created transition dataset with 509 transition pairs\n",
      "Created transition dataset with 128 transition pairs\n",
      "Training Neural Model 0...\n",
      "Predicat 1 Iteration 0 | Starting process 0\n",
      "Iteration 0 Epoch 0/99\n",
      "----------\n",
      "Epoch 0, model processing time: 0.3286 seconds\n",
      "Epoch 0, data loading time: 0.0000 seconds\n",
      "Epoch 0, average training loss (change): 0.68336021900177\n",
      "Epoch 0, average training loss (non-change): 0.004021019674837589\n",
      "Found new best model with val loss 100.0at epoch 0\n",
      "Epoch 1, model processing time: 0.0323 seconds\n",
      "Epoch 1, data loading time: 0.0000 seconds\n",
      "Epoch 1, average training loss (change): 0.5532371401786804\n",
      "Epoch 1, average training loss (non-change): 0.004067802801728249\n",
      "Epoch 2, model processing time: 0.0267 seconds\n",
      "Epoch 2, data loading time: 0.0000 seconds\n",
      "Epoch 2, average training loss (change): 0.4570595920085907\n",
      "Epoch 2, average training loss (non-change): 0.004453834146261215\n",
      "Epoch 3, model processing time: 0.0276 seconds\n",
      "Epoch 3, data loading time: 0.0000 seconds\n",
      "Epoch 3, average training loss (change): 0.377045214176178\n",
      "Epoch 3, average training loss (non-change): 0.0048553054220974445\n",
      "Epoch 4, model processing time: 0.0261 seconds\n",
      "Epoch 4, data loading time: 0.0000 seconds\n",
      "Epoch 4, average training loss (change): 0.3099765181541443\n",
      "Epoch 4, average training loss (non-change): 0.005151686258614063\n",
      "Epoch 5, model processing time: 0.0256 seconds\n",
      "Epoch 5, data loading time: 0.0000 seconds\n",
      "Epoch 5, average training loss (change): 0.2568964958190918\n",
      "Epoch 5, average training loss (non-change): 0.005284240934997797\n",
      "Epoch 6, model processing time: 0.0280 seconds\n",
      "Epoch 6, data loading time: 0.0000 seconds\n",
      "Epoch 6, average training loss (change): 0.21474206447601318\n",
      "Epoch 6, average training loss (non-change): 0.005264622159302235\n",
      "Epoch 7, model processing time: 0.0244 seconds\n",
      "Epoch 7, data loading time: 0.0000 seconds\n",
      "Epoch 7, average training loss (change): 0.18003062903881073\n",
      "Epoch 7, average training loss (non-change): 0.005111303646117449\n",
      "Epoch 8, model processing time: 0.0261 seconds\n",
      "Epoch 8, data loading time: 0.0000 seconds\n",
      "Epoch 8, average training loss (change): 0.15129373967647552\n",
      "Epoch 8, average training loss (non-change): 0.0048373001627624035\n",
      "Epoch 9, model processing time: 0.0256 seconds\n",
      "Epoch 9, data loading time: 0.0000 seconds\n",
      "Epoch 9, average training loss (change): 0.1277407556772232\n",
      "Epoch 9, average training loss (non-change): 0.004479230381548405\n",
      "Found new best model with val loss 0.627296507358551at epoch 9\n",
      "Iteration 0 Epoch 10/99\n",
      "----------\n",
      "Epoch 10, model processing time: 0.0337 seconds\n",
      "Epoch 10, data loading time: 0.0000 seconds\n",
      "Epoch 10, average training loss (change): 0.1085900366306305\n",
      "Epoch 10, average training loss (non-change): 0.0040847002528607845\n",
      "Epoch 11, model processing time: 0.0271 seconds\n",
      "Epoch 11, data loading time: 0.0000 seconds\n",
      "Epoch 11, average training loss (change): 0.09305747598409653\n",
      "Epoch 11, average training loss (non-change): 0.0036946963518857956\n",
      "Epoch 12, model processing time: 0.0268 seconds\n",
      "Epoch 12, data loading time: 0.0000 seconds\n",
      "Epoch 12, average training loss (change): 0.08047081530094147\n",
      "Epoch 12, average training loss (non-change): 0.0033362112008035183\n",
      "Epoch 13, model processing time: 0.0273 seconds\n",
      "Epoch 13, data loading time: 0.0000 seconds\n",
      "Epoch 13, average training loss (change): 0.07025467604398727\n",
      "Epoch 13, average training loss (non-change): 0.0030219312757253647\n",
      "Epoch 14, model processing time: 0.0276 seconds\n",
      "Epoch 14, data loading time: 0.0000 seconds\n",
      "Epoch 14, average training loss (change): 0.061906132847070694\n",
      "Epoch 14, average training loss (non-change): 0.0027537825517356396\n",
      "Epoch 15, model processing time: 0.0260 seconds\n",
      "Epoch 15, data loading time: 0.0000 seconds\n",
      "Epoch 15, average training loss (change): 0.05500367283821106\n",
      "Epoch 15, average training loss (non-change): 0.002527836710214615\n",
      "Epoch 16, model processing time: 0.0391 seconds\n",
      "Epoch 16, data loading time: 0.0000 seconds\n",
      "Epoch 16, average training loss (change): 0.04921586811542511\n",
      "Epoch 16, average training loss (non-change): 0.002338085323572159\n",
      "Epoch 17, model processing time: 0.0275 seconds\n",
      "Epoch 17, data loading time: 0.0000 seconds\n",
      "Epoch 17, average training loss (change): 0.04429445043206215\n",
      "Epoch 17, average training loss (non-change): 0.0021783942356705666\n",
      "Epoch 18, model processing time: 0.0257 seconds\n",
      "Epoch 18, data loading time: 0.0000 seconds\n",
      "Epoch 18, average training loss (change): 0.040057964622974396\n",
      "Epoch 18, average training loss (non-change): 0.002043414395302534\n",
      "Epoch 19, model processing time: 0.0273 seconds\n",
      "Epoch 19, data loading time: 0.0000 seconds\n",
      "Epoch 19, average training loss (change): 0.03637394309043884\n",
      "Epoch 19, average training loss (non-change): 0.0019287168979644775\n",
      "Found new best model with val loss 0.2085227370262146at epoch 19\n",
      "Iteration 0 Epoch 20/99\n",
      "----------\n",
      "Epoch 20, model processing time: 0.0270 seconds\n",
      "Epoch 20, data loading time: 0.0000 seconds\n",
      "Epoch 20, average training loss (change): 0.03314463049173355\n",
      "Epoch 20, average training loss (non-change): 0.0018308386206626892\n",
      "Epoch 21, model processing time: 0.0256 seconds\n",
      "Epoch 21, data loading time: 0.0000 seconds\n",
      "Epoch 21, average training loss (change): 0.03029673546552658\n",
      "Epoch 21, average training loss (non-change): 0.0017471692990511656\n",
      "Epoch 22, model processing time: 0.0254 seconds\n",
      "Epoch 22, data loading time: 0.0000 seconds\n",
      "Epoch 22, average training loss (change): 0.027774102985858917\n",
      "Epoch 22, average training loss (non-change): 0.0016757943667471409\n",
      "Epoch 23, model processing time: 0.0245 seconds\n",
      "Epoch 23, data loading time: 0.0000 seconds\n",
      "Epoch 23, average training loss (change): 0.025532756000757217\n",
      "Epoch 23, average training loss (non-change): 0.0016152861062437296\n",
      "Epoch 24, model processing time: 0.0260 seconds\n",
      "Epoch 24, data loading time: 0.0000 seconds\n",
      "Epoch 24, average training loss (change): 0.02353762835264206\n",
      "Epoch 24, average training loss (non-change): 0.0015645823441445827\n",
      "Epoch 25, model processing time: 0.0265 seconds\n",
      "Epoch 25, data loading time: 0.0000 seconds\n",
      "Epoch 25, average training loss (change): 0.02175995334982872\n",
      "Epoch 25, average training loss (non-change): 0.001522839767858386\n",
      "Epoch 26, model processing time: 0.0261 seconds\n",
      "Epoch 26, data loading time: 0.0000 seconds\n",
      "Epoch 26, average training loss (change): 0.020175427198410034\n",
      "Epoch 26, average training loss (non-change): 0.0014893107581883669\n",
      "Epoch 27, model processing time: 0.0268 seconds\n",
      "Epoch 27, data loading time: 0.0000 seconds\n",
      "Epoch 27, average training loss (change): 0.018762994557619095\n",
      "Epoch 27, average training loss (non-change): 0.0014633152168244123\n",
      "Epoch 28, model processing time: 0.0297 seconds\n",
      "Epoch 28, data loading time: 0.0000 seconds\n",
      "Epoch 28, average training loss (change): 0.01750389114022255\n",
      "Epoch 28, average training loss (non-change): 0.0014441360253840685\n",
      "Epoch 29, model processing time: 0.0286 seconds\n",
      "Epoch 29, data loading time: 0.0000 seconds\n",
      "Epoch 29, average training loss (change): 0.016381127759814262\n",
      "Epoch 29, average training loss (non-change): 0.0014310346450656652\n",
      "Found new best model with val loss 0.03647133335471153at epoch 29\n",
      "Iteration 0 Epoch 30/99\n",
      "----------\n",
      "Epoch 30, model processing time: 0.0263 seconds\n",
      "Epoch 30, data loading time: 0.0000 seconds\n",
      "Epoch 30, average training loss (change): 0.015379289165139198\n",
      "Epoch 30, average training loss (non-change): 0.0014232366811484098\n",
      "Epoch 31, model processing time: 0.0264 seconds\n",
      "Epoch 31, data loading time: 0.0000 seconds\n",
      "Epoch 31, average training loss (change): 0.014484294690191746\n",
      "Epoch 31, average training loss (non-change): 0.0014198506250977516\n",
      "Epoch 32, model processing time: 0.0259 seconds\n",
      "Epoch 32, data loading time: 0.0000 seconds\n",
      "Epoch 32, average training loss (change): 0.013683437369763851\n",
      "Epoch 32, average training loss (non-change): 0.0014199190773069859\n",
      "Epoch 33, model processing time: 0.0266 seconds\n",
      "Epoch 33, data loading time: 0.0000 seconds\n",
      "Epoch 33, average training loss (change): 0.01296530943363905\n",
      "Epoch 33, average training loss (non-change): 0.0014224376063793898\n",
      "Epoch 34, model processing time: 0.0267 seconds\n",
      "Epoch 34, data loading time: 0.0000 seconds\n",
      "Epoch 34, average training loss (change): 0.012319737114012241\n",
      "Epoch 34, average training loss (non-change): 0.0014263419434428215\n",
      "Epoch 35, model processing time: 0.0269 seconds\n",
      "Epoch 35, data loading time: 0.0000 seconds\n",
      "Epoch 35, average training loss (change): 0.011737789958715439\n",
      "Epoch 35, average training loss (non-change): 0.0014305445365607738\n",
      "Epoch 36, model processing time: 0.0256 seconds\n",
      "Epoch 36, data loading time: 0.0000 seconds\n",
      "Epoch 36, average training loss (change): 0.011211587116122246\n",
      "Epoch 36, average training loss (non-change): 0.00143393874168396\n",
      "Epoch 37, model processing time: 0.0276 seconds\n",
      "Epoch 37, data loading time: 0.0000 seconds\n",
      "Epoch 37, average training loss (change): 0.010734282433986664\n",
      "Epoch 37, average training loss (non-change): 0.00143554643727839\n",
      "Epoch 38, model processing time: 0.0259 seconds\n",
      "Epoch 38, data loading time: 0.0000 seconds\n",
      "Epoch 38, average training loss (change): 0.01029995083808899\n",
      "Epoch 38, average training loss (non-change): 0.0014344528317451477\n",
      "Epoch 39, model processing time: 0.0253 seconds\n",
      "Epoch 39, data loading time: 0.0000 seconds\n",
      "Epoch 39, average training loss (change): 0.009903491474688053\n",
      "Epoch 39, average training loss (non-change): 0.0014299224130809307\n",
      "Found new best model with val loss 0.010799997486174107at epoch 39\n",
      "Iteration 0 Epoch 40/99\n",
      "----------\n",
      "Epoch 40, model processing time: 0.0318 seconds\n",
      "Epoch 40, data loading time: 0.0000 seconds\n",
      "Epoch 40, average training loss (change): 0.009540515951812267\n",
      "Epoch 40, average training loss (non-change): 0.0014214247930794954\n",
      "Epoch 41, model processing time: 0.0327 seconds\n",
      "Epoch 41, data loading time: 0.0000 seconds\n",
      "Epoch 41, average training loss (change): 0.009207284078001976\n",
      "Epoch 41, average training loss (non-change): 0.0014086326118558645\n",
      "Epoch 42, model processing time: 0.0271 seconds\n",
      "Epoch 42, data loading time: 0.0000 seconds\n",
      "Epoch 42, average training loss (change): 0.008900584653019905\n",
      "Epoch 42, average training loss (non-change): 0.0013914599549025297\n",
      "Epoch 43, model processing time: 0.0284 seconds\n",
      "Epoch 43, data loading time: 0.0000 seconds\n",
      "Epoch 43, average training loss (change): 0.008617653511464596\n",
      "Epoch 43, average training loss (non-change): 0.0013700048439204693\n",
      "Epoch 44, model processing time: 0.0266 seconds\n",
      "Epoch 44, data loading time: 0.0000 seconds\n",
      "Epoch 44, average training loss (change): 0.008356110192835331\n",
      "Epoch 44, average training loss (non-change): 0.0013445746153593063\n",
      "Epoch 45, model processing time: 0.0281 seconds\n",
      "Epoch 45, data loading time: 0.0000 seconds\n",
      "Epoch 45, average training loss (change): 0.00811387412250042\n",
      "Epoch 45, average training loss (non-change): 0.0013156149070709944\n",
      "Epoch 46, model processing time: 0.0333 seconds\n",
      "Epoch 46, data loading time: 0.0000 seconds\n",
      "Epoch 46, average training loss (change): 0.007889092899858952\n",
      "Epoch 46, average training loss (non-change): 0.0012836847454309464\n",
      "Epoch 47, model processing time: 0.0271 seconds\n",
      "Epoch 47, data loading time: 0.0000 seconds\n",
      "Epoch 47, average training loss (change): 0.007680126465857029\n",
      "Epoch 47, average training loss (non-change): 0.001249385066330433\n",
      "Epoch 48, model processing time: 0.0266 seconds\n",
      "Epoch 48, data loading time: 0.0000 seconds\n",
      "Epoch 48, average training loss (change): 0.007485501002520323\n",
      "Epoch 48, average training loss (non-change): 0.0012133880518376827\n",
      "Epoch 49, model processing time: 0.0395 seconds\n",
      "Epoch 49, data loading time: 0.0000 seconds\n",
      "Epoch 49, average training loss (change): 0.0073038507252931595\n",
      "Epoch 49, average training loss (non-change): 0.001176297664642334\n",
      "Found new best model with val loss 0.006609008647501469at epoch 49\n",
      "Iteration 0 Epoch 50/99\n",
      "----------\n",
      "Epoch 50, model processing time: 0.0271 seconds\n",
      "Epoch 50, data loading time: 0.0000 seconds\n",
      "Epoch 50, average training loss (change): 0.007133913226425648\n",
      "Epoch 50, average training loss (non-change): 0.0011387125123292208\n",
      "Epoch 51, model processing time: 0.0253 seconds\n",
      "Epoch 51, data loading time: 0.0000 seconds\n",
      "Epoch 51, average training loss (change): 0.006974563002586365\n",
      "Epoch 51, average training loss (non-change): 0.001101175555959344\n",
      "Epoch 52, model processing time: 0.0267 seconds\n",
      "Epoch 52, data loading time: 0.0000 seconds\n",
      "Epoch 52, average training loss (change): 0.006824727635830641\n",
      "Epoch 52, average training loss (non-change): 0.0010641373228281736\n",
      "Epoch 53, model processing time: 0.0265 seconds\n",
      "Epoch 53, data loading time: 0.0000 seconds\n",
      "Epoch 53, average training loss (change): 0.006683409679681063\n",
      "Epoch 53, average training loss (non-change): 0.001027985941618681\n",
      "Epoch 54, model processing time: 0.0283 seconds\n",
      "Epoch 54, data loading time: 0.0000 seconds\n",
      "Epoch 54, average training loss (change): 0.006549731362611055\n",
      "Epoch 54, average training loss (non-change): 0.000993028050288558\n",
      "Epoch 55, model processing time: 0.0261 seconds\n",
      "Epoch 55, data loading time: 0.0000 seconds\n",
      "Epoch 55, average training loss (change): 0.006422864273190498\n",
      "Epoch 55, average training loss (non-change): 0.0009594880975782871\n",
      "Epoch 56, model processing time: 0.0253 seconds\n",
      "Epoch 56, data loading time: 0.0000 seconds\n",
      "Epoch 56, average training loss (change): 0.006302070803940296\n",
      "Epoch 56, average training loss (non-change): 0.0009275372140109539\n",
      "Epoch 57, model processing time: 0.0263 seconds\n",
      "Epoch 57, data loading time: 0.0000 seconds\n",
      "Epoch 57, average training loss (change): 0.006186679471284151\n",
      "Epoch 57, average training loss (non-change): 0.0008972715586423874\n",
      "Epoch 58, model processing time: 0.0275 seconds\n",
      "Epoch 58, data loading time: 0.0000 seconds\n",
      "Epoch 58, average training loss (change): 0.0060761054046452045\n",
      "Epoch 58, average training loss (non-change): 0.0008687523659318686\n",
      "Epoch 59, model processing time: 0.0261 seconds\n",
      "Epoch 59, data loading time: 0.0000 seconds\n",
      "Epoch 59, average training loss (change): 0.005969827529042959\n",
      "Epoch 59, average training loss (non-change): 0.0008419854566454887\n",
      "Found new best model with val loss 0.005351956933736801at epoch 59\n",
      "Iteration 0 Epoch 60/99\n",
      "----------\n",
      "Epoch 60, model processing time: 0.0250 seconds\n",
      "Epoch 60, data loading time: 0.0000 seconds\n",
      "Epoch 60, average training loss (change): 0.005867390893399715\n",
      "Epoch 60, average training loss (non-change): 0.0008169561624526978\n",
      "Epoch 61, model processing time: 0.0276 seconds\n",
      "Epoch 61, data loading time: 0.0000 seconds\n",
      "Epoch 61, average training loss (change): 0.005768406670540571\n",
      "Epoch 61, average training loss (non-change): 0.0007936148904263973\n",
      "Epoch 62, model processing time: 0.0257 seconds\n",
      "Epoch 62, data loading time: 0.0000 seconds\n",
      "Epoch 62, average training loss (change): 0.005672553554177284\n",
      "Epoch 62, average training loss (non-change): 0.0007719011045992374\n",
      "Epoch 63, model processing time: 0.0263 seconds\n",
      "Epoch 63, data loading time: 0.0000 seconds\n",
      "Epoch 63, average training loss (change): 0.005579546559602022\n",
      "Epoch 63, average training loss (non-change): 0.0007517351768910885\n",
      "Epoch 64, model processing time: 0.0260 seconds\n",
      "Epoch 64, data loading time: 0.0000 seconds\n",
      "Epoch 64, average training loss (change): 0.005489134229719639\n",
      "Epoch 64, average training loss (non-change): 0.0007330402731895447\n",
      "Epoch 65, model processing time: 0.0258 seconds\n",
      "Epoch 65, data loading time: 0.0000 seconds\n",
      "Epoch 65, average training loss (change): 0.005401136819273233\n",
      "Epoch 65, average training loss (non-change): 0.0007157087093219161\n",
      "Epoch 66, model processing time: 0.0294 seconds\n",
      "Epoch 66, data loading time: 0.0000 seconds\n",
      "Epoch 66, average training loss (change): 0.005315383896231651\n",
      "Epoch 66, average training loss (non-change): 0.0006996562005952001\n",
      "Epoch 67, model processing time: 0.0256 seconds\n",
      "Epoch 67, data loading time: 0.0000 seconds\n",
      "Epoch 67, average training loss (change): 0.005231744144111872\n",
      "Epoch 67, average training loss (non-change): 0.0006847918266430497\n",
      "Epoch 68, model processing time: 0.0274 seconds\n",
      "Epoch 68, data loading time: 0.0000 seconds\n",
      "Epoch 68, average training loss (change): 0.00515009555965662\n",
      "Epoch 68, average training loss (non-change): 0.0006710105808451772\n",
      "Epoch 69, model processing time: 0.0257 seconds\n",
      "Epoch 69, data loading time: 0.0000 seconds\n",
      "Epoch 69, average training loss (change): 0.0050703417509794235\n",
      "Epoch 69, average training loss (non-change): 0.000658230041153729\n",
      "Found new best model with val loss 0.004681626334786415at epoch 69\n",
      "Iteration 0 Epoch 70/99\n",
      "----------\n",
      "Epoch 70, model processing time: 0.0340 seconds\n",
      "Epoch 70, data loading time: 0.0000 seconds\n",
      "Epoch 70, average training loss (change): 0.004992404021322727\n",
      "Epoch 70, average training loss (non-change): 0.000646356726065278\n",
      "Epoch 71, model processing time: 0.0275 seconds\n",
      "Epoch 71, data loading time: 0.0000 seconds\n",
      "Epoch 71, average training loss (change): 0.004984730388969183\n",
      "Epoch 71, average training loss (non-change): 0.0006452413508668542\n",
      "Epoch 72, model processing time: 0.0261 seconds\n",
      "Epoch 72, data loading time: 0.0000 seconds\n",
      "Epoch 72, average training loss (change): 0.004977113101631403\n",
      "Epoch 72, average training loss (non-change): 0.0006441865116357803\n",
      "Epoch 73, model processing time: 0.0257 seconds\n",
      "Epoch 73, data loading time: 0.0000 seconds\n",
      "Epoch 73, average training loss (change): 0.0049695298075675964\n",
      "Epoch 73, average training loss (non-change): 0.0006431817309930921\n",
      "Epoch 74, model processing time: 0.0257 seconds\n",
      "Epoch 74, data loading time: 0.0000 seconds\n",
      "Epoch 74, average training loss (change): 0.004961994010955095\n",
      "Epoch 74, average training loss (non-change): 0.0006422256119549274\n",
      "Epoch 75, model processing time: 0.0277 seconds\n",
      "Epoch 75, data loading time: 0.0000 seconds\n",
      "Epoch 75, average training loss (change): 0.0049544875510036945\n",
      "Epoch 75, average training loss (non-change): 0.0006413055816665292\n",
      "Epoch 76, model processing time: 0.0255 seconds\n",
      "Epoch 76, data loading time: 0.0000 seconds\n",
      "Epoch 76, average training loss (change): 0.00494700251147151\n",
      "Epoch 76, average training loss (non-change): 0.0006404329324141145\n",
      "Epoch 77, model processing time: 0.0255 seconds\n",
      "Epoch 77, data loading time: 0.0000 seconds\n",
      "Epoch 77, average training loss (change): 0.004939543083310127\n",
      "Epoch 77, average training loss (non-change): 0.0006395854288712144\n",
      "Epoch 78, model processing time: 0.0277 seconds\n",
      "Epoch 78, data loading time: 0.0000 seconds\n",
      "Epoch 78, average training loss (change): 0.004932105541229248\n",
      "Epoch 78, average training loss (non-change): 0.000638767727650702\n",
      "Epoch 79, model processing time: 0.0260 seconds\n",
      "Epoch 79, data loading time: 0.0000 seconds\n",
      "Epoch 79, average training loss (change): 0.004924680106341839\n",
      "Epoch 79, average training loss (non-change): 0.000637976685538888\n",
      "Found new best model with val loss 0.004667655099183321at epoch 79\n",
      "Iteration 0 Epoch 80/99\n",
      "----------\n",
      "Epoch 80, model processing time: 0.0265 seconds\n",
      "Epoch 80, data loading time: 0.0000 seconds\n",
      "Epoch 80, average training loss (change): 0.004917273297905922\n",
      "Epoch 80, average training loss (non-change): 0.0006372135831043124\n",
      "Epoch 81, model processing time: 0.0270 seconds\n",
      "Epoch 81, data loading time: 0.0000 seconds\n",
      "Epoch 81, average training loss (change): 0.004909868352115154\n",
      "Epoch 81, average training loss (non-change): 0.0006364668952301145\n",
      "Epoch 82, model processing time: 0.0256 seconds\n",
      "Epoch 82, data loading time: 0.0000 seconds\n",
      "Epoch 82, average training loss (change): 0.004902474116533995\n",
      "Epoch 82, average training loss (non-change): 0.0006357425590977073\n",
      "Epoch 83, model processing time: 0.0266 seconds\n",
      "Epoch 83, data loading time: 0.0000 seconds\n",
      "Epoch 83, average training loss (change): 0.004895081743597984\n",
      "Epoch 83, average training loss (non-change): 0.0006350331241264939\n",
      "Epoch 84, model processing time: 0.0253 seconds\n",
      "Epoch 84, data loading time: 0.0000 seconds\n",
      "Epoch 84, average training loss (change): 0.004887696821242571\n",
      "Epoch 84, average training loss (non-change): 0.0006343291606754065\n",
      "Epoch 85, model processing time: 0.0245 seconds\n",
      "Epoch 85, data loading time: 0.0000 seconds\n",
      "Epoch 85, average training loss (change): 0.004880310036242008\n",
      "Epoch 85, average training loss (non-change): 0.0006336568621918559\n",
      "Epoch 86, model processing time: 0.0251 seconds\n",
      "Epoch 86, data loading time: 0.0000 seconds\n",
      "Epoch 86, average training loss (change): 0.004872919525951147\n",
      "Epoch 86, average training loss (non-change): 0.0006329761818051338\n",
      "Epoch 87, model processing time: 0.0255 seconds\n",
      "Epoch 87, data loading time: 0.0000 seconds\n",
      "Epoch 87, average training loss (change): 0.004865535534918308\n",
      "Epoch 87, average training loss (non-change): 0.0006323108682408929\n",
      "Epoch 88, model processing time: 0.0251 seconds\n",
      "Epoch 88, data loading time: 0.0000 seconds\n",
      "Epoch 88, average training loss (change): 0.0048581394366919994\n",
      "Epoch 88, average training loss (non-change): 0.0006316548679023981\n",
      "Epoch 89, model processing time: 0.0244 seconds\n",
      "Epoch 89, data loading time: 0.0000 seconds\n",
      "Epoch 89, average training loss (change): 0.004850741941481829\n",
      "Epoch 89, average training loss (non-change): 0.0006310043390840292\n",
      "Found new best model with val loss 0.0046095699071884155at epoch 89\n",
      "Iteration 0 Epoch 90/99\n",
      "----------\n",
      "Epoch 90, model processing time: 0.0255 seconds\n",
      "Epoch 90, data loading time: 0.0000 seconds\n",
      "Epoch 90, average training loss (change): 0.0048433467745780945\n",
      "Epoch 90, average training loss (non-change): 0.0006303597474470735\n",
      "Epoch 91, model processing time: 0.0243 seconds\n",
      "Epoch 91, data loading time: 0.0000 seconds\n",
      "Epoch 91, average training loss (change): 0.004835940431803465\n",
      "Epoch 91, average training loss (non-change): 0.0006297094514593482\n",
      "Epoch 92, model processing time: 0.0246 seconds\n",
      "Epoch 92, data loading time: 0.0000 seconds\n",
      "Epoch 92, average training loss (change): 0.004828528501093388\n",
      "Epoch 92, average training loss (non-change): 0.0006290714954957366\n",
      "Epoch 93, model processing time: 0.0253 seconds\n",
      "Epoch 93, data loading time: 0.0000 seconds\n",
      "Epoch 93, average training loss (change): 0.004821114707738161\n",
      "Epoch 93, average training loss (non-change): 0.0006284306291490793\n",
      "Epoch 94, model processing time: 0.0245 seconds\n",
      "Epoch 94, data loading time: 0.0000 seconds\n",
      "Epoch 94, average training loss (change): 0.00481368275359273\n",
      "Epoch 94, average training loss (non-change): 0.0006277901120483875\n",
      "Epoch 95, model processing time: 0.0265 seconds\n",
      "Epoch 95, data loading time: 0.0000 seconds\n",
      "Epoch 95, average training loss (change): 0.004806254990398884\n",
      "Epoch 95, average training loss (non-change): 0.0006271546008065343\n",
      "Epoch 96, model processing time: 0.0261 seconds\n",
      "Epoch 96, data loading time: 0.0000 seconds\n",
      "Epoch 96, average training loss (change): 0.004798816982656717\n",
      "Epoch 96, average training loss (non-change): 0.0006265173433348536\n",
      "Epoch 97, model processing time: 0.0251 seconds\n",
      "Epoch 97, data loading time: 0.0000 seconds\n",
      "Epoch 97, average training loss (change): 0.004791362211108208\n",
      "Epoch 97, average training loss (non-change): 0.0006258714711293578\n",
      "Epoch 98, model processing time: 0.0249 seconds\n",
      "Epoch 98, data loading time: 0.0000 seconds\n",
      "Epoch 98, average training loss (change): 0.004783910233527422\n",
      "Epoch 98, average training loss (non-change): 0.0006252349121496081\n",
      "Epoch 99, model processing time: 0.0247 seconds\n",
      "Epoch 99, data loading time: 0.0000 seconds\n",
      "Epoch 99, average training loss (change): 0.004776445683091879\n",
      "Epoch 99, average training loss (non-change): 0.0006245889235287905\n",
      "Found new best model with val loss 0.004545525647699833at epoch 99\n",
      "Training complete in 0m 51s with train loss 0.00540 and validation loss 0.00455\n",
      "Model 0 Trained\n",
      "Corresponding AE Vector: tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "Corresponding AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Model 0\n",
      "Learned AE Guidance (Lower better): tensor([2.3026e-09, 2.3026e-09, 2.3026e-09, 2.3026e-09, 2.3026e-09, 2.3026e-09,\n",
      "        2.3026e-09, 2.3026e-09])\n",
      "Learning Done in 61.50415587425232 sec, Checking if exists low-objective ae vector...\n",
      "Found 1 init low-objective ae vectors!\n",
      "Checking AE Vector 0...\n",
      "Generating Graph Data with Current AE matrix...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 637/637 [00:00<00:00, 1103.45it/s]\n",
      "Created transition dataset with 509 transition pairs\n",
      "Created transition dataset with 128 transition pairs\n",
      "Checking the learned AP vector...\n",
      "Using the provided AE vector (Add): tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "Using the provided AE vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Del/Add Only Predicate. Checking Rows (Ops): tensor([1, 2, 3, 4, 5, 6, 7])\n",
      "Using Threshold: 0.999\n",
      "Passed!!!\n",
      "Precondition Passed for AE Vector 0! Add to the final list.\n",
      "AE Vector 0 (Add): tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "AE Vector 0 (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "/Users/libowen/Documents/Research/RSS2025/code/IVNTR/predicators/gnn/neupi.py:1595: RuntimeWarning: divide by zero encountered in divide\n",
      "  node.update_value(self.global_zero_loss / self.visits, self.guidance_th)\n",
      "/Users/libowen/Documents/Research/RSS2025/code/IVNTR/predicators/gnn/neupi.py:1595: RuntimeWarning: invalid value encountered in divide\n",
      "  node.update_value(self.global_zero_loss / self.visits, self.guidance_th)\n",
      "/Users/libowen/Documents/Research/RSS2025/code/IVNTR/predicators/gnn/neupi.py:1496: RuntimeWarning: Mean of empty slice.\n",
      "  if (non_zero_guidance.mean() < 0.4) or (non_zero_indexes.size == 0):\n",
      "/Users/libowen/opt/anaconda3/envs/ivntr/lib/python3.8/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Node [0 0 0 0 0 0 0 0] is fully expanded or has -inf value.\n",
      "New Frontier: []\n",
      "-----Iteration: 1 (neural_u_p1)------\n",
      "Generate 12 AE Vectors-Score Pairs for Predicate neural_u_p1\n",
      "GT AE Vec Provided, No Need to Generate AE Vectors.\n",
      "No more sat matrixes can be generated at iteration 1!\n",
      "Early Stopping at Iteration 1!\n",
      "******************Bi-level Optimization Done for neural_u_p1! Summary:******************\n",
      "After 1 iterations, we got 1 basic vectors:\n",
      "******************Predicate neural_u_p1_0(?sat0)******************\n",
      "AE Vector 0 (Add): tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "AE Vector 0 (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Guidance Score: tensor([2.3026e-09, 2.3026e-09, 2.3026e-09, 2.3026e-09, 2.3026e-09, 2.3026e-09,\n",
      "        2.3026e-09, 2.3026e-09])\n",
      "Skipping Negated / Quantified version for unary predicate...\n",
      "Skipping learning for HasChemX since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 0, 1, 0, 0, 0, 0])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for HasChemY since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 0, 0, 1, 0, 0, 0])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for IsCalibrated since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for ViewClear since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 1, 0, 0, 0, 0, 0, 0])\n",
      "AE Vector (Del): tensor([0, 0, 1, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Neural Learning Completed Successfully!\n",
      "   Learning trajectories: 50\n",
      "   Initial atom trajectory: <class 'list'> with 50 entries\n"
     ]
    }
   ],
   "source": [
    "# Test the neural learning process\n",
    "print(\"üß† Starting Neural Predicate Learning Process...\\n\")\n",
    "\n",
    "print(\"üìã Learning Process Overview:\")\n",
    "print(\"   1. Generate training data from demonstration trajectories\")\n",
    "print(\"   2. Setup input fields for neural predicates\") \n",
    "print(\"   3. Initialize Action-Effect (AE) matrix constraints\")\n",
    "print(\"   4. Compute input normalizers for stable training\")\n",
    "print(\"   5. Train neural networks using the effect vector as ground truth\")\n",
    "print(\"   6. Validate learned predicates on held-out data\")\n",
    "\n",
    "# Call the main neural learning method (line 2312 from bilevel_learning_approach.py)\n",
    "print(f\"\\nüöÄ Calling approach.learn_neural_predicates(dataset)...\")\n",
    "learning_trajectories, init_atom_traj = approach.learn_neural_predicates(dataset)\n",
    "\n",
    "print(f\"‚úÖ Neural Learning Completed Successfully!\")\n",
    "print(f\"   Learning trajectories: {len(learning_trajectories)}\")\n",
    "print(f\"   Initial atom trajectory: {type(init_atom_traj)} with {len(init_atom_traj)} entries\" if init_atom_traj else \"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kf7eaj04fh",
   "metadata": {},
   "source": [
    "## 6. Examining Learned Neural Predicate Results\n",
    "\n",
    "After neural learning completes, IVNTR stores detailed information about the learned predicates. Let's examine the results and understand what was learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "lk5os5du45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Detailed Learning Log File\n",
      "   Log file path: /Users/libowen/Documents/Research/RSS2025/code/IVNTR/docs/example_gt_vec_learning.log\n",
      "   You can examine the neural learning process in detail by checking this log file\n",
      "   The log contains training progress, loss values, and model statistics\n"
     ]
    }
   ],
   "source": [
    "# Point to log file for detailed learning process\n",
    "print(\"üìã Detailed Learning Log File\")\n",
    "print(f\"   Log file path: {log_file_path}\")\n",
    "print(f\"   You can examine the neural learning process in detail by checking this log file\")\n",
    "print(f\"   The log contains training progress, loss values, and model statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jg7ixvwzmc",
   "metadata": {},
   "source": [
    "## 7. Neural Informed Symbolic Search\n",
    "\n",
    "In practice, we **don't have the correct effect vector** when inventing new predicates - that's the whole point of predicate invention! A key insight from IVNTR is that searching the potential effect space can be **guided by neural learning**.\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "- **Effect space is exponential**: For 8 actions, we have {0, 1, -1}^8 = 6,561 possible effect vectors \n",
    "- **Naive search is intractable**: We can't try every combination\n",
    "- **Neural guidance**: Bad effect vectors lead to poor neural learning performance\n",
    "\n",
    "### The IVNTR Solution\n",
    "\n",
    "IVNTR uses **neural learning loss as a guidance signal** for symbolic search:\n",
    "1. Try a candidate effect vector\n",
    "2. Train a neural network using that vector's labels  \n",
    "3. Measure the learning performance (validation loss, accuracy)\n",
    "4. Use this score to guide search toward better effect vectors\n",
    "\n",
    "Let's demonstrate this by trying a **deliberately bad effect vector** and seeing how IVNTR detects it's wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "awpbxnb9rzw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Neural Informed Symbolic Search: Bad Effect Vector Example\n",
      "\n",
      "üìã Updating Configuration for Bad Effect Vector Experiment:\n",
      "   Config file: /Users/libowen/Documents/Research/RSS2025/code/IVNTR/docs/example_gt_vec_bad.yaml\n",
      "   Log file: /Users/libowen/Documents/Research/RSS2025/code/IVNTR/docs/example_bad_vec_learning.log\n",
      "\n",
      "üö® Bad Effect Vector: [0, 0, 1, 0, 0, 0, 0, 0]\n",
      "   This says: MoveTo action ADDS IsCalibrated\n",
      "   Reality: Calibrate action should ADD IsCalibrated\n",
      "   This is completely wrong semantically!\n",
      "\n",
      "‚úÖ Configuration updated for bad effect vector experiment\n"
     ]
    }
   ],
   "source": [
    "# Test the bad effect vector approach\n",
    "print(\"üîç Neural Informed Symbolic Search: Bad Effect Vector Example\\n\")\n",
    "\n",
    "# Update configuration to use bad effect vector\n",
    "bad_config_path = os.path.abspath(\"example_gt_vec_bad.yaml\") \n",
    "bad_log_file_path = os.path.abspath(\"example_bad_vec_learning.log\")\n",
    "\n",
    "print(\"üìã Updating Configuration for Bad Effect Vector Experiment:\")\n",
    "print(f\"   Config file: {bad_config_path}\")\n",
    "print(f\"   Log file: {bad_log_file_path}\")\n",
    "\n",
    "# Reset configuration with bad effect vector\n",
    "utils.reset_config({\n",
    "    \"device\": \"cpu\",\n",
    "    \"env\": \"satellites\",\n",
    "    \"approach\": \"ivntr\",\n",
    "    \"neupi_pred_config\": bad_config_path,\n",
    "    \"neupi_gt_ae_matrix\": True,\n",
    "    \"excluded_predicates\": \"ViewClear,IsCalibrated,HasChemX,HasChemY,Sees\",\n",
    "    \"neupi_do_normalization\": True,\n",
    "    \"num_train_tasks\": 50,\n",
    "    \"num_test_tasks\": 20,\n",
    "    \"seed\": 0,\n",
    "    \"bilevel_plan_without_sim\": False,\n",
    "    \"exclude_domain_feat\": None,\n",
    "    \"log_file\": bad_log_file_path,\n",
    "})\n",
    "\n",
    "# Reinitialize logging for bad vector experiment\n",
    "handlers = [logging.StreamHandler()]\n",
    "handlers.append(logging.FileHandler(CFG.log_file, mode='w'))\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(message)s\",\n",
    "                    handlers=handlers,\n",
    "                    force=True)\n",
    "\n",
    "CFG.seed = 42\n",
    "CFG.num_train_tasks = 50\n",
    "CFG.satellites_num_sat_train = [2, 3]\n",
    "CFG.satellites_num_obj_train = [2, 3]\n",
    "CFG.timeout = 10.0\n",
    "CFG.demonstrator = \"oracle\"\n",
    "CFG.max_initial_demos = 50\n",
    "\n",
    "# Analyze the bad effect vector\n",
    "bad_effect_vector = [0, 0, 1, 0, 0, 0, 0, 0]  # MoveTo action adds IsCalibrated (wrong!)\n",
    "print(f\"\\nüö® Bad Effect Vector: {bad_effect_vector}\")\n",
    "print(f\"   This says: MoveTo action ADDS IsCalibrated\")\n",
    "print(f\"   Reality: Calibrate action should ADD IsCalibrated\")\n",
    "print(f\"   This is completely wrong semantically!\")\n",
    "\n",
    "print(\"\\n‚úÖ Configuration updated for bad effect vector experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9jddyo549d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Options (Clusters): Calibrate, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): MoveAway, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): MoveTo, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): ShootChemX, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): ShootChemY, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): UseCamera, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): UseGeiger, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Options (Clusters): UseInfraRed, Arguments: [Type(name='satellite'), Type(name='object')]\n",
      "Using: satellite_0\n",
      "Predicate Type TOBE Invented: neural_u_p1\n",
      "Learning Conifg: {'name': 'neural_u_p1', 'types': ['satellite'], 'gt': [[0, 0, 1, 0, 0, 0, 0, 0]], 'ent_idx': [0], 'architecture': {'type': 'MLP', 'layer_size': 32, 'initializer': 'xavier'}, 'optimizer': {'type': 'AdamW', 'kwargs': {'lr': 0.001}}, 'lr_scheduler': {'type': 'StepLR', 'kwargs': {'step_size': 70, 'gamma': 0.1}}, 'batch_vect_num': 12, 'batch_size': 512, 'epochs': 100, 'gumbel_temp': 0.66, 'val_freq': 10, 'num_iter': 5, 'matrix_vec_try': 100, 'search_tree_max_level': 1, 'guidance_thresh': 0.05, 'loss_thresh': 0.005, 'skip_train': False}\n",
      "GT Provided for Predicate neural_u_p1!\n",
      "GT AE Vector: [0, 0, 1, 0, 0, 0, 0, 0]\n",
      "/Users/libowen/Documents/Research/RSS2025/code/IVNTR/predicators/gnn/neupi_utils.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cate_vector_sampled = torch.tensor(one).unsqueeze(0)\n",
      "Search Region Not Provided, Search All possible effects\n",
      "Quantified Decision Boundary Not Provided, Use 0.5\n",
      "Belived Ent Idx for Predicate: CameraReadingTaken\n",
      "Using: satellite_0\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): CameraReadingTaken\n",
      "Belived Ent Idx for Predicate: GeigerReadingTaken\n",
      "Using: satellite_0\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): GeigerReadingTaken\n",
      "Belived Ent Idx for Predicate: HasChemX\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): HasChemX\n",
      "Belived Ent Idx for Predicate: HasChemY\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): HasChemY\n",
      "Belived Ent Idx for Predicate: InfraredReadingTaken\n",
      "Using: satellite_0\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): InfraredReadingTaken\n",
      "Belived Ent Idx for Predicate: IsCalibrated\n",
      "Using: satellite_0\n",
      "Provided Predicate (No Learning): IsCalibrated\n",
      "Belived Ent Idx for Predicate: Sees\n",
      "Using: satellite_0\n",
      "Using: object_0\n",
      "Provided Predicate (No Learning): Sees\n",
      "Belived Ent Idx for Predicate: ViewClear\n",
      "Using: satellite_0\n",
      "Provided Predicate (No Learning): ViewClear\n",
      "Constructing NeuPi Data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Creating New IVNTR Approach with Bad Effect Vector...\n",
      "\n",
      "‚úÖ Bad Effect Vector Approach Created: ivntr\n",
      "\n",
      "üé¨ Using Existing Demonstration Dataset...\n",
      "   Dataset trajectories: 50\n",
      "\n",
      "üö® Running Neural Learning with BAD Effect Vector...\n",
      "   Expected: Poor learning performance due to wrong labels\n",
      "   Effect vector: [0, 0, 1, 0, 0, 0, 0, 0] (MoveTo adds IsCalibrated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 72.95it/s] \n",
      "Low-level feat not changed for Predicate HasChemX in Row 0\n",
      "Low-level feat not changed for Predicate HasChemX in Row 0\n",
      "Low-level feat not changed for Predicate HasChemY in Row 0\n",
      "Low-level feat not changed for Predicate HasChemY in Row 0\n",
      "Low-level feat not changed for Predicate HasChemX in Row 1\n",
      "Low-level feat not changed for Predicate HasChemX in Row 1\n",
      "Low-level feat not changed for Predicate HasChemY in Row 1\n",
      "Low-level feat not changed for Predicate HasChemY in Row 1\n",
      "Low-level feat not changed for Predicate HasChemX in Row 2\n",
      "Low-level feat not changed for Predicate HasChemX in Row 2\n",
      "Low-level feat not changed for Predicate HasChemY in Row 2\n",
      "Low-level feat not changed for Predicate HasChemY in Row 2\n",
      "Low-level feat not changed for Predicate neural_u_p1 in Row 3\n",
      "Low-level feat not changed for Predicate neural_u_p1 in Row 3\n",
      "Low-level feat not changed for Predicate IsCalibrated in Row 3\n",
      "Low-level feat not changed for Predicate IsCalibrated in Row 3\n",
      "Low-level feat not changed for Predicate ViewClear in Row 3\n",
      "Low-level feat not changed for Predicate ViewClear in Row 3\n",
      "Low-level feat not changed for Predicate neural_u_p1 in Row 4\n",
      "Low-level feat not changed for Predicate neural_u_p1 in Row 4\n",
      "Low-level feat not changed for Predicate IsCalibrated in Row 4\n",
      "Low-level feat not changed for Predicate IsCalibrated in Row 4\n",
      "Low-level feat not changed for Predicate ViewClear in Row 4\n",
      "Low-level feat not changed for Predicate ViewClear in Row 4\n",
      "Low-level feat not changed for Predicate HasChemX in Row 5\n",
      "Low-level feat not changed for Predicate HasChemX in Row 5\n",
      "Low-level feat not changed for Predicate HasChemY in Row 5\n",
      "Low-level feat not changed for Predicate HasChemY in Row 5\n",
      "Low-level feat not changed for Predicate HasChemX in Row 6\n",
      "Low-level feat not changed for Predicate HasChemX in Row 6\n",
      "Low-level feat not changed for Predicate HasChemY in Row 6\n",
      "Low-level feat not changed for Predicate HasChemY in Row 6\n",
      "Low-level feat not changed for Predicate HasChemX in Row 7\n",
      "Low-level feat not changed for Predicate HasChemX in Row 7\n",
      "Low-level feat not changed for Predicate HasChemY in Row 7\n",
      "Low-level feat not changed for Predicate HasChemY in Row 7\n",
      "Predicate neural_u_p1 Not Provided for action Calibrate in Data\n",
      "Predicate neural_u_p1 Not Provided for action MoveAway in Data\n",
      "Predicate neural_u_p1 Not Provided for action MoveTo in Data\n",
      "Predicate neural_u_p1 Not Provided for action ShootChemX in Data\n",
      "Predicate neural_u_p1 Not Provided for action ShootChemY in Data\n",
      "Action UseCamera Definitely has no effect for pred neural_u_p1\n",
      "Action UseGeiger Definitely has no effect for pred neural_u_p1\n",
      "Action UseInfraRed Definitely has no effect for pred neural_u_p1\n",
      "Action Calibrate Definitely has no effect for pred CameraReadingTaken\n",
      "Action MoveAway Definitely has no effect for pred CameraReadingTaken\n",
      "Action MoveTo Definitely has no effect for pred CameraReadingTaken\n",
      "Action ShootChemX Definitely has no effect for pred CameraReadingTaken\n",
      "Action ShootChemY Definitely has no effect for pred CameraReadingTaken\n",
      "Action UseCamera Definitely has add effect for pred CameraReadingTaken\n",
      "Action UseGeiger Definitely has no effect for pred CameraReadingTaken\n",
      "Action UseInfraRed Definitely has no effect for pred CameraReadingTaken\n",
      "Action Calibrate Definitely has no effect for pred GeigerReadingTaken\n",
      "Action MoveAway Definitely has no effect for pred GeigerReadingTaken\n",
      "Action MoveTo Definitely has no effect for pred GeigerReadingTaken\n",
      "Action ShootChemX Definitely has no effect for pred GeigerReadingTaken\n",
      "Action ShootChemY Definitely has no effect for pred GeigerReadingTaken\n",
      "Action UseCamera Definitely has no effect for pred GeigerReadingTaken\n",
      "Action UseGeiger Definitely has add effect for pred GeigerReadingTaken\n",
      "Action UseInfraRed Definitely has no effect for pred GeigerReadingTaken\n",
      "Action Calibrate Definitely has no effect for pred HasChemX\n",
      "Action MoveAway Definitely has no effect for pred HasChemX\n",
      "Action MoveTo Definitely has no effect for pred HasChemX\n",
      "Action ShootChemX Definitely has add effect for pred HasChemX\n",
      "Action ShootChemY Definitely has no effect for pred HasChemX\n",
      "Action UseCamera Definitely has no effect for pred HasChemX\n",
      "Action UseGeiger Definitely has no effect for pred HasChemX\n",
      "Action UseInfraRed Definitely has no effect for pred HasChemX\n",
      "Action Calibrate Definitely has no effect for pred HasChemY\n",
      "Action MoveAway Definitely has no effect for pred HasChemY\n",
      "Action MoveTo Definitely has no effect for pred HasChemY\n",
      "Action ShootChemX Definitely has no effect for pred HasChemY\n",
      "Action ShootChemY Definitely has add effect for pred HasChemY\n",
      "Action UseCamera Definitely has no effect for pred HasChemY\n",
      "Action UseGeiger Definitely has no effect for pred HasChemY\n",
      "Action UseInfraRed Definitely has no effect for pred HasChemY\n",
      "Action Calibrate Definitely has no effect for pred InfraredReadingTaken\n",
      "Action MoveAway Definitely has no effect for pred InfraredReadingTaken\n",
      "Action MoveTo Definitely has no effect for pred InfraredReadingTaken\n",
      "Action ShootChemX Definitely has no effect for pred InfraredReadingTaken\n",
      "Action ShootChemY Definitely has no effect for pred InfraredReadingTaken\n",
      "Action UseCamera Definitely has no effect for pred InfraredReadingTaken\n",
      "Action UseGeiger Definitely has no effect for pred InfraredReadingTaken\n",
      "Action UseInfraRed Definitely has add effect for pred InfraredReadingTaken\n",
      "Action Calibrate Definitely has add effect for pred IsCalibrated\n",
      "Action MoveAway Definitely has no effect for pred IsCalibrated\n",
      "Action MoveTo Definitely has no effect for pred IsCalibrated\n",
      "Action ShootChemX Definitely has no effect for pred IsCalibrated\n",
      "Action ShootChemY Definitely has no effect for pred IsCalibrated\n",
      "Action UseCamera Definitely has no effect for pred IsCalibrated\n",
      "Action UseGeiger Definitely has no effect for pred IsCalibrated\n",
      "Action UseInfraRed Definitely has no effect for pred IsCalibrated\n",
      "Action Calibrate Definitely has no effect for pred Sees\n",
      "Action MoveAway Definitely has delete effect for pred Sees\n",
      "Action MoveTo Definitely has add effect for pred Sees\n",
      "Action ShootChemX Definitely has no effect for pred Sees\n",
      "Action ShootChemY Definitely has no effect for pred Sees\n",
      "Action UseCamera Definitely has no effect for pred Sees\n",
      "Action UseGeiger Definitely has no effect for pred Sees\n",
      "Action UseInfraRed Definitely has no effect for pred Sees\n",
      "Action Calibrate Definitely has no effect for pred ViewClear\n",
      "Action MoveAway Definitely has add effect for pred ViewClear\n",
      "Action MoveTo Definitely has delete effect for pred ViewClear\n",
      "Action ShootChemX Definitely has no effect for pred ViewClear\n",
      "Action ShootChemY Definitely has no effect for pred ViewClear\n",
      "Action UseCamera Definitely has no effect for pred ViewClear\n",
      "Action UseGeiger Definitely has no effect for pred ViewClear\n",
      "Action UseInfraRed Definitely has no effect for pred ViewClear\n",
      "Computing Normalizer for Inouts...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 637/637 [00:00<00:00, 2050.58it/s]\n",
      "Skipping learning for CameraReadingTaken since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 0, 0, 0, 1, 0, 0])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for GeigerReadingTaken since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 0, 0, 0, 0, 1, 0])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for InfraredReadingTaken since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 0, 0, 0, 0, 0, 1])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for Sees since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 1, 0, 0, 0, 0, 0])\n",
      "AE Vector (Del): tensor([0, 1, 0, 0, 0, 0, 0, 0])\n",
      "**************Learning Typed Predicate: neural_u_p1**************\n",
      "Learning Config: {'name': 'neural_u_p1', 'types': ['satellite'], 'gt': [[0, 0, 1, 0, 0, 0, 0, 0]], 'ent_idx': [0], 'architecture': {'type': 'MLP', 'layer_size': 32, 'initializer': 'xavier'}, 'optimizer': {'type': 'AdamW', 'kwargs': {'lr': 0.001}}, 'lr_scheduler': {'type': 'StepLR', 'kwargs': {'step_size': 70, 'gamma': 0.1}}, 'batch_vect_num': 12, 'batch_size': 512, 'epochs': 100, 'gumbel_temp': 0.66, 'val_freq': 10, 'num_iter': 5, 'matrix_vec_try': 100, 'search_tree_max_level': 1, 'guidance_thresh': 0.05, 'loss_thresh': 0.005, 'skip_train': False, 'decision_b': 0.5}\n",
      "Created transition dataset with 1 transition pairs\n",
      "Training from scratch.\n",
      "***************Bi-level Optimizing (neural_u_p1)***************\n",
      "-----Iteration: 0 (neural_u_p1)------\n",
      "Generate 12 AE Vectors-Score Pairs for Predicate neural_u_p1\n",
      "GT AE Vec Provided, No Need to Generate AE Vectors.\n",
      "Optimizing 1 Neural Models with AE Vector from BO...\n",
      "*******Vec 0 (neural_u_p1)*******\n",
      "(Add): \n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0])\n",
      "(Del): \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Generating Graph Data with Current AE matrix...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 637/637 [00:00<00:00, 1171.46it/s]\n",
      "Created transition dataset with 509 transition pairs\n",
      "Created transition dataset with 128 transition pairs\n",
      "Training Neural Model 0...\n",
      "Predicat 1 Iteration 0 | Starting process 0\n",
      "Iteration 0 Epoch 0/99\n",
      "----------\n",
      "Epoch 0, model processing time: 0.0427 seconds\n",
      "Epoch 0, data loading time: 0.0000 seconds\n",
      "Epoch 0, average training loss (change): 0.7999709248542786\n",
      "Epoch 0, average training loss (non-change): 0.008833817206323147\n",
      "Found new best model with val loss 100.0at epoch 0\n",
      "Epoch 1, model processing time: 0.0268 seconds\n",
      "Epoch 1, data loading time: 0.0000 seconds\n",
      "Epoch 1, average training loss (change): 0.7217023372650146\n",
      "Epoch 1, average training loss (non-change): 0.005897631868720055\n",
      "Epoch 2, model processing time: 0.0265 seconds\n",
      "Epoch 2, data loading time: 0.0000 seconds\n",
      "Epoch 2, average training loss (change): 0.6905587315559387\n",
      "Epoch 2, average training loss (non-change): 0.004762373864650726\n",
      "Epoch 3, model processing time: 0.0253 seconds\n",
      "Epoch 3, data loading time: 0.0000 seconds\n",
      "Epoch 3, average training loss (change): 0.681906521320343\n",
      "Epoch 3, average training loss (non-change): 0.004446692299097776\n",
      "Epoch 4, model processing time: 0.0330 seconds\n",
      "Epoch 4, data loading time: 0.0000 seconds\n",
      "Epoch 4, average training loss (change): 0.6795080304145813\n",
      "Epoch 4, average training loss (non-change): 0.004305888433009386\n",
      "Epoch 5, model processing time: 0.0252 seconds\n",
      "Epoch 5, data loading time: 0.0000 seconds\n",
      "Epoch 5, average training loss (change): 0.6773359775543213\n",
      "Epoch 5, average training loss (non-change): 0.004104221239686012\n",
      "Epoch 6, model processing time: 0.0258 seconds\n",
      "Epoch 6, data loading time: 0.0000 seconds\n",
      "Epoch 6, average training loss (change): 0.6740240454673767\n",
      "Epoch 6, average training loss (non-change): 0.003804309293627739\n",
      "Epoch 7, model processing time: 0.0263 seconds\n",
      "Epoch 7, data loading time: 0.0000 seconds\n",
      "Epoch 7, average training loss (change): 0.6699739098548889\n",
      "Epoch 7, average training loss (non-change): 0.0034430776722729206\n",
      "Epoch 8, model processing time: 0.0256 seconds\n",
      "Epoch 8, data loading time: 0.0000 seconds\n",
      "Epoch 8, average training loss (change): 0.6659952998161316\n",
      "Epoch 8, average training loss (non-change): 0.0030745086260139942\n",
      "Epoch 9, model processing time: 0.0270 seconds\n",
      "Epoch 9, data loading time: 0.0000 seconds\n",
      "Epoch 9, average training loss (change): 0.6627234220504761\n",
      "Epoch 9, average training loss (non-change): 0.002745009958744049\n",
      "Found new best model with val loss 0.7202061414718628at epoch 9\n",
      "Iteration 0 Epoch 10/99\n",
      "----------\n",
      "Epoch 10, model processing time: 0.0427 seconds\n",
      "Epoch 10, data loading time: 0.0000 seconds\n",
      "Epoch 10, average training loss (change): 0.6603749394416809\n",
      "Epoch 10, average training loss (non-change): 0.0024832759518176317\n",
      "Epoch 11, model processing time: 0.0296 seconds\n",
      "Epoch 11, data loading time: 0.0000 seconds\n",
      "Epoch 11, average training loss (change): 0.6587467193603516\n",
      "Epoch 11, average training loss (non-change): 0.0022986209951341152\n",
      "Epoch 12, model processing time: 0.0265 seconds\n",
      "Epoch 12, data loading time: 0.0000 seconds\n",
      "Epoch 12, average training loss (change): 0.6574398279190063\n",
      "Epoch 12, average training loss (non-change): 0.0021851544734090567\n",
      "Epoch 13, model processing time: 0.0344 seconds\n",
      "Epoch 13, data loading time: 0.0000 seconds\n",
      "Epoch 13, average training loss (change): 0.6561069488525391\n",
      "Epoch 13, average training loss (non-change): 0.0021292215678840876\n",
      "Epoch 14, model processing time: 0.0270 seconds\n",
      "Epoch 14, data loading time: 0.0000 seconds\n",
      "Epoch 14, average training loss (change): 0.6545655727386475\n",
      "Epoch 14, average training loss (non-change): 0.0021158666349947453\n",
      "Epoch 15, model processing time: 0.0268 seconds\n",
      "Epoch 15, data loading time: 0.0000 seconds\n",
      "Epoch 15, average training loss (change): 0.6527837514877319\n",
      "Epoch 15, average training loss (non-change): 0.002132680034264922\n",
      "Epoch 16, model processing time: 0.0256 seconds\n",
      "Epoch 16, data loading time: 0.0000 seconds\n",
      "Epoch 16, average training loss (change): 0.6508064270019531\n",
      "Epoch 16, average training loss (non-change): 0.002171141328290105\n",
      "Epoch 17, model processing time: 0.0243 seconds\n",
      "Epoch 17, data loading time: 0.0000 seconds\n",
      "Epoch 17, average training loss (change): 0.6486796140670776\n",
      "Epoch 17, average training loss (non-change): 0.002226543379947543\n",
      "Epoch 18, model processing time: 0.0282 seconds\n",
      "Epoch 18, data loading time: 0.0000 seconds\n",
      "Epoch 18, average training loss (change): 0.6464253067970276\n",
      "Epoch 18, average training loss (non-change): 0.0022972491569817066\n",
      "Epoch 19, model processing time: 0.0268 seconds\n",
      "Epoch 19, data loading time: 0.0000 seconds\n",
      "Epoch 19, average training loss (change): 0.6440703868865967\n",
      "Epoch 19, average training loss (non-change): 0.002384194638580084\n",
      "Found new best model with val loss 0.707775354385376at epoch 19\n",
      "Iteration 0 Epoch 20/99\n",
      "----------\n",
      "Epoch 20, model processing time: 0.0256 seconds\n",
      "Epoch 20, data loading time: 0.0000 seconds\n",
      "Epoch 20, average training loss (change): 0.6416842937469482\n",
      "Epoch 20, average training loss (non-change): 0.002489537000656128\n",
      "Epoch 21, model processing time: 0.0255 seconds\n",
      "Epoch 21, data loading time: 0.0000 seconds\n",
      "Epoch 21, average training loss (change): 0.6393831372261047\n",
      "Epoch 21, average training loss (non-change): 0.0026149507611989975\n",
      "Epoch 22, model processing time: 0.0267 seconds\n",
      "Epoch 22, data loading time: 0.0000 seconds\n",
      "Epoch 22, average training loss (change): 0.6372568011283875\n",
      "Epoch 22, average training loss (non-change): 0.0027603288181126118\n",
      "Epoch 23, model processing time: 0.0260 seconds\n",
      "Epoch 23, data loading time: 0.0000 seconds\n",
      "Epoch 23, average training loss (change): 0.6352824568748474\n",
      "Epoch 23, average training loss (non-change): 0.002923519816249609\n",
      "Epoch 24, model processing time: 0.0259 seconds\n",
      "Epoch 24, data loading time: 0.0000 seconds\n",
      "Epoch 24, average training loss (change): 0.6333705186843872\n",
      "Epoch 24, average training loss (non-change): 0.00309997471049428\n",
      "Epoch 25, model processing time: 0.0250 seconds\n",
      "Epoch 25, data loading time: 0.0000 seconds\n",
      "Epoch 25, average training loss (change): 0.6314126253128052\n",
      "Epoch 25, average training loss (non-change): 0.003283779602497816\n",
      "Epoch 26, model processing time: 0.0261 seconds\n",
      "Epoch 26, data loading time: 0.0000 seconds\n",
      "Epoch 26, average training loss (change): 0.6293035745620728\n",
      "Epoch 26, average training loss (non-change): 0.003469616174697876\n",
      "Epoch 27, model processing time: 0.0264 seconds\n",
      "Epoch 27, data loading time: 0.0000 seconds\n",
      "Epoch 27, average training loss (change): 0.6269916296005249\n",
      "Epoch 27, average training loss (non-change): 0.0036542117595672607\n",
      "Epoch 28, model processing time: 0.0259 seconds\n",
      "Epoch 28, data loading time: 0.0000 seconds\n",
      "Epoch 28, average training loss (change): 0.6245109438896179\n",
      "Epoch 28, average training loss (non-change): 0.0038364510983228683\n",
      "Epoch 29, model processing time: 0.0263 seconds\n",
      "Epoch 29, data loading time: 0.0000 seconds\n",
      "Epoch 29, average training loss (change): 0.621954083442688\n",
      "Epoch 29, average training loss (non-change): 0.0040161944925785065\n",
      "Found new best model with val loss 0.7017202973365784at epoch 29\n",
      "Iteration 0 Epoch 30/99\n",
      "----------\n",
      "Epoch 30, model processing time: 0.0326 seconds\n",
      "Epoch 30, data loading time: 0.0000 seconds\n",
      "Epoch 30, average training loss (change): 0.6193921566009521\n",
      "Epoch 30, average training loss (non-change): 0.004192921798676252\n",
      "Epoch 31, model processing time: 0.0260 seconds\n",
      "Epoch 31, data loading time: 0.0000 seconds\n",
      "Epoch 31, average training loss (change): 0.6168305277824402\n",
      "Epoch 31, average training loss (non-change): 0.004365713335573673\n",
      "Epoch 32, model processing time: 0.0253 seconds\n",
      "Epoch 32, data loading time: 0.0000 seconds\n",
      "Epoch 32, average training loss (change): 0.6142293214797974\n",
      "Epoch 32, average training loss (non-change): 0.004533940926194191\n",
      "Epoch 33, model processing time: 0.0265 seconds\n",
      "Epoch 33, data loading time: 0.0000 seconds\n",
      "Epoch 33, average training loss (change): 0.6115394234657288\n",
      "Epoch 33, average training loss (non-change): 0.004698397126048803\n",
      "Epoch 34, model processing time: 0.0258 seconds\n",
      "Epoch 34, data loading time: 0.0000 seconds\n",
      "Epoch 34, average training loss (change): 0.6087333559989929\n",
      "Epoch 34, average training loss (non-change): 0.004862481262534857\n",
      "Epoch 35, model processing time: 0.0249 seconds\n",
      "Epoch 35, data loading time: 0.0000 seconds\n",
      "Epoch 35, average training loss (change): 0.6058217287063599\n",
      "Epoch 35, average training loss (non-change): 0.005031477194279432\n",
      "Epoch 36, model processing time: 0.0245 seconds\n",
      "Epoch 36, data loading time: 0.0000 seconds\n",
      "Epoch 36, average training loss (change): 0.6028341054916382\n",
      "Epoch 36, average training loss (non-change): 0.005210536066442728\n",
      "Epoch 37, model processing time: 0.0255 seconds\n",
      "Epoch 37, data loading time: 0.0000 seconds\n",
      "Epoch 37, average training loss (change): 0.5997774600982666\n",
      "Epoch 37, average training loss (non-change): 0.005402297247201204\n",
      "Epoch 38, model processing time: 0.0248 seconds\n",
      "Epoch 38, data loading time: 0.0000 seconds\n",
      "Epoch 38, average training loss (change): 0.5966233015060425\n",
      "Epoch 38, average training loss (non-change): 0.005606313701719046\n",
      "Epoch 39, model processing time: 0.0252 seconds\n",
      "Epoch 39, data loading time: 0.0000 seconds\n",
      "Epoch 39, average training loss (change): 0.5933631062507629\n",
      "Epoch 39, average training loss (non-change): 0.005820424761623144\n",
      "Iteration 0 Epoch 40/99\n",
      "----------\n",
      "Epoch 40, model processing time: 0.0274 seconds\n",
      "Epoch 40, data loading time: 0.0000 seconds\n",
      "Epoch 40, average training loss (change): 0.5900267362594604\n",
      "Epoch 40, average training loss (non-change): 0.006042448338121176\n",
      "Epoch 41, model processing time: 0.0257 seconds\n",
      "Epoch 41, data loading time: 0.0000 seconds\n",
      "Epoch 41, average training loss (change): 0.5866126418113708\n",
      "Epoch 41, average training loss (non-change): 0.006268091034144163\n",
      "Epoch 42, model processing time: 0.0258 seconds\n",
      "Epoch 42, data loading time: 0.0000 seconds\n",
      "Epoch 42, average training loss (change): 0.5830573439598083\n",
      "Epoch 42, average training loss (non-change): 0.0064885602332651615\n",
      "Epoch 43, model processing time: 0.0603 seconds\n",
      "Epoch 43, data loading time: 0.0000 seconds\n",
      "Epoch 43, average training loss (change): 0.5792936682701111\n",
      "Epoch 43, average training loss (non-change): 0.006691478192806244\n",
      "Epoch 44, model processing time: 0.0267 seconds\n",
      "Epoch 44, data loading time: 0.0000 seconds\n",
      "Epoch 44, average training loss (change): 0.5753231644630432\n",
      "Epoch 44, average training loss (non-change): 0.006863788701593876\n",
      "Epoch 45, model processing time: 0.0253 seconds\n",
      "Epoch 45, data loading time: 0.0000 seconds\n",
      "Epoch 45, average training loss (change): 0.5712458491325378\n",
      "Epoch 45, average training loss (non-change): 0.006994469091296196\n",
      "Epoch 46, model processing time: 0.0257 seconds\n",
      "Epoch 46, data loading time: 0.0000 seconds\n",
      "Epoch 46, average training loss (change): 0.5672443509101868\n",
      "Epoch 46, average training loss (non-change): 0.007076868787407875\n",
      "Epoch 47, model processing time: 0.0281 seconds\n",
      "Epoch 47, data loading time: 0.0000 seconds\n",
      "Epoch 47, average training loss (change): 0.5634635090827942\n",
      "Epoch 47, average training loss (non-change): 0.007114059291779995\n",
      "Epoch 48, model processing time: 0.0261 seconds\n",
      "Epoch 48, data loading time: 0.0000 seconds\n",
      "Epoch 48, average training loss (change): 0.5597698092460632\n",
      "Epoch 48, average training loss (non-change): 0.007141010835766792\n",
      "Epoch 49, model processing time: 0.0269 seconds\n",
      "Epoch 49, data loading time: 0.0000 seconds\n",
      "Epoch 49, average training loss (change): 0.5558542013168335\n",
      "Epoch 49, average training loss (non-change): 0.007213731296360493\n",
      "Iteration 0 Epoch 50/99\n",
      "----------\n",
      "Epoch 50, model processing time: 0.0255 seconds\n",
      "Epoch 50, data loading time: 0.0000 seconds\n",
      "Epoch 50, average training loss (change): 0.5519222617149353\n",
      "Epoch 50, average training loss (non-change): 0.007327784784138203\n",
      "Epoch 51, model processing time: 0.0265 seconds\n",
      "Epoch 51, data loading time: 0.0000 seconds\n",
      "Epoch 51, average training loss (change): 0.5481436848640442\n",
      "Epoch 51, average training loss (non-change): 0.007455894723534584\n",
      "Epoch 52, model processing time: 0.0259 seconds\n",
      "Epoch 52, data loading time: 0.0000 seconds\n",
      "Epoch 52, average training loss (change): 0.5444086790084839\n",
      "Epoch 52, average training loss (non-change): 0.007590589113533497\n",
      "Epoch 53, model processing time: 0.0275 seconds\n",
      "Epoch 53, data loading time: 0.0000 seconds\n",
      "Epoch 53, average training loss (change): 0.5402472615242004\n",
      "Epoch 53, average training loss (non-change): 0.007737342268228531\n",
      "Epoch 54, model processing time: 0.0255 seconds\n",
      "Epoch 54, data loading time: 0.0000 seconds\n",
      "Epoch 54, average training loss (change): 0.5358570218086243\n",
      "Epoch 54, average training loss (non-change): 0.00786592811346054\n",
      "Epoch 55, model processing time: 0.0275 seconds\n",
      "Epoch 55, data loading time: 0.0000 seconds\n",
      "Epoch 55, average training loss (change): 0.5312610864639282\n",
      "Epoch 55, average training loss (non-change): 0.007969099096953869\n",
      "Epoch 56, model processing time: 0.0258 seconds\n",
      "Epoch 56, data loading time: 0.0000 seconds\n",
      "Epoch 56, average training loss (change): 0.5265177488327026\n",
      "Epoch 56, average training loss (non-change): 0.008104396983981133\n",
      "Epoch 57, model processing time: 0.0244 seconds\n",
      "Epoch 57, data loading time: 0.0000 seconds\n",
      "Epoch 57, average training loss (change): 0.5217437148094177\n",
      "Epoch 57, average training loss (non-change): 0.00829820055514574\n",
      "Epoch 58, model processing time: 0.0265 seconds\n",
      "Epoch 58, data loading time: 0.0000 seconds\n",
      "Epoch 58, average training loss (change): 0.5170130729675293\n",
      "Epoch 58, average training loss (non-change): 0.008489372208714485\n",
      "Epoch 59, model processing time: 0.0273 seconds\n",
      "Epoch 59, data loading time: 0.0000 seconds\n",
      "Epoch 59, average training loss (change): 0.5123484134674072\n",
      "Epoch 59, average training loss (non-change): 0.008701572194695473\n",
      "Iteration 0 Epoch 60/99\n",
      "----------\n",
      "Epoch 60, model processing time: 0.0264 seconds\n",
      "Epoch 60, data loading time: 0.0000 seconds\n",
      "Epoch 60, average training loss (change): 0.5075312256813049\n",
      "Epoch 60, average training loss (non-change): 0.009017787873744965\n",
      "Epoch 61, model processing time: 0.0253 seconds\n",
      "Epoch 61, data loading time: 0.0000 seconds\n",
      "Epoch 61, average training loss (change): 0.5025290846824646\n",
      "Epoch 61, average training loss (non-change): 0.009284822270274162\n",
      "Epoch 62, model processing time: 0.0283 seconds\n",
      "Epoch 62, data loading time: 0.0000 seconds\n",
      "Epoch 62, average training loss (change): 0.49748605489730835\n",
      "Epoch 62, average training loss (non-change): 0.00964170042425394\n",
      "Epoch 63, model processing time: 0.0259 seconds\n",
      "Epoch 63, data loading time: 0.0000 seconds\n",
      "Epoch 63, average training loss (change): 0.49264124035835266\n",
      "Epoch 63, average training loss (non-change): 0.009872731752693653\n",
      "Epoch 64, model processing time: 0.0317 seconds\n",
      "Epoch 64, data loading time: 0.0000 seconds\n",
      "Epoch 64, average training loss (change): 0.48765769600868225\n",
      "Epoch 64, average training loss (non-change): 0.010161885060369968\n",
      "Epoch 65, model processing time: 0.0261 seconds\n",
      "Epoch 65, data loading time: 0.0000 seconds\n",
      "Epoch 65, average training loss (change): 0.4833374321460724\n",
      "Epoch 65, average training loss (non-change): 0.010154178366065025\n",
      "Epoch 66, model processing time: 0.0262 seconds\n",
      "Epoch 66, data loading time: 0.0000 seconds\n",
      "Epoch 66, average training loss (change): 0.4798934757709503\n",
      "Epoch 66, average training loss (non-change): 0.010426530614495277\n",
      "Epoch 67, model processing time: 0.0262 seconds\n",
      "Epoch 67, data loading time: 0.0000 seconds\n",
      "Epoch 67, average training loss (change): 0.47537580132484436\n",
      "Epoch 67, average training loss (non-change): 0.010335732251405716\n",
      "Epoch 68, model processing time: 0.0269 seconds\n",
      "Epoch 68, data loading time: 0.0000 seconds\n",
      "Epoch 68, average training loss (change): 0.46856871247291565\n",
      "Epoch 68, average training loss (non-change): 0.010738620534539223\n",
      "Epoch 69, model processing time: 0.0266 seconds\n",
      "Epoch 69, data loading time: 0.0000 seconds\n",
      "Epoch 69, average training loss (change): 0.4633355438709259\n",
      "Epoch 69, average training loss (non-change): 0.01099725067615509\n",
      "Iteration 0 Epoch 70/99\n",
      "----------\n",
      "Epoch 70, model processing time: 0.0242 seconds\n",
      "Epoch 70, data loading time: 0.0000 seconds\n",
      "Epoch 70, average training loss (change): 0.4596737027168274\n",
      "Epoch 70, average training loss (non-change): 0.011144887655973434\n",
      "Epoch 71, model processing time: 0.0259 seconds\n",
      "Epoch 71, data loading time: 0.0000 seconds\n",
      "Epoch 71, average training loss (change): 0.458572655916214\n",
      "Epoch 71, average training loss (non-change): 0.011190390214323997\n",
      "Epoch 72, model processing time: 0.0245 seconds\n",
      "Epoch 72, data loading time: 0.0000 seconds\n",
      "Epoch 72, average training loss (change): 0.457195520401001\n",
      "Epoch 72, average training loss (non-change): 0.011249585077166557\n",
      "Epoch 73, model processing time: 0.0245 seconds\n",
      "Epoch 73, data loading time: 0.0000 seconds\n",
      "Epoch 73, average training loss (change): 0.45635905861854553\n",
      "Epoch 73, average training loss (non-change): 0.011294740252196789\n",
      "Epoch 74, model processing time: 0.0249 seconds\n",
      "Epoch 74, data loading time: 0.0000 seconds\n",
      "Epoch 74, average training loss (change): 0.45615267753601074\n",
      "Epoch 74, average training loss (non-change): 0.011317596770823002\n",
      "Epoch 75, model processing time: 0.0249 seconds\n",
      "Epoch 75, data loading time: 0.0000 seconds\n",
      "Epoch 75, average training loss (change): 0.45600444078445435\n",
      "Epoch 75, average training loss (non-change): 0.011331843212246895\n",
      "Epoch 76, model processing time: 0.0261 seconds\n",
      "Epoch 76, data loading time: 0.0000 seconds\n",
      "Epoch 76, average training loss (change): 0.4554363191127777\n",
      "Epoch 76, average training loss (non-change): 0.011351700872182846\n",
      "Epoch 77, model processing time: 0.0263 seconds\n",
      "Epoch 77, data loading time: 0.0000 seconds\n",
      "Epoch 77, average training loss (change): 0.45447641611099243\n",
      "Epoch 77, average training loss (non-change): 0.011377323418855667\n",
      "Epoch 78, model processing time: 0.0291 seconds\n",
      "Epoch 78, data loading time: 0.0000 seconds\n",
      "Epoch 78, average training loss (change): 0.45349958539009094\n",
      "Epoch 78, average training loss (non-change): 0.011399094946682453\n",
      "Epoch 79, model processing time: 0.0260 seconds\n",
      "Epoch 79, data loading time: 0.0000 seconds\n",
      "Epoch 79, average training loss (change): 0.45285025238990784\n",
      "Epoch 79, average training loss (non-change): 0.011410292237997055\n",
      "Iteration 0 Epoch 80/99\n",
      "----------\n",
      "Epoch 80, model processing time: 0.0247 seconds\n",
      "Epoch 80, data loading time: 0.0000 seconds\n",
      "Epoch 80, average training loss (change): 0.45249634981155396\n",
      "Epoch 80, average training loss (non-change): 0.011417310684919357\n",
      "Epoch 81, model processing time: 0.0286 seconds\n",
      "Epoch 81, data loading time: 0.0000 seconds\n",
      "Epoch 81, average training loss (change): 0.4520861506462097\n",
      "Epoch 81, average training loss (non-change): 0.011434739455580711\n",
      "Epoch 82, model processing time: 0.0264 seconds\n",
      "Epoch 82, data loading time: 0.0000 seconds\n",
      "Epoch 82, average training loss (change): 0.4513772130012512\n",
      "Epoch 82, average training loss (non-change): 0.0114704305306077\n",
      "Epoch 83, model processing time: 0.0272 seconds\n",
      "Epoch 83, data loading time: 0.0000 seconds\n",
      "Epoch 83, average training loss (change): 0.45047613978385925\n",
      "Epoch 83, average training loss (non-change): 0.011519389227032661\n",
      "Epoch 84, model processing time: 0.0260 seconds\n",
      "Epoch 84, data loading time: 0.0000 seconds\n",
      "Epoch 84, average training loss (change): 0.4496612548828125\n",
      "Epoch 84, average training loss (non-change): 0.011569919064640999\n",
      "Epoch 85, model processing time: 0.0268 seconds\n",
      "Epoch 85, data loading time: 0.0000 seconds\n",
      "Epoch 85, average training loss (change): 0.44906628131866455\n",
      "Epoch 85, average training loss (non-change): 0.011613829992711544\n",
      "Epoch 86, model processing time: 0.0259 seconds\n",
      "Epoch 86, data loading time: 0.0000 seconds\n",
      "Epoch 86, average training loss (change): 0.44856971502304077\n",
      "Epoch 86, average training loss (non-change): 0.011651536449790001\n",
      "Epoch 87, model processing time: 0.0261 seconds\n",
      "Epoch 87, data loading time: 0.0000 seconds\n",
      "Epoch 87, average training loss (change): 0.4479699432849884\n",
      "Epoch 87, average training loss (non-change): 0.011687792837619781\n",
      "Epoch 88, model processing time: 0.0251 seconds\n",
      "Epoch 88, data loading time: 0.0000 seconds\n",
      "Epoch 88, average training loss (change): 0.4472126066684723\n",
      "Epoch 88, average training loss (non-change): 0.011724305339157581\n",
      "Epoch 89, model processing time: 0.0262 seconds\n",
      "Epoch 89, data loading time: 0.0000 seconds\n",
      "Epoch 89, average training loss (change): 0.4464157223701477\n",
      "Epoch 89, average training loss (non-change): 0.011758345179259777\n",
      "Iteration 0 Epoch 90/99\n",
      "----------\n",
      "Epoch 90, model processing time: 0.0264 seconds\n",
      "Epoch 90, data loading time: 0.0000 seconds\n",
      "Epoch 90, average training loss (change): 0.44572222232818604\n",
      "Epoch 90, average training loss (non-change): 0.011787308380007744\n",
      "Epoch 91, model processing time: 0.0265 seconds\n",
      "Epoch 91, data loading time: 0.0000 seconds\n",
      "Epoch 91, average training loss (change): 0.44514286518096924\n",
      "Epoch 91, average training loss (non-change): 0.011813274584710598\n",
      "Epoch 92, model processing time: 0.0245 seconds\n",
      "Epoch 92, data loading time: 0.0000 seconds\n",
      "Epoch 92, average training loss (change): 0.44455358386039734\n",
      "Epoch 92, average training loss (non-change): 0.01184243056923151\n",
      "Epoch 93, model processing time: 0.0252 seconds\n",
      "Epoch 93, data loading time: 0.0000 seconds\n",
      "Epoch 93, average training loss (change): 0.4438572824001312\n",
      "Epoch 93, average training loss (non-change): 0.011878778226673603\n",
      "Epoch 94, model processing time: 0.0246 seconds\n",
      "Epoch 94, data loading time: 0.0000 seconds\n",
      "Epoch 94, average training loss (change): 0.443089097738266\n",
      "Epoch 94, average training loss (non-change): 0.011920704506337643\n",
      "Epoch 95, model processing time: 0.0254 seconds\n",
      "Epoch 95, data loading time: 0.0000 seconds\n",
      "Epoch 95, average training loss (change): 0.4423562288284302\n",
      "Epoch 95, average training loss (non-change): 0.011962905526161194\n",
      "Epoch 96, model processing time: 0.0253 seconds\n",
      "Epoch 96, data loading time: 0.0000 seconds\n",
      "Epoch 96, average training loss (change): 0.44170278310775757\n",
      "Epoch 96, average training loss (non-change): 0.01200160849839449\n",
      "Epoch 97, model processing time: 0.0242 seconds\n",
      "Epoch 97, data loading time: 0.0000 seconds\n",
      "Epoch 97, average training loss (change): 0.44106796383857727\n",
      "Epoch 97, average training loss (non-change): 0.01203718688338995\n",
      "Epoch 98, model processing time: 0.0260 seconds\n",
      "Epoch 98, data loading time: 0.0000 seconds\n",
      "Epoch 98, average training loss (change): 0.44038140773773193\n",
      "Epoch 98, average training loss (non-change): 0.012071274220943451\n",
      "Epoch 99, model processing time: 0.0264 seconds\n",
      "Epoch 99, data loading time: 0.0000 seconds\n",
      "Epoch 99, average training loss (change): 0.43964284658432007\n",
      "Epoch 99, average training loss (non-change): 0.012104488909244537\n",
      "Training complete in 0m 50s with train loss 0.45175 and validation loss 0.70172\n",
      "Model 0 Trained\n",
      "Corresponding AE Vector: tensor([0, 0, 1, 0, 0, 0, 0, 0])\n",
      "Corresponding AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Model 0\n",
      "Learned AE Guidance (Lower better): tensor([7.5286e-01, 7.4332e-01, 9.9220e-01, 2.3026e-09, 2.3026e-09, 7.4895e-01,\n",
      "        8.2120e-01, 4.6737e-01])\n",
      "Learning Done in 60.71571612358093 sec, Checking if exists low-objective ae vector...\n",
      "No low-objective ae vectors found for this iteration..\n",
      "Node [0 0 0 0 0 0 0 0] is fully expanded or has -inf value.\n",
      "New Frontier: []\n",
      "-----Iteration: 1 (neural_u_p1)------\n",
      "Generate 12 AE Vectors-Score Pairs for Predicate neural_u_p1\n",
      "GT AE Vec Provided, No Need to Generate AE Vectors.\n",
      "No more sat matrixes can be generated at iteration 1!\n",
      "Early Stopping at Iteration 1!\n",
      "******************Bi-level Optimization Done for neural_u_p1! Summary:******************\n",
      "After 1 iterations, we got 0 basic vectors:\n",
      "Skipping learning for HasChemX since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 0, 1, 0, 0, 0, 0])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for HasChemY since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 0, 0, 0, 1, 0, 0, 0])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for IsCalibrated since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "AE Vector (Del): tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Skipping learning for ViewClear since it is already provided! Generate vectors for it!\n",
      "Sampling 0/100 AE Vectors (Tgt 1)\n",
      "Found a satisfying AE Vector with Guidance after 0 tries.\n",
      "AE Vector (Add): tensor([0, 1, 0, 0, 0, 0, 0, 0])\n",
      "AE Vector (Del): tensor([0, 0, 1, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Bad Effect Vector Learning Completed!\n",
      "   This will show poor learning performance in the guidance scores\n"
     ]
    }
   ],
   "source": [
    "# Create new approach with bad effect vector and test neural learning\n",
    "print(\"üß† Creating New IVNTR Approach with Bad Effect Vector...\\n\")\n",
    "\n",
    "# Create fresh environment and tasks (reuse existing dataset)\n",
    "env_bad = SatellitesEnv(use_gui=False)\n",
    "train_tasks_bad = env_bad.get_train_tasks()\n",
    "training_tasks_bad = [task.task for task in train_tasks_bad]\n",
    "options_bad = get_gt_options(env_bad.get_name())\n",
    "\n",
    "# Create the approach with bad configuration\n",
    "approach_bad = create_approach(\n",
    "    CFG.approach,\n",
    "    env_bad.predicates,\n",
    "    options_bad,\n",
    "    env_bad.types,\n",
    "    env_bad.action_space,\n",
    "    training_tasks_bad\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Bad Effect Vector Approach Created: {approach_bad.get_name()}\")\n",
    "\n",
    "# Use the same demonstration dataset (reuse for consistency)\n",
    "print(f\"\\nüé¨ Using Existing Demonstration Dataset...\")\n",
    "print(f\"   Dataset trajectories: {len(dataset.trajectories)}\")\n",
    "\n",
    "# Run neural learning with bad effect vector\n",
    "print(f\"\\nüö® Running Neural Learning with BAD Effect Vector...\")\n",
    "print(f\"   Expected: Poor learning performance due to wrong labels\")\n",
    "print(f\"   Effect vector: [0, 0, 1, 0, 0, 0, 0, 0] (MoveTo adds IsCalibrated)\")\n",
    "\n",
    "learning_trajectories_bad, init_atom_traj_bad = approach_bad.learn_neural_predicates(dataset)\n",
    "\n",
    "print(f\"\\n‚úÖ Bad Effect Vector Learning Completed!\")\n",
    "print(f\"   This will show poor learning performance in the guidance scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2gsz6x8wjmu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analyzing Guidance Scores from Bad Effect Vector Learning\n",
      "\n",
      "üìã Reading AE Guidance Scores from Log File...\n",
      "üéØ Found AE Guidance Scores in Log File:\n",
      "\n",
      "   Learned AE Guidance (Lower better): tensor([7.5286e-01, 7.4332e-01, 9.9220e-01, 2.3026e-09, 2.3026e-09, 7.4895e-01,\n",
      "           8.2120e-01, 4.6737e-01])\n",
      "   Learning Done in 60.71571612358\n",
      "\n",
      "üö® Key Interpretation:\n",
      "   - Lower guidance scores = better neural learning performance\n",
      "   - Higher guidance scores = worse neural learning performance\n",
      "   - Bad effect vector [0,0,1,0,0,0,0,0] should show some poor scores\n",
      "   - These scores guide IVNTR's tree search algorithm\n",
      "\n",
      "üí° Next Step: These guidance scores feed into the tree search algorithm\n",
      "   at predicators/approaches/bilevel_learning_approach.py::L1683\n"
     ]
    }
   ],
   "source": [
    "# Analyze the guidance scores from bad effect vector learning\n",
    "print(\"üìä Analyzing Guidance Scores from Bad Effect Vector Learning\\n\")\n",
    "\n",
    "# Read guidance scores from the log file\n",
    "print(\"üìã Reading AE Guidance Scores from Log File...\")\n",
    "try:\n",
    "    with open(bad_log_file_path, 'r') as f:\n",
    "        log_content = f.read()\n",
    "\n",
    "    # Find the \"Learned AE Guidance (Lower better):\" section\n",
    "    guidance_start = log_content.find(\"Learned AE Guidance (Lower better):\")\n",
    "    if guidance_start != -1:\n",
    "        # Extract the guidance section (next few lines after the marker)\n",
    "        guidance_section = log_content[guidance_start:guidance_start+180]  # Get reasonable chunk\n",
    "        guidance_lines = guidance_section.split('\\n')\n",
    "\n",
    "        print(\"üéØ Found AE Guidance Scores in Log File:\")\n",
    "        print()\n",
    "        for i, line in enumerate(guidance_lines):\n",
    "            if i == 0 or line.strip():  # Print header and non-empty lines\n",
    "                print(f\"   {line}\")\n",
    "            if i > 15:  # Limit output to avoid too much text\n",
    "                break\n",
    "        print()\n",
    "\n",
    "        print(\"üö® Key Interpretation:\")\n",
    "        print(\"   - Lower guidance scores = better neural learning performance\")\n",
    "        print(\"   - Higher guidance scores = worse neural learning performance\")\n",
    "        print(\"   - Bad effect vector [0,0,1,0,0,0,0,0] should show some poor scores\")\n",
    "        print(\"   - These scores guide IVNTR's tree search algorithm\")\n",
    "\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è 'Learned AE Guidance (Lower better):' not found in log file\")\n",
    "        print(\"   This might mean the learning process hasn't completed yet\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"   ‚ö†Ô∏è Log file not found: {bad_log_file_path}\")\n",
    "    print(\"   Run the bad effect vector learning first to generate the log\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Error reading log file: {e}\")\n",
    "\n",
    "print(\"\\nüí° Next Step: These guidance scores feed into the tree search algorithm\")\n",
    "print(\"   at predicators/approaches/bilevel_learning_approach.py::L1683\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dmpvvudw7nl",
   "metadata": {},
   "source": [
    "### Tree Search Algorithm Integration\n",
    "\n",
    "The guidance scores computed above feed into IVNTR's **tree search algorithm** for systematic effect vector exploration. This is implemented in the bilevel learning approach at a specific location in the codebase.\n",
    "\n",
    "**Implementation Reference**: \n",
    "- **File**: `predicators/approaches/bilevel_learning_approach.py`\n",
    "- **Method**: Tree search logic around **line 1683**\n",
    "- **Function**: Uses guidance scores to expand/prune search tree nodes\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Initialize Search Tree**: Start with candidate effect vectors\n",
    "2. **Neural Evaluation**: For each candidate, train neural network and compute guidance score  \n",
    "3. **Tree Expansion**: Expand high-scoring candidates, prune low-scoring ones\n",
    "4. **Iterative Refinement**: Continue until convergence or resource limits\n",
    "\n",
    "This completes our exploration of IVNTR's neural-informed symbolic search - the key insight that neural learning performance can guide symbolic effect discovery!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304974a0",
   "metadata": {},
   "source": [
    "## 8. Advanced Understanding\n",
    "\n",
    "So far, we have gone through how IVNTR:\n",
    "- Trains neural networks with effect vector labels.\n",
    "- Performs symbolic effect search with neural guidance.\n",
    "\n",
    "To invent all the predicates in satellites domain, please see `scripts/train/satellites/satellites_biplan.sh` and also check the predicate learning configurations `predicators/config/satellites/pred.yaml`.\n",
    "\n",
    "We next highlights some advanced implementation in predicate learning for better understanding.\n",
    "\n",
    "### Binary Predicates\n",
    "\n",
    "`IsCalibrated(?sat)` is a unary predicate that takes in feature vectors of a single entity (object/satellite). For binary predicates, we input the concatination of feature vectors. details see `predicators/approaches/bilevel_learning_approach.py::gen_graph_data`.\n",
    "\n",
    "### Variable Bindings\n",
    "\n",
    "IVNTR algorithm made an implicit assumption that, for a certain predicate, it can only appear at most once in the effect set of an operator (add effect or delete effect or no effect). However, there are cases where a single predicate appears twice, e.g., in `Blocks` domain we have the `Clear(?block)` predicate like this:\n",
    "```\n",
    "UnStack::\n",
    "    ?x0:block, ?x1:block, ?x2:robot\n",
    "    pre-cond:\n",
    "        HandEmpty(?x2),\n",
    "        On(?x0,?x1),\n",
    "        Clear(?x0)\n",
    "    add-eff:\n",
    "        Holding(?x2,?x0),\n",
    "        Clear(?x1)\n",
    "    del-eff:\n",
    "        HandEmpty(?x2),\n",
    "        On(?x0,?x1),\n",
    "        Clear(?x0)\n",
    "```\n",
    "In this case, `Clear(?block)` can't be fully captured by [0,1,-1] effect vectors. To tackle this, each predicate is additionally associated with a varibale binding index, and we only learn&evaluate the specified variable binding (which varible in the operator should be binded to the predicate). In the code we used `ent_idx` (default to 0) for this.\n",
    "\n",
    "Specifically, we will invent two predicates to capture the original `Clear(?block)`, with `Clear1(?block)_0` (it only binds the first block variable in any operator) and `Clear2(?block)_1` (it only binds the second block variable in any operator). And this way, we can use the [0,1,-1] effect vectors to represent the distribution (another way to explain is that the varible binding is actually part of the effect search).\n",
    "For more details, you can try `Blocks` domain, and understand the code: `predicators/approaches/bilevel_learning_approach.py::ent_idx`.\n",
    "\n",
    "\n",
    "### Negation and Quantifiers\n",
    "Invented predicates can be augmented with negation (`Not`) and quantifiers (`ForAll` and `Exist`), the quantifiers can also be applied to different variables. The augmented predicates will be added to the predicate pool before predicate selection. details see L1725 of `predicators/approaches/bilevel_learning_approach.py`.\n",
    "\n",
    "### Predicate Selection\n",
    "After we get the pool of neural predicates (low-loss classifiers), we conduct hill-climbing search similar to [previous work](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=CMcsygMAAAAJ&citation_for_view=CMcsygMAAAAJ:J_g5lzvAfSwC). The core algorithm evaluates a set of predicates by:\n",
    "1. Use the predicates to learn an operator set.\n",
    "2. Use the operator set to do task planning.\n",
    "3. Compare the generated task plan with demonstration length, and additionally capture search statistics (num_nodes_expanded / created).\n",
    "The implementation is in: `predicators/approaches/bilevel_learning_approach.py::_select_predicates_by_score_search` and `predicators/predicate_search_score_functions.py::_OperatorBeliefScoreFunction`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ivntr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
