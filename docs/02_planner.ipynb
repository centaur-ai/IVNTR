{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "planner-intro",
   "metadata": {},
   "source": "# Ground Truth Planning Model: NSRTs and Motion Primitives\n\nIn this notebook, we'll explore the ground truth planning model for the Satellites domain. This includes:\n\n1. **Motion Primitives (Options)**: Low-level controllers that execute actions\n2. **NSRTs**: High-level symbolic operators that use predicates and options\n3. **Bilevel Planning Process**: How high-level and low-level planning work together\n\nUnderstanding this ground truth model is crucial because **this is what IVNTR learns to approximate** using neural predicates!\n\n## Key Insight\nThe ground truth model represents the **\"oracle knowledge\"** that a perfect planner would have. IVNTR's goal is to learn this knowledge from demonstrations, replacing some predicates with neural networks while maintaining the same planning capabilities."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Set, List, Sequence\n",
    "\n",
    "# Add the project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import IVNTR components\n",
    "from predicators.envs.satellites import SatellitesEnv\n",
    "from predicators.ground_truth_models.satellites.nsrts import SatellitesGroundTruthNSRTFactory\n",
    "from predicators.ground_truth_models.satellites.options import SatellitesGroundTruthOptionFactory\n",
    "from predicators.structs import Object, State, GroundAtom, Action, NSRT, ParameterizedOption\n",
    "from predicators import utils\n",
    "from predicators.settings import CFG\n",
    "\n",
    "# Configure for tutorial\n",
    "CFG.seed = 42\n",
    "CFG.num_train_tasks = 3\n",
    "CFG.satellites_num_sat_train = [2, 3]\n",
    "CFG.satellites_num_obj_train = [2, 3]\n",
    "\n",
    "# Create environment and get a sample task\n",
    "env = SatellitesEnv(use_gui=False)\n",
    "train_tasks = env.get_train_tasks()\n",
    "sample_task = train_tasks[0]\n",
    "\n",
    "print(\"✅ Setup complete! Environment and sample task ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "options-intro",
   "metadata": {},
   "source": [
    "## 1. Motion Primitives (Options)\n",
    "\n",
    "Options are parameterized low-level controllers that bridge symbolic planning and continuous control. Each option:\n",
    "\n",
    "- **Takes objects as arguments** (e.g., which satellite and object to act on)\n",
    "- **Takes continuous parameters** (e.g., target positions, sampled by NSRTs)\n",
    "- **Produces low-level actions** (e.g., the 10-dimensional action vectors)\n",
    "\n",
    "In the Satellites domain, we have 8 different options corresponding to different types of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-options",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth options\n",
    "types_dict = {t.name: t for t in env.types}\n",
    "predicates_dict = {p.name: p for p in env.predicates}\n",
    "\n",
    "options = SatellitesGroundTruthOptionFactory.get_options(\n",
    "    \"satellites\", types_dict, predicates_dict, env.action_space\n",
    ")\n",
    "options_dict = {opt.name: opt for opt in options}\n",
    "\n",
    "print(\"🎯 Ground Truth Options (Motion Primitives):\\n\")\n",
    "\n",
    "for opt_name, opt in sorted(options_dict.items()):\n",
    "    types_str = \", \".join([t.name for t in opt.types])\n",
    "    param_space = getattr(opt, 'params_space', 'No parameters')\n",
    "    \n",
    "    print(f\"📦 {opt_name}({types_str})\")\n",
    "    if hasattr(param_space, 'shape'):\n",
    "        print(f\"   Parameters: {param_space.shape[0]}D continuous vector\")\n",
    "        print(f\"   Bounds: [{param_space.low[0]:.1f}, {param_space.high[0]:.1f}]\")\n",
    "    else:\n",
    "        print(f\"   Parameters: {param_space}\")\n",
    "    print()\n",
    "\n",
    "print(\"💡 Key Insight: Options abstract away the details of how to execute actions,\")\n",
    "print(\"   letting the symbolic planner focus on WHAT to do, not HOW to do it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "option-policies",
   "metadata": {},
   "source": [
    "### Understanding Option Policies\n",
    "\n",
    "Let's examine how options work by looking at their policies. Each option has a policy that maps:\n",
    "- **State** + **Objects** + **Parameters** → **Action**\n",
    "\n",
    "This is the \"motion primitive\" - a reusable piece of control logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "option-policies",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine specific option policies\n",
    "state = sample_task.init\n",
    "satellites = list(state.get_objects(env._sat_type))\n",
    "objects = list(state.get_objects(env._obj_type))\n",
    "\n",
    "sat = satellites[0]\n",
    "obj = objects[0]\n",
    "\n",
    "print(f\"🛸 Using satellite: {sat.name}\")\n",
    "print(f\"🎯 Using object: {obj.name}\")\n",
    "print(f\"Initial satellite position: ({state.get(sat, 'x'):.3f}, {state.get(sat, 'y'):.3f})\")\n",
    "print(f\"Object position: ({state.get(obj, 'x'):.3f}, {state.get(obj, 'y'):.3f})\\n\")\n",
    "\n",
    "# 1. MoveTo option - requires 2D target position parameters\n",
    "moveto_option = options_dict[\"MoveTo\"]\n",
    "target_params = np.array([0.5, 0.5])  # Move to center\n",
    "\n",
    "print(\"🚀 MoveTo Option:\")\n",
    "print(f\"   Target parameters: {target_params}\")\n",
    "\n",
    "# Execute the policy to get an action\n",
    "moveto_action = moveto_option.policy(state, {}, [sat, obj], target_params)\n",
    "print(f\"   Generated action: {moveto_action.arr}\")\n",
    "print(f\"   Action breakdown:\")\n",
    "print(f\"     - Current sat pos: ({moveto_action.arr[0]:.3f}, {moveto_action.arr[1]:.3f})\")\n",
    "print(f\"     - Object pos: ({moveto_action.arr[2]:.3f}, {moveto_action.arr[3]:.3f})\")\n",
    "print(f\"     - Target sat pos: ({moveto_action.arr[4]:.3f}, {moveto_action.arr[5]:.3f})\")\n",
    "print(f\"     - Action flags: {moveto_action.arr[6:]}\")\n",
    "\n",
    "# 2. Calibrate option - no parameters needed\n",
    "calibrate_option = options_dict[\"Calibrate\"]\n",
    "print(f\"\\n🔧 Calibrate Option:\")\n",
    "print(f\"   Parameters needed: None (uses current positions)\")\n",
    "\n",
    "calibrate_action = calibrate_option.policy(state, {}, [sat, obj], np.array([]))\n",
    "print(f\"   Generated action: {calibrate_action.arr}\")\n",
    "print(f\"   Note: calibrate flag (index 6) = {calibrate_action.arr[6]} (set to 1.0)\")\n",
    "\n",
    "# 3. ShootChemX option - no parameters\n",
    "shoot_option = options_dict[\"ShootChemX\"]\n",
    "print(f\"\\n💥 ShootChemX Option:\")\n",
    "shoot_action = shoot_option.policy(state, {}, [sat, obj], np.array([]))\n",
    "print(f\"   Generated action: {shoot_action.arr}\")\n",
    "print(f\"   Note: shoot_chem_x flag (index 7) = {shoot_action.arr[7]} (set to 1.0)\")\n",
    "\n",
    "print(f\"\\n💡 Pattern: Each option encodes a specific type of action by setting the right flags\")\n",
    "print(f\"   and using appropriate parameter interpretation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nsrts-intro",
   "metadata": {},
   "source": [
    "## 2. NSRTs: Neuro-Symbolic Relational Transitions\n",
    "\n",
    "NSRTs are the high-level symbolic operators that define the planning model. Each NSRT specifies:\n",
    "\n",
    "- **Parameters**: Which objects it operates on\n",
    "- **Preconditions**: Predicates that must be true before execution\n",
    "- **Add Effects**: Predicates that become true after execution  \n",
    "- **Delete Effects**: Predicates that become false after execution\n",
    "- **Option**: Which motion primitive to use\n",
    "- **Sampler**: How to generate continuous parameters for the option\n",
    "\n",
    "NSRTs are the symbolic \"recipes\" for achieving goals in the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-nsrts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth NSRTs\n",
    "nsrts = SatellitesGroundTruthNSRTFactory.get_nsrts(\n",
    "    \"satellites\", types_dict, predicates_dict, options_dict\n",
    ")\n",
    "\n",
    "print(f\"🧠 Ground Truth NSRTs: {len(nsrts)} operators\\n\")\n",
    "\n",
    "# Create a dictionary for easy access\n",
    "nsrts_dict = {nsrt.name: nsrt for nsrt in nsrts}\n",
    "\n",
    "# Display all NSRTs with their structure\n",
    "for nsrt_name, nsrt in sorted(nsrts_dict.items()):\n",
    "    print(f\"📋 {nsrt_name}\")\n",
    "    print(f\"   Parameters: {[str(p) for p in nsrt.parameters]}\")\n",
    "    print(f\"   Option: {nsrt.option.name}\")\n",
    "    \n",
    "    if nsrt.preconditions:\n",
    "        print(f\"   Preconditions:\")\n",
    "        for pre in nsrt.preconditions:\n",
    "            print(f\"     ✓ {pre}\")\n",
    "    else:\n",
    "        print(f\"   Preconditions: None\")\n",
    "    \n",
    "    if nsrt.add_effects:\n",
    "        print(f\"   Add Effects:\")\n",
    "        for eff in nsrt.add_effects:\n",
    "            print(f\"     + {eff}\")\n",
    "    \n",
    "    if nsrt.delete_effects:\n",
    "        print(f\"   Delete Effects:\")\n",
    "        for eff in nsrt.delete_effects:\n",
    "            print(f\"     - {eff}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-nsrt-analysis",
   "metadata": {},
   "source": [
    "### Detailed NSRT Analysis\n",
    "\n",
    "Let's analyze some key NSRTs to understand how they work and why they're designed this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-nsrts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze key NSRTs in detail\n",
    "\n",
    "print(\"🔍 Detailed NSRT Analysis\\n\")\n",
    "\n",
    "# 1. MoveTo NSRT - Basic positioning\n",
    "moveto_nsrt = nsrts_dict[\"MoveTo\"]\n",
    "print(\"1️⃣ MoveTo NSRT - Basic Positioning\")\n",
    "print(\"   Purpose: Move satellite to see objects\")\n",
    "print(f\"   Precondition: ViewClear(?sat) - satellite's view must be unobstructed\")\n",
    "print(f\"   Add Effect: Sees(?sat, ?obj) - satellite will see the object\")\n",
    "print(f\"   Delete Effect: ViewClear(?sat) - view might become obstructed\")\n",
    "print(f\"   💡 Design Logic: Only move when view is clear, movement enables seeing\")\n",
    "print()\n",
    "\n",
    "# 2. Calibrate NSRT - Preparation for readings\n",
    "calibrate_nsrt = nsrts_dict[\"Calibrate\"]\n",
    "print(\"2️⃣ Calibrate NSRT - Instrument Preparation\")\n",
    "print(\"   Purpose: Calibrate satellite's instrument\")\n",
    "print(f\"   Preconditions:\")\n",
    "for pre in calibrate_nsrt.preconditions:\n",
    "    print(f\"     - {pre}\")\n",
    "print(f\"   Add Effect: IsCalibrated(?sat) - satellite becomes ready for readings\")\n",
    "print(f\"   💡 Design Logic: Must see correct calibration target, then can take readings\")\n",
    "print()\n",
    "\n",
    "# 3. TakeCameraReading NSRT - Complex coordination\n",
    "camera_nsrt = nsrts_dict[\"TakeCameraReading\"]\n",
    "print(\"3️⃣ TakeCameraReading NSRT - Complex Coordination\")\n",
    "print(\"   Purpose: Take a camera reading of an object\")\n",
    "print(f\"   Preconditions (ALL must be true):\")\n",
    "for pre in camera_nsrt.preconditions:\n",
    "    print(f\"     - {pre}\")\n",
    "print(f\"   Add Effect: {list(camera_nsrt.add_effects)[0]}\")\n",
    "print(f\"   💡 Design Logic: Camera readings require ChemX preparation!\")\n",
    "print(f\"      This creates coordination dependencies between satellites.\")\n",
    "print()\n",
    "\n",
    "# 4. TakeGeigerReading NSRT - No coordination needed\n",
    "geiger_nsrt = nsrts_dict[\"TakeGeigerReading\"]\n",
    "print(\"4️⃣ TakeGeigerReading NSRT - Independent Operation\")\n",
    "print(\"   Purpose: Take a Geiger reading (no chemicals needed)\")\n",
    "print(f\"   Preconditions:\")\n",
    "for pre in geiger_nsrt.preconditions:\n",
    "    print(f\"     - {pre}\")\n",
    "print(f\"   💡 Design Logic: Geiger readings don't need chemical preparation,\")\n",
    "print(f\"      making them easier to achieve but still requiring basic setup.\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 Key Insight: NSRTs encode the domain's coordination requirements!\")\n",
    "print(\"   Different instruments have different dependencies, forcing the planner\")\n",
    "print(\"   to reason about multi-step, multi-agent coordination.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planning-demo",
   "metadata": {},
   "source": "## 3. Bilevel Planning Process\n\nNow let's demonstrate the actual bilevel planning process used in IVNTR. Planning happens in two steps:\n\n1. **High-level Planning**: Generate symbolic skeletons (sequences of NSRTs)\n2. **Low-level Refinement**: Sample continuous parameters and refine into executable options\n\nThis two-step process is what enables symbolic reasoning while handling continuous control."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planning-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate planning with NSRTs\n",
    "print(\"🎯 Planning with NSRTs\\n\")\n",
    "\n",
    "# Analyze current state and applicable NSRTs\n",
    "print(\"📊 Current State Analysis:\")\n",
    "print(f\"   Satellites: {len(satellites)}\")\n",
    "print(f\"   Objects: {len(objects)}\")\n",
    "print(f\"   Goals: {len(sample_task.goal)}\")\n",
    "print()\n",
    "\n",
    "# Check which predicates hold initially\n",
    "print(\"✅ Initial Predicate Status:\")\n",
    "for sat in satellites:\n",
    "    is_calibrated = env._IsCalibrated_holds(state, [sat])\n",
    "    view_clear = env._ViewClear_holds(state, [sat])\n",
    "    print(f\"   {sat.name}: IsCalibrated={is_calibrated}, ViewClear={view_clear}\")\n",
    "    \n",
    "    for obj in objects:\n",
    "        sees = env._Sees_holds(state, [sat, obj])\n",
    "        cal_target = env._CalibrationTarget_holds(state, [sat, obj])\n",
    "        if sees or cal_target:\n",
    "            print(f\"     → {obj.name}: Sees={sees}, CalibrationTarget={cal_target}\")\n",
    "\n",
    "for obj in objects:\n",
    "    has_x = env._HasChemX_holds(state, [obj])\n",
    "    has_y = env._HasChemY_holds(state, [obj])\n",
    "    if has_x or has_y:\n",
    "        print(f\"   {obj.name}: HasChemX={has_x}, HasChemY={has_y}\")\n",
    "print()\n",
    "\n",
    "# Find applicable NSRTs\n",
    "print(\"🔍 Applicable NSRTs in Current State:\")\n",
    "applicable_nsrts = []\n",
    "\n",
    "for nsrt in nsrts:\n",
    "    # Try all possible object bindings\n",
    "    for sat in satellites:\n",
    "        for obj in objects:\n",
    "            objects_list = [sat, obj]\n",
    "            \n",
    "            # Check if all preconditions are satisfied\n",
    "            all_preconditions_met = True\n",
    "            for precondition in nsrt.preconditions:\n",
    "                # Ground the precondition with actual objects\n",
    "                pred_name = precondition.predicate.name\n",
    "                pred_args = [objects_list[i] for i in range(len(precondition.predicate.types))]\n",
    "                \n",
    "                # Check if predicate holds\n",
    "                pred_func = getattr(env, f\"_{pred_name}_holds\")\n",
    "                if not pred_func(state, pred_args):\n",
    "                    all_preconditions_met = False\n",
    "                    break\n",
    "            \n",
    "            if all_preconditions_met:\n",
    "                applicable_nsrts.append((nsrt.name, sat.name, obj.name))\n",
    "\n",
    "# Display applicable NSRTs\n",
    "if applicable_nsrts:\n",
    "    for nsrt_name, sat_name, obj_name in applicable_nsrts:\n",
    "        print(f\"   ✓ {nsrt_name}({sat_name}, {obj_name})\")\n",
    "else:\n",
    "    print(f\"   ❌ No NSRTs applicable (all have unmet preconditions)\")\n",
    "print()\n",
    "\n",
    "# Show what we need to achieve goals\n",
    "print(\"🏆 Goal Analysis:\")\n",
    "for goal_atom in sample_task.goal:\n",
    "    pred_name = goal_atom.predicate.name\n",
    "    obj_names = [obj.name for obj in goal_atom.objects]\n",
    "    print(f\"   Goal: {pred_name}({', '.join(obj_names)})\")\n",
    "    \n",
    "    # Find which NSRT achieves this goal\n",
    "    achieving_nsrt = None\n",
    "    if \"Camera\" in pred_name:\n",
    "        achieving_nsrt = \"TakeCameraReading\"\n",
    "    elif \"Infrared\" in pred_name:\n",
    "        achieving_nsrt = \"TakeInfraredReading\"\n",
    "    elif \"Geiger\" in pred_name:\n",
    "        achieving_nsrt = \"TakeGeigerReading\"\n",
    "    \n",
    "    if achieving_nsrt:\n",
    "        nsrt = nsrts_dict[achieving_nsrt]\n",
    "        print(f\"     Achieved by: {achieving_nsrt}\")\n",
    "        print(f\"     Requires:\")\n",
    "        for precond in nsrt.preconditions:\n",
    "            print(f\"       - {precond}\")\n",
    "print()\n",
    "\n",
    "print(\"💡 Planning Insight: The planner needs to work backwards from goals,\")\n",
    "print(\"   figuring out which NSRTs achieve them and what preconditions those require.\")\n",
    "print(\"   This creates chains of dependencies that must be satisfied in sequence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planning-sequence",
   "metadata": {},
   "source": "### Bilevel Planning Architecture\n\nLet's demonstrate the difference between symbolic and continuous levels of planning."
  },
  {
   "cell_type": "code",
   "id": "rngtjfp8e6l",
   "source": "# Demonstrate the difference between symbolic and continuous levels\nprint(\"🏗️ Bilevel Planning Architecture\\n\")\n\nprint(\"📊 LEVEL 1: Symbolic/High-level Planning\")\nprint(\"   Input: Initial predicates + Goal predicates\")  \nprint(\"   Process: Search over NSRT sequences\")\nprint(\"   Output: Skeleton (sequence of symbolic actions)\")\nprint(\"   Example skeleton for CameraReadingTaken goal:\")\nprint(\"     1. MoveAway(?sat, ?obj)\")\nprint(\"     2. MoveTo(?sat, ?calibration_target)\")  \nprint(\"     3. Calibrate(?sat, ?calibration_target)\")\nprint(\"     4. MoveTo(?sat, ?goal_object)\")\nprint(\"     5. ShootChemX(?chemsat, ?goal_object)\")\nprint(\"     6. TakeCameraReading(?camsat, ?goal_object)\")\nprint()\n\nprint(\"⚙️ LEVEL 2: Continuous/Low-level Planning\")\nprint(\"   Input: Skeleton + Current state\")\nprint(\"   Process: Sample continuous parameters for each NSRT\")\nprint(\"   Output: Executable option sequence\")\nprint(\"   Example refinement:\")\nprint(\"     1. MoveAway(sat0, obj1) → MoveTo_option(sat0, obj1, params=[0.1, 0.9])\")\nprint(\"     2. MoveTo(sat0, obj0) → MoveTo_option(sat0, obj0, params=[0.3, 0.4])\")\nprint(\"     3. Calibrate(sat0, obj0) → Calibrate_option(sat0, obj0, params=[])\")\nprint(\"     4. MoveTo(sat0, obj1) → MoveTo_option(sat0, obj1, params=[0.6, 0.7])\")\nprint(\"     5. ShootChemX(sat1, obj1) → ShootChemX_option(sat1, obj1, params=[])\")\nprint(\"     6. TakeCameraReading(sat0, obj1) → UseCamera_option(sat0, obj1, params=[])\")\nprint()\n\nprint(\"🎯 Why This Matters for Learning:\")\nprint(\"   • Symbolic level: Learns WHICH actions to take and WHEN\")\nprint(\"   • Continuous level: Learns HOW to execute actions (parameter sampling)\")\nprint(\"   • IVNTR must learn symbolic predicates that enable this decomposition\")\nprint(\"   • Neural predicates replace ground truth predicates but maintain structure\")\nprint()\n\nprint(\"🧠 The Learning Challenge:\")\nprint(\"   Traditional approaches:\")\nprint(\"   ❌ Pure neural: Can't handle symbolic reasoning over long horizons\")\nprint(\"   ❌ Pure symbolic: Can't learn abstractions from continuous data\")\nprint(\"   ✅ IVNTR: Learns neural predicates that plug into symbolic planning!\")\nprint()\n\nprint(\"💡 Key Insight: The symbolic structure acts as 'scaffolding' for learning.\")\nprint(\"   Instead of learning everything from scratch, neural networks only need\")\nprint(\"   to learn the predicate mappings while leveraging proven planning algorithms.\")\n\n# Trace a typical planning sequence\nprint(\"\\n\" + \"=\"*60)\nprint(\"📋 Typical Planning Sequence for Camera Reading\\n\")\n\n# Goal: Take a camera reading\nprint(\"🎯 Goal: CameraReadingTaken(satellite, object)\")\nprint(\"\\n📝 Required Planning Steps:\")\n\ncamera_nsrt = nsrts_dict[\"TakeCameraReading\"]\nprint(f\"\\n5️⃣ Final Step: {camera_nsrt.name}\")\nprint(f\"   Requires: Sees + IsCalibrated + HasCamera + HasChemX\")\nprint(f\"   Effect: CameraReadingTaken ✓\")\n\nprint(f\"\\n4️⃣ Chemical Preparation: ShootChemX\")\nshoot_nsrt = nsrts_dict[\"ShootChemX\"]\nprint(f\"   Requires: {', '.join([str(p) for p in shoot_nsrt.preconditions])}\")\nprint(f\"   Effect: HasChemX ✓\")\n\nprint(f\"\\n3️⃣ Calibration: Calibrate\")\ncal_nsrt = nsrts_dict[\"Calibrate\"]\nprint(f\"   Requires: {', '.join([str(p) for p in cal_nsrt.preconditions])}\")\nprint(f\"   Effect: IsCalibrated ✓\")\n\nprint(f\"\\n2️⃣ Position for Calibration: MoveTo (to calibration target)\")\nmoveto_nsrt = nsrts_dict[\"MoveTo\"]\nprint(f\"   Requires: {', '.join([str(p) for p in moveto_nsrt.preconditions])}\")\nprint(f\"   Effect: Sees(satellite, calibration_target) ✓\")\n\nprint(f\"\\n1️⃣ Clear View: MoveAway (if needed)\")\nmoveaway_nsrt = nsrts_dict[\"MoveAway\"]\nprint(f\"   Requires: {', '.join([str(p) for p in moveaway_nsrt.preconditions])}\")\nprint(f\"   Effect: ViewClear ✓\")\n\nprint(\"\\n🎭 Multi-Agent Coordination:\")\nprint(\"   • Different satellites may handle different steps\")\nprint(\"   • Satellite with camera ≠ satellite with ChemX capability\")\nprint(\"   • Planner must coordinate multiple agents in time\")\n\nprint(\"\\n🧠 Learning Challenge:\")\nprint(\"   IVNTR must learn predicates like 'Sees' and 'HasChemX' from demonstrations\")\nprint(\"   while maintaining this complex coordination capability!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planning-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace a typical planning sequence\n",
    "print(\"📋 Typical Planning Sequence for Camera Reading\\n\")\n",
    "\n",
    "# Goal: Take a camera reading\n",
    "print(\"🎯 Goal: CameraReadingTaken(satellite, object)\")\n",
    "print(\"\\n📝 Required Planning Steps:\")\n",
    "\n",
    "camera_nsrt = nsrts_dict[\"TakeCameraReading\"]\n",
    "print(f\"\\n5️⃣ Final Step: {camera_nsrt.name}\")\n",
    "print(f\"   Requires: Sees + IsCalibrated + HasCamera + HasChemX\")\n",
    "print(f\"   Effect: CameraReadingTaken ✓\")\n",
    "\n",
    "print(f\"\\n4️⃣ Chemical Preparation: ShootChemX\")\n",
    "shoot_nsrt = nsrts_dict[\"ShootChemX\"]\n",
    "print(f\"   Requires: {', '.join([str(p) for p in shoot_nsrt.preconditions])}\")\n",
    "print(f\"   Effect: HasChemX ✓\")\n",
    "\n",
    "print(f\"\\n3️⃣ Calibration: Calibrate\")\n",
    "cal_nsrt = nsrts_dict[\"Calibrate\"]\n",
    "print(f\"   Requires: {', '.join([str(p) for p in cal_nsrt.preconditions])}\")\n",
    "print(f\"   Effect: IsCalibrated ✓\")\n",
    "\n",
    "print(f\"\\n2️⃣ Position for Calibration: MoveTo (to calibration target)\")\n",
    "moveto_nsrt = nsrts_dict[\"MoveTo\"]\n",
    "print(f\"   Requires: {', '.join([str(p) for p in moveto_nsrt.preconditions])}\")\n",
    "print(f\"   Effect: Sees(satellite, calibration_target) ✓\")\n",
    "\n",
    "print(f\"\\n1️⃣ Clear View: MoveAway (if needed)\")\n",
    "moveaway_nsrt = nsrts_dict[\"MoveAway\"]\n",
    "print(f\"   Requires: {', '.join([str(p) for p in moveaway_nsrt.preconditions])}\")\n",
    "print(f\"   Effect: ViewClear ✓\")\n",
    "\n",
    "print(\"\\n🎭 Multi-Agent Coordination:\")\n",
    "print(\"   • Different satellites may handle different steps\")\n",
    "print(\"   • Satellite with camera ≠ satellite with ChemX capability\")\n",
    "print(\"   • Planner must coordinate multiple agents in time\")\n",
    "\n",
    "print(\"\\n🧠 Learning Challenge:\")\n",
    "print(\"   IVNTR must learn predicates like 'Sees' and 'HasChemX' from demonstrations\")\n",
    "print(\"   while maintaining this complex coordination capability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "## Summary\n\nIn this notebook, we've explored the ground truth planning model for the Satellites domain:\n\n### 🎯 **Motion Primitives (Options)**\n- **8 different options** for different action types (MoveTo, Calibrate, Shoot chemicals, Use instruments)\n- **Parameterized controllers** that bridge symbolic decisions and continuous control\n- **Domain-specific encoding** of how to execute each type of action\n\n### 🧠 **NSRTs (Symbolic Operators)**\n- **8 symbolic operators** that define legal state transitions\n- **Precondition-effect structure** that captures domain constraints\n- **Coordination requirements** encoded through predicate dependencies\n\n### ⚙️ **Bilevel Planning Process**\n- **High-level planning**: Generate symbolic skeletons using predicate-based reasoning\n- **Low-level refinement**: Sample continuous parameters and refine into executable options  \n- **Two-level architecture** that separates symbolic reasoning from continuous control\n\n### 🎯 **The IVNTR Learning Challenge**\nThis ground truth model represents **perfect domain knowledge**. IVNTR's challenge is to:\n\n1. **Replace key predicates** (like `Sees`, `IsCalibrated`, `HasChemX`) with neural networks\n2. **Learn from demonstrations** to predict when these predicates hold\n3. **Maintain planning capability** with learned predicates\n4. **Generalize to new scenarios** not seen during training\n\n### 💡 **Key Insight: Scaffolded Learning**\nThe symbolic structure provides **scaffolding** for neural learning:\n- Neural networks don't learn planning algorithms from scratch\n- They only learn specific predicate relationships from continuous features  \n- The proven bilevel planning architecture remains intact\n- This enables both learning and generalization\n\nThe bilevel approach bridges the gap between symbolic reasoning (which enables long-horizon planning) and neural learning (which enables abstraction from continuous data).\n\n---\n\n## What's Next?\n\nBefore diving into the IVNTR learning algorithm, we need to understand:\n- **What data does IVNTR learn from?** (Demonstration trajectories)\n- **What must it output?** (Neural predicates that enable planning)\n\n**Next: `03_demonstration_data_generation.ipynb` - Understanding IVNTR's Training Data**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}